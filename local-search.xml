<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Distributed Lock</title>
    <link href="/2022/06/10/DistributedLock/"/>
    <url>/2022/06/10/DistributedLock/</url>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><blockquote><p>Distributed locks are a very useful primitive in many environments where different processes must operate with shared resources in a mutually exclusive way.</p></blockquote><p>Developers want to ensure that a shared resource (which can be an operation Update, or some content Read) on multi-instance nodes in the cluster can only be shared by one client at a time ( <strong>mutual exclusion</strong> ).</p><p>After reading the debate between Martin Kleppmann (DDIA Author) and Antirez (Father of Redis ) on whether the Redlock algorithm is safe, it is found that the application of distributed locks can actually be divided into different business scenarios:</p><ul><li><p>Efficiency: Using distributed locks can prevent different nodes from repeating the same work, but if the lock occasionally fails, it will not bring too severe impact. For example: a <strong>verification code</strong> is repeatedly sent.</p></li><li><p>Correctness: The business expects that under no circumstances will there be an error situation, because once it happens, it will have a huge impact. For example: repeat transactions (deductions &#x2F;additions ).</p></li></ul><p>Based on the above analysis, the implementation of distributed locks usually has two schemes based on Redis and ZK to my knowledge. Their comparison is shown in the following table:</p><table><thead><tr><th></th><th>强一致</th><th>高可用</th></tr></thead><tbody><tr><td>典型组件</td><td>ETCD, ZooKeeper, Chubby</td><td>Redis, Abase</td></tr><tr><td>时延 (Latency)</td><td>跨洋RTT 200ms</td><td>国内多机房 2ms 一次读写</td></tr><tr><td>优点</td><td>不会出现数据不一致的问题</td><td>脑裂切换的时候可能会出现不一致</td></tr><tr><td>缺点</td><td>延迟和吞吐受限</td><td>延迟极低，吞吐极大</td></tr></tbody></table><p>The selection of distributed locks is based on the concept of a certain component at the beginning of the design. For example, the Redis distributed solution is designed to be a highly available model at the beginning. According to CAP, however, by consulting the official doc of Redis , I found that there are actually many open source practices that have introduced additional algorithms on top of the basic version of Redis to achieve business expectations.</p><h3 id="Distributed-Lock-Principle-x2F-Rules"><a href="#Distributed-Lock-Principle-x2F-Rules" class="headerlink" title="Distributed Lock Principle &#x2F; Rules"></a>Distributed Lock Principle &#x2F; Rules</h3><ul><li><p><strong>Mutex (Consistency)</strong>: At Anytime, there’s only one client can have the lock.</p></li><li><p>Safety: Avoid deadlock, when client failover blocking the lock, the lock should be release as well.</p></li><li><p><strong>Availability:</strong> Avoid standalone service, when the master node failover, there must be some follower node can take over and continue providing service.</p></li><li><p>Reentran: For the same lock, only the process that acquired the lock can release the lock.</p></li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Strong-Consistency-Distributed-Lock"><a href="#Strong-Consistency-Distributed-Lock" class="headerlink" title="Strong Consistency Distributed Lock"></a>Strong Consistency Distributed Lock</h3><p>Based on strong consistency, we will have ZooKeeper, etcd , and Google’s Chubby. In these implementations, ZK is based on the ZAB consensus algorithm and etcd is based on Raft. On the other hand, although Google proposed <a href="https://research.google.com/archive/chubby-osdi06.pdf">chubby </a>in the paper, the open source code has not been published, so Chubby is generally not used as a distributed lock solution, and Chubby has an open source version called ZooKeeper.</p><p>Chubby&#x2F; ZK is inspired by the file system and designed as a service for <strong>metadata management</strong> . Different from large files, these two systems are designed for small data, usually the data volume should be below 10G. Chubby internally achieves consistency through Paxos, and ZK ensures internal consistency through ZAB.</p><p>In addition to metadata management, ZK also supports the following:</p><ul><li><p>Distributed lock</p></li><li><p>Coordination services: For example, the Master in the MapReduce can be implemented in ZK to issue tasks to idle workers.</p></li><li><p>“Sequencer”: write sequentially</p></li></ul><p>For Raft-based etcd , it is clear that throughput and latency are limited by the performance of the Raft Group Leader. If we deploy our services across continents or IDCs , e.g. VA , SG , RU , EU  TTP , etc. The entire system must do transoceanic network transmission for communication. No matter where the leader is, the Follower must go far away to synchronize the RaftLog from the leader, so the performance will be very low.</p><blockquote><p>I have discussed with one of my friends who’s from Tsinghua University about this problem and he provides me some basic insights that Leaderless &#x2F; Multi-Leader Architecture might be able to ease this high latency problem.</p></blockquote><p>For ZAB-based ZooKeeper, its latency should be smaller than ETCD , and the throughput should also meet the needs of the business. But from the monitoring point of view, SG ‘s cluster does not have much traffic, so it is speculated that the usage in SG’s business should not be very large and the infra support is not quite well.</p><h4 id="etcd-Distributed-Lock-Implementation"><a href="#etcd-Distributed-Lock-Implementation" class="headerlink" title="etcd Distributed Lock Implementation"></a>etcd Distributed Lock Implementation</h4><blockquote><p>KV Pair form to do the lock, based on Raft guarantee Linearizability, high latency, in fact, the company’s internal support scheme for ETCD and ByteKV similar.</p></blockquote><p>I feel that the performance of these strongly consistent locks is very related to the deployment method, and it is impossible to generalize which method will be better than the other. And the fineness of different business scenarios is not the same. To my knowledge, business operations scenarios do not require very high mutual exclusion of critical sections, or there are some fall back strategies, which can reduce the dependence on strong consistency. Because the probability of the consistency problem of highly available distributed locks is relatively small after all, it may be painful to choose a strongly consistent KV with low availability for small probability events.</p><p>Etcd provides Lock API：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">LOCK [<span class="hljs-keyword">options</span>] <span class="hljs-symbol">&lt;lockname&gt;</span> [<span class="hljs-keyword">command</span> arg1 arg2 ...]<br></code></pre></td></tr></table></figure><p>Description：LOCK acquires a distributed mutex with a given name. Once the lock is acquired, it will be held until <strong>etcdctl</strong> is terminated.</p><p>Options:</p><ul><li>ttl - time out in seconds of lock session. (Default Value: 60s).</li></ul><p>If the client side does not release the lock due to some problem, other processes will be blocked by the time of the TTL . The default time of the TTL is 60s, which means that the lock service is unavailable during these 60s.</p><h4 id="ZK-Distributed-Lock-Implementation"><a href="#ZK-Distributed-Lock-Implementation" class="headerlink" title="ZK Distributed Lock Implementation"></a>ZK Distributed Lock Implementation</h4><p>Based on ZNode, the only way to use file namespace to do Mutex, based on the ZAB protocol to ensure synchronization between replicas, the delay is lower than that of Raft (further investigation is required).</p><p>The following code is written based on the understanding of Chubby. Maybe ZK will be slightly different. I only have limited time to look at ZK for the time being. I wrote it according to the previous understanding of Chubby’s paper.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Go">x = Open(<span class="hljs-string">&quot;/$&#123;PSM&#125;/$&#123;biz_content&#125;/resource_key&quot;</span>)<br><span class="hljs-keyword">if</span> TryAcquire(x) == ok &#123;<br>    <span class="hljs-comment">// Lock Success</span><br>    SetContents(x, resource_random_value)<br>    reply(ok)<br>&#125;<span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">// Lock failed</span><br>    reply(err)<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="HA-Distributed-Lock"><a href="#HA-Distributed-Lock" class="headerlink" title="HA Distributed Lock"></a>HA Distributed Lock</h3><p>Based on the idea of High Availability, we will have Redis (-liked) storage system. Redis is the typical implementation in the industry. Though it might contradict with the principle idea of distributed lock, some programmers consider this mechanism due to easy implementation, low latency but sacrifice some consistency on the application layer. In which, the application has to ensure that even when there are some mistakes like two process enter the <strong>critical region</strong>, the application won’t reveal some mistakes.</p><h4 id="3-2-1-Redis-Distributed-Lock-Implementation-StandAlone"><a href="#3-2-1-Redis-Distributed-Lock-Implementation-StandAlone" class="headerlink" title="3.2.1 Redis Distributed Lock Implementation (StandAlone)"></a>3.2.1 Redis Distributed Lock Implementation (StandAlone)</h4><ul><li><strong>Acquire Lock</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell">SET resource_key resource_random_value NX PX 30000<br>SETNX<br>EXpire<br></code></pre></td></tr></table></figure><ol><li><p>PX: Add a TTL to the lock: To prevent deadlock when client acquires lock then permanently failover.</p></li><li><p>NX: To ensure mutex in distributed lock which means that only when <code>resource_key</code>  is not exist, the process can acquire the lock.</p></li><li><p><code>resource_random_value</code>: some unique identifier which denotes the owner of the distributed lock to prevent process A wrongly release the lock own by process B.</p></li></ol><p><strong>Counter Example:</strong></p><p>![AcLock](..&#x2F;img&#x2F;tech&#x2F;Distributed Lock&#x2F;AcLock.jpg)</p><ul><li><strong>Release Lock</strong></li></ul><p>The unlock operation is like a CAD (Compare and Delete) command, where the redis should maintain its atomic semantics.</p><ul><li><p>In Redis, we can use LUA script to implement this.</p></li><li><p>In ByteDance Redis, they provide a CAD command for the client to use.</p><ul><li>For details of CAD: Refer to <a href="https://site.bytedance.net/docs/2707/3269/88129/#cad">Kv_Redis_CAD</a></li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell">if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then<br>    return redis.call(&quot;del&quot;,KEYS[1])<br>else<br>    return 0<br>end<br></code></pre></td></tr></table></figure><p>![AcLock](..&#x2F;img&#x2F;tech&#x2F;Distributed Lock&#x2F;RlsLock.jpg)</p><h3 id="Redis-Distributed-Lock-Cluster-Redlock"><a href="#Redis-Distributed-Lock-Cluster-Redlock" class="headerlink" title="Redis Distributed Lock (Cluster + Redlock)"></a>Redis Distributed Lock (Cluster + Redlock)</h3><p>As can see from last part, the problem for <strong>Naive Redis</strong> Distributed Lock is the inconsistency during <strong>Leader Transfering</strong>. There has been a Algorithm, Redlock, proposed by Antires (father of Redis), might be able to handle this problem. The typical flow is described as below:</p><p>Assume that there are five node in a Redis Cluster, in order to acquire lock, the client performs the following operations:</p><ol><li><p>START: Gets the current timestamp.</p></li><li><p>It tries to acquire locks in all the N instances sequentially, using the command described in 3.2.1. The client will set a small timeout compare to the valid time of lock. For example, if the TTL for lock is 10s, then the timeout for client waiting for nodes’ reply may be around 5-50 ms. This prevents client waiting nodes’ reply for so long and when there’s a instance timeout, the client should immediately try next node in Redis Cluster.</p></li><li><p>After step:2 success, the client compute the current timestamp, END and minus START for the time of acquiring lock. If the time is under lock valid time and there are majority nodes reply <code>ok</code> of step:2 THEN</p></li><li><p>If the lock was successfully acquired, its validity time is considered to be the initial validity time minus the time elapsed in step:3.</p></li><li><p>If the client failed to acquire the lock <code>(END-START &gt; Lock valid time || less then majority nodes reply ok)</code>, then the client will try to unlock all the instances.</p></li></ol><p><strong>[Clock Drift]</strong></p><p>The <a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">time</a> in Distributed System is not reliable due to network delay or atomic drifting. So any service that deploys across different server cannot rely on the TRUETIME assumption.</p><p>The <a href="https://redis.io/docs/reference/patterns/distributed-locks/">original redlock post</a> states that the algorithm do not rely on synchronized clock and take clock drift in distributed system into account.</p><p><strong>[Retry on Failure]</strong></p><p>After the client fails to acquire the lock, it should sleep for a while so that other client will not synchronizing acquire lock which might give rise to a split brain (where no client can acquire the lock). Ideally, the client should try to send the SET command using multiplexing.</p><p><strong>[More Reliable]</strong></p><p>The client can extend the lock’s TTL during the computation of lock’s valid time. By sending an extension command with LUA script to all the instances that the key exist on, the client can extend the liveness of the lock, but should be stay within the validity time.</p><p><strong>[Performance, Crash Recovery and Fsync]</strong></p><ul><li>Persistence</li></ul><p>Without persistence, the lock service cannot guarantee that mutex semantic. For example, when client A acquires the lock in 3 in 5 instances and one of the instances restarted, then we will have 3 instances are lock-free at this point.</p><p>AOF: Redis by default will fsync to disk by every seconds, it is possible that after a  restart, the lock key is missing. In theory, if we want to ensure any kind of instance start, we need to enable <code>fsync=always</code> but will affect performance due to DiskIO.</p><ul><li>Delay Restart</li></ul><p>To guarantee that there’s no two instance acquire lock at the same time, we can implement delay restart mechanism in which the shutdown server can only restart after <code>TTL</code>. After it recover, all servers is clean and can be locked.</p><p>But there’s downsides for this strategy when there are too many servers shutdown within <code>TTL</code> so that the lock service is unavailable for at least <code>TTL</code>.</p><p>More Information about Redlock: There’s a famous debate session within this topic between Martin Kleppmann (Distributed System Expert) and <a href="http://antirez.com/">Antirez</a> (Father of Redis).</p><h4 id="Analysis-of-Redlock"><a href="#Analysis-of-Redlock" class="headerlink" title="Analysis of Redlock"></a>Analysis of Redlock</h4><p>Martin Kleppmann <a href="http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">analyzed Redlock here</a>. A counterpoint to this analysis can be <a href="http://antirez.com/news/101">found here</a>.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>Regarding distributed locks, the solutions that are better supported within the company can be distinguished according to high availability and strong consistency scenarios<ul><li>High Availability Scenario: With Redis , businesses need to be compatible with inconsistent events with low probability.</li><li>Strongly consistent scheme: Using ByteKV :<ul><li>For a single cluster across the room, the business needs to tolerate the delay of Raft synchronization across the room.</li><li>For multi-cluster cross-room, the business needs to tolerate that the service cannot write data to the leader of multiple clusters at the same time, otherwise unpredictable errors will occur.</li></ul></li></ul></li></ul><p>Personally speaking, I think a better solution is to align with the overall service structure + business scenario, because different service structures may require different TradeOff, and different business scenarios have different preferences for different scenarios. But overall, through this survey, about a week before and after, I gained some understanding of the company and open source projects, including: ByteKV , ZooKeeper, Chubby, Redis , ETCD ; also let me The concept of distributed locks and business selection TradeOff has more experience.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SegmentQuery</title>
    <link href="/2022/06/06/SegmentQuery/"/>
    <url>/2022/06/06/SegmentQuery/</url>
    
    <content type="html"><![CDATA[<h1 id="区间问题"><a href="#区间问题" class="headerlink" title="区间问题"></a>区间问题</h1><h2 id="QuickView"><a href="#QuickView" class="headerlink" title="QuickView"></a>QuickView</h2><p>读者可以自行思考一个问题：对于一个数组 a，如果我想对 a 的进行以下三种操作，在不使用本文数据结构的情况下，暴力算法的时间复杂度为多少：</p><ul><li><p>单点修改，区间查询：将第x个元素加k，求出 [x, y] 的区间和。</p></li><li><p>区间修改，单点查询：将 [x, y] 区域内的元素加k，并且求出第$ i (x &lt;&#x3D; i &lt;&#x3D; y) $个元素的值。</p></li><li><p>区间修改，区间查询：将 [x1, y1] 区域内的元素加k，并求出 [x2, y2] 的区间和</p></li></ul><h2 id="本文介绍数据结构"><a href="#本文介绍数据结构" class="headerlink" title="本文介绍数据结构"></a>本文介绍数据结构</h2><ul><li><p>差分数组</p></li><li><p>树状数组</p></li><li><p>线段树</p></li></ul><h3 id="差分数组"><a href="#差分数组" class="headerlink" title="差分数组"></a>差分数组</h3><p>定义：假设原数组为: <code>a = int[]&#123;2,5,7,3,6,9&#125;</code>，则差分数组b 的每一项为：<br>$$<br>b(i)&#x3D;\left{ \begin{aligned}  &amp; a[i] - a[i-1], i &gt; 0 \ &amp; a[0], i&#x3D;0 \ \end{aligned} \right.<br>$$<br>如下图所示。</p><table><thead><tr><th>index</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><td>a[i]</td><td>2</td><td>5</td><td>7</td><td>3</td><td>6</td><td>9</td></tr><tr><td>b[i]</td><td>2</td><td>3</td><td>2</td><td>-4</td><td>3</td><td>3</td></tr></tbody></table><h4 id="差分数组的性质"><a href="#差分数组的性质" class="headerlink" title="差分数组的性质"></a>差分数组的性质</h4><ul><li>性质一：</li></ul><p>求原数组中元素 a[i]，相当于求差分数组的前缀和<br>$$<br>a[i] &#x3D; \sum_{j&#x3D;0}^{i}b[j]<br>$$</p><ul><li>性质二：<ul><li><p>求原数组中的前缀和：$\sum_{i&#x3D;0}^{x}a[i] $，代入上式可得：</p></li><li><p>$\sum_{i&#x3D;0}^{x}a[i] &#x3D; \sum_{i&#x3D;0}^{x}\sum_{j&#x3D;0}^{i}b[j] &#x3D; \sum_{i&#x3D;0}^{x} (x-i+1)*b[i] $</p></li><li><p>关于第二个等号，可以举个例子来说明：</p></li><li><p><img src="/img/tech/SegmentQuery/diff_array_eg.png" alt="img"></p></li></ul></li></ul><h4 id="差分数组的用途"><a href="#差分数组的用途" class="headerlink" title="差分数组的用途"></a>差分数组的用途</h4><blockquote><p>区间修改，区间查询</p></blockquote><ol><li>快速处理区间修改操作</li></ol><p>区间修改的操作可以分两步执行，例如对于操作给 [x, y] 区间加 K 这个操作：</p><ul><li><p>b[x] +&#x3D; K</p></li><li><p>b[y+1] -&#x3D; K</p></li></ul><ol><li>快速处理区间查询问题</li></ol><p>由性质二，可以在O(n) 的时间范围内查出原数组的前缀和；对于区间 [L, R] 的和为：</p><p>$SUM_{L, R} &#x3D; SUM[R] - SUM[L-1] $</p><p>对于区间查询的问题，可以通过进一步地维护一个额外的空间来Trade-Off来压缩时间复杂度，感兴趣的读者可以自行了解。</p><h3 id="树状数组"><a href="#树状数组" class="headerlink" title="树状数组"></a>树状数组</h3><p>树状数组 t[x] 的每一个元素保存了以x为根节点的子树中叶子节点值的和，如下图所示。</p><p><img src="/img/tech/SegmentQuery/TreeArray.png" alt="img"></p><p><strong>Lowbit 操作</strong></p><p>Lowbit(x): 表示x在二进制位表示下最低位的1及其之后的0构成的数值。如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Plain">lowbit(44) = lowbit(0b101100) = (0b100) = 4<br></code></pre></td></tr></table></figure><p>对于任意一个数 x 来说，它的lowbit值为：<code>lowbit(x) = x &amp; (-x)</code></p><p><strong>性质</strong></p><ul><li><p>在t[x] 数组中，Lowbit(x) 表示了t[x] 覆盖的元素个数。</p></li><li><p>t[x - lowbit(x)+1] 为 t[x] 覆盖原数组的第一个元素</p></li><li><p>t[x + lowbit(x)] 为 t[x] 在树状数组中的父节点元素</p></li></ul><p><strong>使用场景</strong></p><blockquote><p>单点修改，区间查询</p></blockquote><p>树状数组对于以下场景的计算具备良好的时间效率：</p><ul><li><strong>单点修改，区间查询</strong>，例如：修改 a[i]， 查询 $SUM_{L,R} $</li></ul><p>反应到树状数组中，主要提供两个API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Plain">Ask(x): Return the sum value from 0 to x<br>Add(x, k): Add k to x<br></code></pre></td></tr></table></figure><p>对于Ask(x) 操作，从树状数组 t[x] 开始，往树的根节点方向寻找元素并相加，直到最顶层。伪代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Go">sum := <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i:=x;i&gt;<span class="hljs-number">0</span>;i-=lowbit(i) &#123;<br>    sum += t[i]<br>&#125;<br></code></pre></td></tr></table></figure><p>对于Add (x, K) 操作，从树状数组 t[x] 开始，往树的根节点方向寻找元素，并且把寻找到的每个元素值加 K，直到最顶层。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Go"><span class="hljs-keyword">for</span> i:=x;i&lt;n;i+=lowbit(i)&#123;<br>    t[i] += K<br>&#125;    <br></code></pre></td></tr></table></figure><h3 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h3><p><img src="/img/tech/SegmentQuery/%E7%BA%BF%E6%AE%B5%E6%A0%91.jpg" alt="线段树"><br>线段树解决的问题是：区间修改，区间查询。</p><p>线段树的每个叶子节点对应原数组中的值，每个非叶子节点表示子节点的节点值之和。</p><p>每个节点的数据结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">struct</span> TreeNode&#123;<br>  left, right <span class="hljs-type">int</span> <span class="hljs-comment">// 表示当前节点覆盖的最左和最右节点的下标</span><br>  val <span class="hljs-type">int</span> <span class="hljs-comment">// 表示当前节点的节点值</span><br>  lazy <span class="hljs-type">int</span> <span class="hljs-comment">// lazy标记</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>区间查询</strong>的一般步骤为，自顶向下地递归查询：</p><ol><li>遇到query 区间和当前节点区间不相交的时候直接返回0；</li><li>如果query 区间完全覆盖当前节点的区间，则直接返回当前节点的值；</li><li>如果query 区间和当前节点的区间相交，则向下寻找被完全覆盖的节点并且返回该节点的值。</li></ol><p><strong>区间更新</strong>的操作为</p><blockquote><p>大体包括四个步骤：判断是否覆盖 -&gt; 看是否有lazy标记 -&gt; lazy标记下推 -&gt; 更新左右节点 -&gt; 更新父节点</p></blockquote><ol><li>如果Query 区间完全覆盖当前区间则给当前区间打上Lazy标记后返回，回溯的过程中更新父节点的值；</li><li>如果Query 区间完全覆盖的区间上有lazy标记，则将lazy标记下推，下推的操作是将当前的lazy操作应用到两个子节点上，然后返回，回溯的过程中更新父节点的值；</li></ol><h3 id="使用场景的总结"><a href="#使用场景的总结" class="headerlink" title="使用场景的总结"></a>使用场景的总结</h3><p><img src="/img/tech/SegmentQuery/%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.png" alt="区间问题总结"></p><h3 id="可能帮助你理解的例子"><a href="#可能帮助你理解的例子" class="headerlink" title="可能帮助你理解的例子"></a>可能帮助你理解的例子</h3><h4 id="732-我的日程安排表-III"><a href="#732-我的日程安排表-III" class="headerlink" title="732. 我的日程安排表 III"></a><a href="https://leetcode.cn/problems/my-calendar-iii/">732. 我的日程安排表 III</a></h4><h4 id="1109-航班预订统计"><a href="#1109-航班预订统计" class="headerlink" title="1109. 航班预订统计"></a><a href="https://leetcode.cn/problems/corporate-flight-bookings/">1109. 航班预订统计</a></h4><p><strong>For More Information:</strong> <a href="https://github.com/SharingSource/LogicStack-LeetCode">SharingSource</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
      <tag>High Advanced DS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次面试算法题准备过程</title>
    <link href="/2022/06/03/OnceInterviewPreparation/"/>
    <url>/2022/06/03/OnceInterviewPreparation/</url>
    
    <content type="html"><![CDATA[<h2 id="我面试之前是如何准备算法题的"><a href="#我面试之前是如何准备算法题的" class="headerlink" title="我面试之前是如何准备算法题的"></a>我面试之前是如何准备算法题的</h2><p>先说说笔者自身的情况：</p><ul><li>北航本 &#x2F; 新加坡Top2 硕</li><li>本科（毕业后）的时候先后在腾讯和字节跳动实习 + 转正工作，经历过大大小小的面试总计大概10来场吧，选择性的粘贴一下之前的面经，可以感觉到不同的公司有不同的面试风格:<ul><li><p>阿里支付宝：比较偏重于Java世界，对于New Grad来说八股文的考察比较多，但很基础，电话面试。</p><ul><li><a href="https://www.nowcoder.com/discuss/862658?source_id=profile_create_nctrack&channel=-1">蚂蚁面经</a></li></ul></li><li><p>腾讯CSIG：比较偏重于 Java，特别是Spring框架，会考察应试者的知识广度，包括消息队列等等，但没有涉及分布式的topic。</p></li><li><p>字节跳动：字节的面试风格比较清新，直接了当，上来先写题，写完再聊，聊的东西也比较入门级别（对于NG而言），包括：Redis的基本数据结构，MySQL的并发设计；对于搞过分布式系统&#x2F;分布式存储的同学来说，只要算法没问题，基本秒过。</p><ul><li><a href="https://www.nowcoder.com/discuss/862652?source_id=profile_create_nctrack&channel=-1">Tiktok直播 SG实习</a></li><li><a href="https://www.nowcoder.com/discuss/596218?source_id=profile_create_nctrack&channel=-1">字节基础架构</a></li></ul></li><li><p>商汤：商汤面的<strong>算法岗</strong>，主要是就着Paper 问，只要Paper中的工作是自己做的，把AI框架搞懂了，Pass 应该没啥问题。但是感觉商汤的算法题有点天马行空，事先可能准备不到。</p></li><li><p>美团：美团面的基础架构，主要做AP系统的应该是，可能我给面试官的感觉比较菜？全程没问我分布式的内容，在聊项目，MySQL聊的比较多，Redis分布式锁。</p><ul><li><a href="https://www.nowcoder.com/discuss/865348?source_id=profile_create_nctrack&channel=-1">美团基础架构面经</a></li></ul></li></ul></li></ul><p>回到现实：这篇Blog记录了我第一次在新加坡准备面试的刷题路线，因为感觉论坛中热门的Hot200， Top100基本都刷了好几遍了，并且向UC Berkerly &#x2F; FaceBook 的大佬请教了经验之后，认为分类刷题是一种很好的习惯。准备面试算法，就和准备高中数学期末考试一样，既有广度，也要考察深度，我应对这种考试一般的方式就是分类刷题，每一个类别都需要细致的研究和总结。</p><p>下面这个链接是我的刷题记录。</p><p> <a href="https://github.com/GaryGky/leetcode-update">个人力扣刷题记录：leetcode-update</a></p><p> 最后还想说一点笔者的拙见：【心态要好】其实面试的过程是双向的，应试者没有必要抱着舔狗或者非要进XX公司不可的心态去面试，而应该将面试看做是一个双向了解和沟通的过程，面试官既是在考察应试者的技术能力，应试者也可以通过面试官的提问路线（是否循序渐进，是否能指出错误，沟通是否顺畅）来评估自己是否适合进入该组工作。</p><h3 id="隆重推荐，没收广告费"><a href="#隆重推荐，没收广告费" class="headerlink" title="隆重推荐，没收广告费"></a>隆重推荐，没收广告费</h3><blockquote><p>GitHub整理的LeetCode刷题指南：<a href="https://github.com/youngyangyang04/leetcode-master">https://github.com/youngyangyang04/leetcode-master</a></p></blockquote><p><img src="/img/tech/Algorithm/Category.png" alt="Leetcode Category Distribution"></p><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><blockquote><p>已掌握：双指针 滑动窗口 前缀和 动态规划</p></blockquote><blockquote><p>听说过未掌握：线段树</p></blockquote><h3 id="搜索旋转排序数组"><a href="#搜索旋转排序数组" class="headerlink" title="搜索旋转排序数组"></a>搜索旋转排序数组</h3><p>都需要在 **O(logN)**的时间复杂度内完成，因此思路都是希望每次迭代排除一半的元素。</p><ul><li>寻找旋转排序数组中最小值 (不允许重复)</li></ul><p><strong>解法</strong>：对于数组中最后一个元素，在最小值左边的元素，都严格大于最后一个元素，在最小值右边的元素，都严格小于最后一个元素。基于此发现，可以将中间位置与数组最后一个值比较。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs Swift">int left<span class="hljs-operator">=</span><span class="hljs-number">0</span>, right<span class="hljs-operator">=</span>nums.length<span class="hljs-operator">-</span><span class="hljs-number">1</span>;<br><br><span class="hljs-keyword">while</span>(left<span class="hljs-operator">&lt;</span>right)&#123;<br><br>    int mid <span class="hljs-operator">=</span> (left<span class="hljs-operator">+</span>right)<span class="hljs-operator">/</span><span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] <span class="hljs-operator">&lt;=</span> nums[right])&#123; <br><br>        right<span class="hljs-operator">--</span>; <span class="hljs-comment">// 这里会导致变成O(n)</span><br><br>    &#125;<span class="hljs-keyword">else</span> &#123;<br><br>        left <span class="hljs-operator">=</span> mid<span class="hljs-operator">+</span><span class="hljs-number">1</span>;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> nums[left];<br></code></pre></td></tr></table></figure><ul><li>寻找旋转排序数组中最小值 (允许重复)</li></ul><p>是否允许重复并不会影响搜索的结果，所以解法和上面一道题相同。</p><ul><li>寻找旋转排序数组中是否存在某个值（不允许重复）</li></ul><p>高亮的部分严格控制了区域内的有序性，根据这个有序性，每次排除掉一半的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-type">int</span> left=<span class="hljs-number">0</span>, right=nums.length-<span class="hljs-number">1</span>;<br><br><span class="hljs-type">int</span> last=nums[right];<br><br><br><br><span class="hljs-keyword">while</span>(left&lt;=right)&#123;<br><br>    <span class="hljs-type">int</span> mid=(left+right)/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] == target) <span class="hljs-keyword">return</span> mid;<br><br>    <br><br>    <span class="hljs-keyword">if</span>(nums[mid]&gt;last)&#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&gt;target &amp;&amp; target&gt;last)&#123;<br><br>        <span class="hljs-comment">// 这里一定时严格有序的</span><br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>    <span class="hljs-keyword">else</span> &#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&lt;target &amp;&amp; target&lt;=last)&#123;<br><br>        <span class="hljs-comment">// 这里一定是严格有序的</span><br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><ul><li>寻找旋转排序数组中是否存在某个值（允许重复）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-type">int</span> left=<span class="hljs-number">0</span>, right=nums.length-<span class="hljs-number">1</span>;<br><br><span class="hljs-type">int</span> last=nums[right];<br><br><br><br><span class="hljs-keyword">while</span>(left&lt;=right)&#123;<br><br>    <span class="hljs-type">int</span> mid=(left+right)/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] == target || nums[left]==target || nums[right]==target) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br><br>    <br><br>    <span class="hljs-comment">// 这种情况下无法判断 -&gt; O(N)</span><br><br>    <span class="hljs-keyword">if</span>(nums[mid]==nums[right] &amp;&amp; nums[mid]==nums[left])&#123;<br><br>        right--;<br><br>        left++;<br><br>        <span class="hljs-keyword">continue</span>;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">if</span>(nums[mid]&gt;last)&#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&gt;target &amp;&amp; target&gt;last)&#123;<br><br>        <span class="hljs-comment">// 这里一定时严格有序的</span><br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<span class="hljs-keyword">else</span> &#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&lt;target &amp;&amp; target&lt;=last)&#123;<br><br>        <span class="hljs-comment">// 这里一定是严格有序的</span><br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br></code></pre></td></tr></table></figure><h3 id="删除有序数组重复项"><a href="#删除有序数组重复项" class="headerlink" title="删除有序数组重复项"></a>删除有序数组重复项</h3><blockquote><p>O(N)</p></blockquote><ul><li>不能重复 LC26</li><li>最多只有一个重复 LC80</li></ul><p>思路：以<strong>不能重复</strong>为例，使用快慢指针，slow指针之前的元素都是唯一的，fast之前的元素都被检查过。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Java">如果 nums[slow-<span class="hljs-number">1</span>] = nums[fast]，说明fast这个位置是重复的元素，应该跳过，直接更新fast++<br><br>如果 nums[slow-<span class="hljs-number">1</span>] != nums[fast]，有序性保证了fast位置的元素在slow之前都没有出现过，所以替换nums[slow]=nums[fast], 并且slow++，fast++<br></code></pre></td></tr></table></figure><p>循环做以上逻辑，直到fast超出原始数组长度，结束。此时slow保存了去重后的数组长度。</p><h3 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h3><p>能够求解的问题：</p><ul><li>判断图中的连通分量 LC547</li><li>判断是否能够产生二分图 LC785</li></ul><p>模板：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">UnionFind</span>&#123;<br><br>    <span class="hljs-type">int</span>[] parents;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">UnionFind</span><span class="hljs-params">(<span class="hljs-type">int</span> nodeNum)</span>&#123;<br><br>        parents = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[nodeNum];<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;nodeNum;i++) parents[i]=i;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">find</span><span class="hljs-params">(<span class="hljs-type">int</span> node)</span>&#123;<br><br>        <span class="hljs-keyword">while</span>(parents[node] != node)&#123;<br><br>            node = parents[node];<br><br>        &#125;<br><br>        <span class="hljs-keyword">return</span> node;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">union</span><span class="hljs-params">(<span class="hljs-type">int</span> node1, <span class="hljs-type">int</span> node2)</span>&#123;<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">root1</span> <span class="hljs-operator">=</span> find(node1);<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">root2</span> <span class="hljs-operator">=</span> find(node2);<br><br>        parents[root1]=root2;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isConnected</span><span class="hljs-params">(<span class="hljs-type">int</span> node1, <span class="hljs-type">int</span> node2)</span>&#123;<br><br>        <span class="hljs-keyword">return</span> find(node1) == find(node2);<br><br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>使用注意：</p><ul><li>二维矩阵问题需要转换为一维问题。</li></ul><h3 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a><a href="https://leetcode-cn.com/problems/last-stone-weight-ii/solution/yi-pian-wen-zhang-chi-tou-bei-bao-wen-ti-5lfv/">背包问题</a></h3><p>背包问题的定义是：给定一个背包的容量target，再给定一个数组nums表示物品，能否按照一定的方式选取nums中的元素得到target。</p><blockquote><p>在解决实际问题的时候， 通常是将背包类型和问题类型做笛卡尔乘积，然后选择合适的算法。</p></blockquote><p><strong>按照背包类型进行分组</strong>：</p><table><thead><tr><th>背包类型</th><th>内层遍历顺序</th><th>例题</th></tr></thead><tbody><tr><td>0-1 背包：每个元素只能使用一次</td><td>倒序遍历</td><td>给定背包容量，最多可以拿价值多少的物品 目标和问题 石头最小剩余的重量</td></tr><tr><td>完全背包问题：每个元素可以重复取</td><td>正序遍历</td><td>零钱兑换问题</td></tr><tr><td>组合背包问题：每个元素要求有序</td><td>正序遍历</td><td></td></tr><tr><td>分组背包：多个背包 （没见过）</td><td></td><td></td></tr></tbody></table><p><strong>按照问题的类型进行分组</strong>：</p><table><thead><tr><th>问题类型</th><th>递推公式</th><th>例题</th></tr></thead><tbody><tr><td>组合问题</td><td><code>dp[i]+=dp[i-num] </code></td><td>目标和</td></tr><tr><td>最值问题</td><td><code>dp[i]=min/max(dp[i],dp[i-num]) </code></td><td>剩余最少石头的数量 零钱兑换 完全平方数</td></tr><tr><td>存在性问题</td><td>&#96;dp[i]&#x3D;dp[i]</td><td></td></tr></tbody></table><h3 id="股票买卖"><a href="#股票买卖" class="headerlink" title="股票买卖"></a>股票买卖</h3><blockquote><p>股票买卖是一道用动态规划记录状态转移的问题</p></blockquote><ul><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/">LC121 股票 1</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii">LC122 股票 2</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iii">LC123 股票 3</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iv">LC 188 股票 4</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-cooldown">LC 309 股票 + 冷冻期</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee">LC714 股票 + 手续费</a></li></ul><p>无法复制加载中的内容</p><p><strong>K 次买卖</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> Math.min(k, n/<span class="hljs-number">2</span>);<br><br>    <span class="hljs-type">int</span>[][][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][k+<span class="hljs-number">1</span>][<span class="hljs-number">2</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">// init: 第零天买入</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;=k;i++)&#123;<br><br>        <span class="hljs-comment">// dp[0][i][0] = 0;</span><br><br>        dp[<span class="hljs-number">0</span>][i][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>];<br><br>    &#125;<br><br>    <span class="hljs-comment">// DP</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 在第i天买入</span><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">1</span>;j&lt;=k;j++)&#123;<br><br>            <span class="hljs-comment">// buy：昨天的买入状态 or 昨天卖出今天买入</span><br><br>            dp[i][j][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]-prices[i]);<br><br>            <span class="hljs-comment">// sell: 昨天sell的状态 or 今天买入 然后卖出的状态</span><br><br>            dp[i][j][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">0</span>], dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>]+prices[i]);<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(dp[i][j][<span class="hljs-number">0</span>], ans);<br><br>        &#125;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>N次买卖包含冷冻期</strong></p><blockquote><p>添加一个冷冻期的状态frozen；</p></blockquote><ul><li><blockquote><p>卖出状态的前一天一定是冷冻期</p></blockquote></li><li><blockquote><p>冷冻期前一天可以是冷冻期或者买入状态</p></blockquote></li><li><blockquote><p>买入状态前一天可以是买入或者卖出状态</p></blockquote></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> n/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-type">int</span>[][][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][k+<span class="hljs-number">1</span>][<span class="hljs-number">3</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">// init: 第零天买入</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;=k;i++)&#123;<br><br>        <span class="hljs-comment">// dp[0][i][0] = 0;</span><br><br>        dp[<span class="hljs-number">0</span>][i][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>];<br><br>    &#125;<br><br>    <span class="hljs-comment">// DP</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 在第i天买入</span><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">1</span>;j&lt;=k;j++)&#123;<br><br>            <span class="hljs-comment">// buy：昨天的买入状态 or 昨天卖出今天买入</span><br><br>            dp[i][j][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]-prices[i]);<br><br>            <span class="hljs-comment">// frozen: 昨天是冷冻期 或者 昨天卖出了股票</span><br><br>            dp[i][j][<span class="hljs-number">2</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">2</span>], dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>] + prices[i]);<br><br>            <span class="hljs-comment">// sell: 昨天是冷冻期</span><br><br>            dp[i][j][<span class="hljs-number">0</span>] = dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">2</span>];<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(Math.max(dp[i][j][<span class="hljs-number">0</span>], dp[i][j][<span class="hljs-number">2</span>]), ans);<br><br>        &#125;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>实际上，在进行无限次交易的时候，可以简化一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>        <span class="hljs-type">int</span>[][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][<span class="hljs-number">3</span>];<br><br>        <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <br><br>        <span class="hljs-comment">// init: 第1天状态</span><br><br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = dp[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 卖出和冷冻期都是0 因为不进行操作</span><br><br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>]; <span class="hljs-comment">// 买入是 -price[0] 因为第一天买入</span><br><br>    <br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>            <span class="hljs-comment">// 第i天买入状态：i-1天买入状态 今天不操作 或者 i-1天卖出状态 今天买入</span><br><br>            dp[i][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] - prices[i]); <br><br>            <span class="hljs-comment">// 第i天冷冻期状态 i-1天必是买入状态</span><br><br>            dp[i][<span class="hljs-number">2</span>] = dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + prices[i];<br><br>            <span class="hljs-comment">// 第i天卖出状态 i-1天冷冻期 或者 i-1卖出今天不操作</span><br><br>            dp[i][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]);<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(dp[i][<span class="hljs-number">0</span>], dp[i][<span class="hljs-number">2</span>]);<br><br>        &#125;<br><br>        <span class="hljs-keyword">return</span> ans;<br><br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>N次买卖包含手续费</strong></p><p>除了状态转移方程外，其他与N次买卖相同：假设手续费为：fee</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices, <span class="hljs-type">int</span> fee)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span>[][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][<span class="hljs-number">2</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br><br><br>    <span class="hljs-comment">// init: 第1天状态</span><br><br>    dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 卖出为0 因为不进行操作</span><br><br>    dp[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>]; <span class="hljs-comment">// 买入是 -price[0] 因为第一天买入</span><br><br><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 第i天买入状态：i-1天买入状态 今天不操作 或者 i-1天卖出状态 今天买入</span><br><br>        dp[i][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] - prices[i]); <br><br>        <span class="hljs-comment">// 第i天卖出状态 i-1卖出今天不操作 或者 在今天卖出</span><br><br>        dp[i][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], dp[i][<span class="hljs-number">1</span>] + prices[i] - fee);<br><br>        <span class="hljs-comment">// 记录最大值</span><br><br>        ans = Math.max(dp[i][<span class="hljs-number">0</span>],ans);<br><br>    &#125;<br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="图-amp-树"><a href="#图-amp-树" class="headerlink" title="图 &amp; 树"></a>图 &amp; 树</h2><blockquote><p>树：<strong>连通非循环无向图</strong></p></blockquote><ul><li>搜索算法：BFS，DFS，<strong>拓扑排序</strong></li><li>单元最短路径：Dijkstra，Floyd （要求图中没有环）</li><li>最小生成树：Kruskal 算法</li></ul><h2 id="递归回溯"><a href="#递归回溯" class="headerlink" title="递归回溯"></a>递归回溯</h2><blockquote><p><a href="https://leetcode-cn.com/problems/subsets/solution/c-zong-jie-liao-hui-su-wen-ti-lei-xing-dai-ni-gao-/">力扣</a></p></blockquote><p><strong>DFS和回溯的区别</strong></p><p>DFS是搜索算法，在搜索过程结束之后返回到上一层不会再多执行操作。而回溯算法在返回到上一层之后，会再次进行搜索。</p><p>DFS和回溯最大的区别是：<strong>有无状态重置</strong>。</p><p><strong>何时使用回溯算法</strong></p><p>当问题需要“回头”来找出所有解，即满足结束条件或者发现不是正确路径之后，要撤销选择，回退到上一个状态，继续尝试，直到找出所有解。</p><p><strong>回溯算法的模板</strong></p><ul><li>画出递归树，找到状态变量（写出回溯函数的参数）</li></ul><p>例如，在子集问题中，给定一组不含重复元素的数组，返回所有可能的子集：</p><p><img src="/img/tech/Algorithm/backtrack.png" alt="img"></p><ul><li>根据题意，确立结束条件</li><li>找准选择列表</li><li>判断是否需要剪枝</li><li>做出选择，递归调用，进入下一层</li><li>撤销选择</li></ul><p><strong>回溯问题的类型</strong></p><blockquote><p>回溯问题通常需要找到一个<strong>序列</strong>而不是<strong>总数（种类数），</strong>如果要求总数的话，可以使用背包来优化时间复杂度。因为回溯的时间复杂度是$$O(2^n)$$</p></blockquote><table><thead><tr><th>类型</th><th>题目链接</th></tr></thead><tbody><tr><td>子集、组合</td><td>LC78 子集1 LC90 子集2</td></tr><tr><td>全排列</td><td>LC46 全排列1 Lc47 全排列2</td></tr><tr><td>搜索</td><td>LC 79 单词搜索 N皇后</td></tr></tbody></table><h2 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h2><blockquote><p>深度优先搜索，一种用于<strong>遍历搜索</strong>树或者图的算法，过程是对每一个可能的分支路径深入到不能深入为止，并且每个节点只能访问一次。</p></blockquote><blockquote><p>DFS的发明者在1986年获得图灵奖。</p></blockquote><p>比较经典的例题：</p><ul><li>LC 111 二叉树深度</li><li>LC 98 验证二叉搜索树</li><li>LC 113 路径总和</li></ul><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><blockquote><p>排序算法的任务简单，但是解决问题的思想非常经典。应用在快排、归并排序中的分治思想、递归实现在计算机的世界里有着广泛的应用。</p></blockquote><p><strong>注意：</strong></p><p>很多算法题中不会直接考察排序算法，而是考察排序算法中的思想，比如：快排的Partition操作，计算逆序对和第K大元素等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Leetcode</tag>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spanner</title>
    <link href="/2022/05/31/spanner/"/>
    <url>/2022/05/31/spanner/</url>
    
    <content type="html"><![CDATA[<p>Spanner: Google’s Globally-Distributed Database<br>前言：个人感觉这篇paper组织的逻辑比 Dynamo和GFS 难理解一些，因为时而讲存储引擎层的内容，时而讲存储Service层的内容。笔者按照自己的逻辑重新组织了一下文章，希望能够帮助读者更好的理解Spanner。</p><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnALxkfZlATXInaIF2BcyMUf">Spanner</a>.</p><p>特征</p><ul><li>可扩展的</li><li>基于多版本读 (MVCC) </li><li>基于两阶段锁的写 (2PL)</li><li>同步复制</li><li>提供 Externally Consistent<br>Spanner 简介<br>Spanner是一个基于Paxos的，支持数据分片的全球部署的强一致数据库。<br>即使在极端自然灾害下，Spanner能够对上层应用提供高可用的服务，因为Spanner的数据是跨大陆存储的。F1 Google 是Spanner最初的用户，它为实现HA在美国部署了5个分片。<br>大多数应用对于HA的要求基本是能够容忍1-2个DC断电，根据Paxos算法 (N &#x3D; 2F + 1)，Spanner只需要提供 3-5 个服务节点就可以满足Fault Tolerance的需求。</li></ul><p>Spanner 主要的关注点在于跨机房的数据同步。</p><ul><li>对比Bigtable，Spanner能够提供DB级别更好的支持：<ul><li>支持更加复杂的数据Schema；</li><li>在全球化部署的同时保证强一致性；</li></ul></li><li>对比MegaStore，Spanner提供更为高级的支持有：<ul><li>在支持同步复制的基础之上，提供更高的读写吞吐；<br>此外，Spanner还能为数据提供多版本的读写，每个版本通过一个TimeStamp来标识。旧版本的数据可以通过配置化的GC来回收，释放存储空间。Spanner提供的是类SQL接口，并且支持ACID的事务操作。<br>Spanner作为一个全球部署的数据库，其实考虑了边缘计算的概念，比如：用户可以访问最近的副本来读取数据（数据是强一致的，所以总是能读到最新的数据）；并且能够支持配置每一个DC存储的数据，数据距离用户的距离（提升读速率），多副本之间的距离（提升写速率），以及维护多少个数据副本。<br>Spanner作为一个分布式的数据库，提供了两个非常宝贵的功能：</li></ul></li><li>支持 Externally Consistency (我认为可以理解为 Linearizable)；</li><li>支持全球的用户同一个时刻读到一致的数据。<br>对于如此高级别的一致性提供保障最重要的一个功能是 TimeAPI，Google通过原子钟和卫星授时服务保证每个节点都能获得一个 &lt;10ms 的时间戳，这个时间区间成为 Clock Uncertainty ，服务节点在CU内是需要阻塞请求的。<br>Spanner 架构简介</li><li>Zone：Spanner 是通过Zones来进行管理的，每一个Zone是最小的管理单元，也是数据复制的范围（Replicas只能分布在同一个Zone中）</li><li>ZoneMaster：主要负责与Server交互，每一个Zone会有一个ZoneMaster主管数据的分配以及副本管理。</li><li>Location Proxy：主要负责与Client交互，告知Client SpannerServer的信息。</li><li>UniverseMaster 我理解主要是一个控制台和可视化界面。</li><li>Placement Driver 主要负责数据的迁移和心跳检测。<br>ZoneMaster + Location Proxy  + Placement Driver 几乎等于GFS中Master Server的功能。</li></ul><p>Storage Arch<br>首先每一个Replica中存储的数据是形如：<br>(key:string, timestamp:string) -&gt; value:string</p><p>（存储层）支持的数据结构比较单一，只有String类型；每一个Replica中都存放了成百上千个tablet，每一个tablet中又存放了大量的KV键值对。Tablet是在分布式文件系统Colossus持久化的，通过一种类似于B树的文件形式存储，使用WAL来保证故障恢复。<br>（业务层）每一个Replica都是运行在一个Paxos Server上的，保存相同数据的Replica构成一个Replicas Group，Replica Group会选择一个Leader对外提供一致性的读写服务。<br>对于Leader的任期，在Spanner的版本中会给Leader发一个默认10s 的Lease，在这10s内都会以持有Lease的服务节点为Leader，用于防止Split Brain。<br>Leader上会额外存储两个数据结构：</p><ul><li>Lock Table：用来表示Key Range的上锁状态。2PL：因为Spanner和Bigtable一样都需要满足对于长事务的支持，长事务如果使用MVCC这样的乐观并发控制性能会比较差，因为需要解决MultiVersion的冲突，导致事务不断地 Abort - Retry。所以基于悲观控制的方法来做长事务是一个Trade-Off的选择。</li><li>Transaction Manager：用于协调分布式事务。2PC：直接充当了2PC中的Coordinator，用来发布Prepare 和 Commit 的命令。</li></ul><p>Replication<br>Spanner 数据模型的概念比较 多 &#x2F; 复杂，需要仔细分析。<br>我理解在Spanner 中， Directory 相当于是分布式系统中的 Partition，也等于是数据最小的移动单位。一个Directory 是一系列拥有共同前缀的 Key组成的。<br>Spanner 一个Paxos Group管理一个Tablet，一个Tablet是许多Directory的组合（为了利用空间局部性原理，将经常被同时访问的Key 在物理上就近存储）。<br>从应用层的视角来看，Dir是数据存放的最小单位，APP在写入数据的时候可以通过 Tag的方式指定Dir，这样就能将数据写入到不同的Region中。<br>在Dir内部，还有一个更细粒度的数据存储单元：Fragment，当Dir数据过大的时候，会被拆分为不同的Fragment，不同的Fragment由不同的Paxos Group存储。</p><p>感觉这块内容描述得过于细致了，理解起来有点困难。。</p><p>Data Model<br>Spanner 暴露的接口为：半关系型的数据模型，类SQL接口，事务。Spanner对于事务和半关系型数据模型的支持很大程度是受MegaStore启发的，因为在谷歌内部，APP通常在MegaStore 和 BigTable选型的时候，会因为MegaStore对于跨机房同步的支持是同步，强一致的，而选择MegaStore；即使MegaStore的性能会因为一致性而受到削弱。<br>在对于事务的支持上：Spanner 的作者认为，即使2PC带来的性能损失比较大，但是不能总是让业务在无事务的存储层上编码，建议是业务层应该通过一些方式规避2PC带来的性能瓶颈。BigTable就是因为对cross-row的事务支持不够完善而饱受诟病，Google 内部为了解决BigTable对于事务的缺失，专门开发了一套叫做 Perlocate 的系统。（据笔者了解，PingCAP很多产品 (TiDB &#x2F; TiKV) 仍然使用Perlocate的方案来支持事务。）<br>Spanner暴露给用户的的存储结构是类似传统关系型数据库的，row, columns, SQL-Like Query 等等，但底层的存储却是一种KV的模型：通过每一个Row的PrimaryKey映射到其他Columns。<br>$$(PrimaryKey -&gt; (V1, V2, V3 …))$$<br>并且PrimaryKey是按照某种规则组成的自增唯一Key （索引）。<br>以下为一个用户存储的照片元信息对应的例子，从图中可以观察到 Spanner的接口语言，存储结构。</p><p>TrueTime<br>时间是一个很特殊的物理量，我们通常看到的时间只是时间的观察值，而不是时间的绝对值。</p><p>三个API<br>TT.now() -&gt; int[]: TTInterval: [earliest, latest]<br>TT.after(t) -&gt; bool: true if t has definitely passed<br>TT.before(t) _&gt; bool: true if t has definitely not arrived</p><p>这里需要强调一下的是，在TrueTime这里的授时不是一个确定的时间（与我们通常从电脑&#x2F;手机&#x2F;手表上看到的时间不同）。TT返回的是一个不确定的时间范围。<br>对于一个写事务 w 来说，它提交的时间 无法复制加载中的内容满足：无法复制加载中的内容。</p><p>时间的来源<br>TT 的时间来源于两个方面，GPS和原子钟</p><ul><li>GPS：主要受限于信号传递有可能会失败的影响；</li><li>原子钟：原子震荡可能产生误差，使其与真实的时间相差较大。<br>TrueTime 架构<br>无法复制加载中的内容<br>整体的TrueTime 架构如图所示，每个DataCenter中包含一个TimeMaster 集群用于给Spanner Server提供时间。每个Spanner Server会启动一个Daemon 进程，每隔30s向TimerMaster发起时间 Poll，用于更新本地最新的时间。TimerMaster内部会通过一些补偿机制来调整自己的时间，使得各个机器的时间误差尽可能小，在此不赘述了，感兴趣的话可以参考Google Spanner 白皮书。<br>并发控制 &#x2F;&#x2F; Concurrency Control<br>Spanner为了支持分布式事务，对于不同的事务类型：Read-Write &#x2F; Read-Only 采用了不同的策略；为了实现强一致，在事务隔离级别的基础之上加入了绝对时间，将事务执行的效率大大提升。<br>不得不引出一段经典名言：<br>We believe it is better to have application programers deal with performance problems due to overuseof transactions as bottleneck arise, rather than always coding around the lack of transactions.</li></ul><p>Read Write Transaction</p><ul><li>2PL </li><li>2PC</li><li>Timestamp: Commit Time<br>Read Only Transaction</li><li>Snapshot + TrueTime</li><li>Timestamp: Start Time<br>总结：分布式系统时钟授时方案<br>分布式系统中，主要有两种授时的方式：</li><li>通过网络授时，不同节点之间可能会存在网络延迟；</li><li>通过石英震荡产生时间，但石英震荡会产生较大的误差；<br>所以想要通过物理时钟来构建一个全序（以时间为比较单位）的分布式节点集合是不够的。基于此，谷歌为业界提供了几种解决思路:<br>Spanner: True Time方案，通过给每一个分布式节点配备一个GPS时间校准和原子钟硬件，来对接国际标准时间，并且通过卫星通信的方式进行授时，以及通过集群中的原子钟来提供双保险。<br>集中授时服务TSO：通过集群内一个发号器获取自增长的逻辑时间，为了避免单点故障，这个发号器通常使用Paxos来构成一个Group互相备份。(TiDB 使用的方案)<br>混合逻辑时钟HLC：可以保证同一个进程内部事件的时钟顺序，但是解决不了系统外事件发生的逻辑前后顺序与物理时间前后顺序的一致性，因此做不到: Linearizability and external consistency.</li></ul><p>Reference</p><ul><li>Google F1</li><li>MegaStore</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce</title>
    <link href="/2022/05/22/MapReduce/"/>
    <url>/2022/05/22/MapReduce/</url>
    
    <content type="html"><![CDATA[<p>MapReduce<br>Abstract: A high-level idea of Map-Reduce</p><ul><li>Map<br>用户使用Map函数计算出一系列的中间 K-V pairs</li><li>Reduce<br>Reduce 函数在对所有的中间K-V Pairs进行聚合（Merge）操作<br>假设给我十个数组，让我找出每个数组的第五个元素，那么Map函数的作用就是找到每一个数组的第五个元素，Reduce函数就是将每个数组的第五个元素进行聚合，得到一个新的数组。</li></ul><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnOSf3ldikYI6JOgdn5B6Gac">MapReduce</a></p><p>Introduction<br>Google对于处理大数据做了上百种special-purpose 的优化，这些计算方法用来处理大量的原始数据，比如：文档爬虫，Web日志等；也用来计算各种类型的衍生数据，包括倒排索引，每天最经常出现的请求等等。<br>如果在可接受的时间内完成运算，如何处理并行计算，如何分发数据，如何处理错误…都对系统设计提出了很大的挑战。<br>Google的工程师在函数式编程的Map &amp; Reduce函数中获得了启发，并且将其应用到了大数据计算中，使得大量的计算能够并行而且通过重试来处理fault。<br>Outline</p><ul><li><p>Sec.2: 描述一些基本的编程模型 </p></li><li><p>Sec.3: MapReduce的基本接口实现</p></li><li><p>Sec.4: 优化</p></li><li><p>Sec.5: 实验</p></li><li><p>Sec.6: MapReduce在Google中的应用</p></li><li><p>Sec.7: 未来的发展方向<br>Programing Model<br>Example：统计文件中单词出现的数量</p></li><li><p>Map<br>The map function emits each word plus an associated count of occurrences, 1 in this example.<br>map(string key, string value):<br>  &#x2F;&#x2F; key: document name<br>  &#x2F;&#x2F; value: document content<br>  for word in value:<br>  EmitIntermediate(word, “1”)</p></li><li><p>Reduce<br>The reduce function sums all counts emitted for a particular word.<br>reduce(string key, Iterator values):<br>  &#x2F;&#x2F; key: a word<br>  &#x2F;&#x2F; values: a list of counts<br>  for v in values:<br>  result +&#x3D; ParseInt(v)<br>  Emit(AsString(result))</p></li></ul><p>在概念上用户定义的Map和Reduce函数都有相关联的类型：<br>map(k1,v1) -&gt;list(k2,v2)<br>reduce(k2,list(v2)) -&gt;list(v2)</p><p>Other Example</p><ul><li>Distributed Grep: Map函数将emit一行如果改行能够匹配用户定义的Pattern，Reduce函数将数据拷贝到输出Buffer中。</li><li>Count URLAccess Frequency：类似单词统计，每次遇到URL，Map函数都会emit 1，Reduce函数将结果聚合得到 &lt;URL, total_count&gt;。</li><li>Inverted Index (倒排索引)：Map函数emit (word, document ID), Reduce 函数对word进行聚合，得到 (word, list<Document ID>)<br>Implementation<br>Map-Reduce 整体工作流程</li></ul><p>用户能够决定将Input File分割成多个数据片段分发到不同的机器上进行处理，使用分区函数将Map产生的中间函数分布到不同的分区上进行执行，Reduce的调用也被分布到多台机器上执行，用户可以自定义分区函数和分区数量。<br>任务执行过程</p><ol><li>用户程序将输入文件划分为M块，通常每块的大小为16MB-64MB，然后在Cluster上创建大量的程序副本。（注意是程序副本而不是数据副本）</li><li>其中一个程序是特殊的，作为Master对其他所有的副本程序 (Worker) 发起控制请求；Master总共有M个Map任务和R个Reduce任务将一一分配给空闲的Worker。</li><li>Worker被分配任务后会读取输入文件对应的 split，并执行Map函数，中间结果被保留在内存缓冲区中。</li><li>Worker会将中间结果周期性的写入磁盘，并且按照Partition函数划分为R个区域，并且将文件在磁盘上的位置返回给Master。</li><li>当Reduce Worker收到来自Master的处理请求后，会向Master发来的Location发起RPC调用，读取磁盘的数据。在收到磁盘数据之后，会先对key进行排序，如果内存不足需要使用外部排序。需要排序是因为不同的key会被match到相同的Reduce任务上。</li><li>Reduce Worker会遍历排序后的Key，并将这个Key和它对应的中间值传给Reduce函数，Reduce函数的输出将被追加到所属分区的输出文件。</li><li>当所有的Map&amp;Reduce任务完成后，Master唤醒用户程序，并将结果返回给用户程序。<br>通常在MapReduce任务结束之后，会产生R个文件（每个Reduce Worker产生一个输出文件），通常用户不需要合并这R个文件，而是将他们传给另一个MapReduce函数或者分布式应用对文件进行后处理。</li></ol><p>Fault Tolerance<br>Worker Failure</p><ul><li>Master会向Worker定期发送心跳，如果判定Worker Failure：<ul><li>Complete Map Task or In-Progress Map Task 会被设置为IDLE状态， 并且重新执行。Complete Map Task也要重新执行，因为Worker Failure之后，其磁盘的数据是不可访问的。</li><li>Complete Reduce Task不需要重新执行，因为Reduce的结果是直接写入Output File的，Output File存储在GFS上<br>Master Failure</li></ul></li><li>Master会定期将metadata 进行checkpoint 保存到磁盘上，recover的时候加载最新的checkpoint恢复数据。</li><li>Master如果Failure 会将任务Abort，由Client进行重试。<br>Semantics in the Presence of Failure</li><li>Map 和 Reduce都是deterministic的函数（一致性），他们的输出理论上都是确定的，不论发生任何错误。<br>使用原子提交 (Atomic Commit) 来保证一致性：</li><li>Map：每个Map任务执行完成后，生成R个临时文件，并将临时文件的地址封装为Message发送给Master，如果Master判断这个Message已经收到过，则直接忽略它。</li><li>Reduce：Reduce的原子提交是依赖操作系统的重命名命令实现的 (mv in Linux), 一个Reduce任务执行完成后，会将临时文件重命名为输出文件名。如果同一个Task在多台Reduce Worker上执行，最终只有一个Worker的文件能够被保留。<br>空间局部性<br>在MapReduce提出的年代，网络带宽会成为系统的bottleneck。所以Master会考虑从GFS中寻找距离最近的Replica以减少网络的带宽使用。如果任务失败了，Master会优先在保存有Input File (Split) 数据拷贝的机器上执行Map任务。这样当执行大规模MapReduce任务的时候，大多数的Input data都是从本地读取的。(Worker 会缓存Data)，Master作为GFS的Client，不会缓存Data[1]。<br>备用任务<br>为了防止“木桶效应”，一个Worker的慢执行，导致整个MapReduce任务执行变慢，Master会在一个任务马上完成的时候，调度备用Worker来执行剩下的处于in-progress中的任务。不论是backup Worker还是初始Worker完成了任务，Master都会认可这个任务被完成。<br>应用场景</li><li>分布式Grep</li><li>统计URL访问频率</li><li>构建倒排索引</li><li>分布式排序：分布式MergeSort<br>Related System</li><li>Distributed Storage：GFS</li><li>Batch Processing: Spark</li><li>Stream Processing: Flink</li><li>High Availability: Chubby<br>总结<br>MapReduce的成功归因于以下三点：<br>模型使用起来非常简单，因为开放的API接口隐藏了内部的故障处理和并行机制；<br>大规模的应用场景能够使用MapReduce来处理，比如：机器学习，数据挖掘，搜索推荐；<br>谷歌的工程师们提供了一套完整的实现，使其能够应用于大规模的机器集群上。</li></ul><p>经验<br>编程模型的约束使得任务的并行和计算，故障容错变得更加容易，在MapReduce中用户只需要定义Map函数，Reduce函数和一些超参就能完成对任务的执行。<br>网络带宽是宝贵资源，（现在都光纤了，应该不存在这个问题了吧）。<br>冗余的处理能够一定程度上避免木桶效应。</p><p>局限<br>历史局限，当时的背景下单机的性能远远不足，所以没有考虑使用内存进行更高效的计算。<br>没有将资源调度和计算调度分离。在后续的Hadoop生态中，MapReduce只关注与计算而资源调度由Yarn进行管理。</p><p>Reference<br>[1] Google File System<br>[2] <a href="https://research.google.com/archive/mapreduce-osdi04.pdf">https://research.google.com/archive/mapreduce-osdi04.pdf</a><br>[3]<br><a href="https://tanxinyu.work/mapreduce-thesis/">https://tanxinyu.work/mapreduce-thesis/</a><br>[4] 正在实现的：<br><a href="https://github.com/GaryGky/MIT-6.824/tree/lab1-mr/src/mr">https://github.com/GaryGky/MIT-6.824/tree/lab1-mr/src/mr</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Google</tag>
      
      <tag>Distribtued System</tag>
      
      <tag>Distributed Computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Twitter 缓存应用分析</title>
    <link href="/2022/05/22/Twitter-%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90/"/>
    <url>/2022/05/22/Twitter-%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>Twitter 缓存应用分析</p><p>为了更好的阅读体验，请移步至 <a href="https://lo845xqmx7.feishu.cn/docs/doccn329gaovix2csddT3FXFzvb">飞书文档</a></p><p>Abstract</p><ul><li><p>TTL 缓存过期时间的设置十分重要。</p></li><li><p>FIFO 在workload非常大的时候工作得更好（至少和LRU性能差不多，但是理解起来会容易很多）。<br>1 Introduction<br>In Memory-Cache: </p></li><li><p>Redis</p></li><li><p>MemCache<br>将研究者的注意力吸引到：缓存命中率，吞吐量和减小延迟。目前的评估方法是存在缺陷的，大多数是基于：Storage Caching Trace，KV Database Benchmark。但是现在的评估方法都没有将Object Size考虑进去，然而Object Size是影响缓存命中率和吞吐量的重要因子。<br>本文基于100个Twitter缓存集群的模拟和分析，提出了以下几个重要观点：</p></li><li><p>In-memory 缓存并不总是服务于Read-Heavy的场景，有很多缓存也是服务于Write-Heavy的场景的。</p></li><li><p>TTL 会影响工作集的大小所以必须将TTL考虑到缓存的设置中，有效的清除缓存应该优先与缓存淘汰策略。</p></li><li><p>Slab-Based 的缓存比如：Memcache，会受到对象大小分布不均的影响；并且瞬时增大，或者周期性变化的workload，也会对缓存造成影响。</p></li><li><p>根据Zipfian（奇夫定律），访问量最大的key通常是访问量第二大的key的两倍，这在写吞吐较大的系统中会出现明显的倾斜。</p></li><li><p>在合理的缓存空间中，FIFO和LRU在数据量很大的情况下性能差异不明显，LRU在小数据集中的性能更好。<br>2 Twitter 体系中使用的In-Memory 缓存<br>2.1 服务架构和缓存<br>2011 年， Twitter开始了向微服务和容器化迁移的进程，其中缓存作为一项重要的基础架构，随着DRAM的容量扩大，也随之发展。<br>在Twitter中，主要有两种常用的业务缓存组件：Twemcache 和 Nightawk，前者是基于Memcache构建的缓存，具备低延迟、高吞吐的特点；后者是基于Redis构建的缓存，具有更丰富的数据结构，主备提供了高可用的服务。<br>2.2 Twemcache Overview<br>Twemcache fork了Memcache早期的版本，加入了Twitter自身的一些改进。</p></li><li><p>Slab-Based 内存管理<br>传统对于缓存对象的分配策略是使用 <code>heap memory allocators</code>，比如：ptmalloc，jemalloc；这些分配策略会产生“无限大”的外部碎片，而Twitter缓存对象的大小从 若干字节到 10s KB不等，传统分配策略不适用与在小容量容器中使用。<br>基于Slab的分配方式如上图所示，内存会被分为若干等级的Slab，每一个Slab内部又会将内存划分为大小相同的item；Slab的等级越高，表示每一个对象占用的内存越多，每一个缓存对象会被分配到大小最合适的Slab中。Slab-Based 内存分配策略防止了外部碎片，将内存碎片局限在一个item当中。</p></li><li><p>Slab-Based 淘汰策略<br>当Twemcache接收到一个新的缓存对象obj时，首先会查找对应的slab-class是否存在合适大小的块，如果存在的话会直接分配给obj，否则会创建一个更大的slab-class。当内存不足时，需要淘汰掉一些内存中的内容。<br>Memcache 的淘汰策略基本都是基于<code>item</code>这个粒度去做的，通常使用LRU算法对内存中的缓存对象进行淘汰。这么做在 Key Size稳定的情况下没有问题，但是如果 Key Size 随着使用不断增大，后来的大对象将没有位置存储，这种情形被称为：Slab-Calcification.<br>为了解决上述问题，Twemcache使用了一种基于Slab的内存淘汰策略，包括 Slab-Random, Slab-LRU, Slab-LRC。<br>2.3 Cache 使用场景<br>Twitter使用缓存的场景可以概括为三类：存储、计算和短暂的数据。</p></li></ul><p>2.3.1 Cache for Storage<br>最常见最常用的场景，因为后台的数据库延迟高、带宽小。<br>该领域一些重要的研究内容包括：提升缓存命中率，重新设计一个更密集的存储设备来适应更大的工作集，增强负载均衡，增加写吞吐。<br>如上图所示，该缓存集群在整个业务缓存中使用的比例虽然只有30%，但是涵盖了大多数的请求，占用了大多数的内存。<br>2.3.2 Cache for Computation<br>实时流计算使用较多<br>包括一些深度学习框架中也十分常见（PyTorch &#x2F; TensorFlow）<br>跟机器学习系统相关的场景接触的可能会比较多，该场景主要是缓存一些模型计算的中间变量，包括：特征(Feature), 和预测结果(Prediction)。<br>2.3.3 Cache for Transient Data<br>用来存储一些瞬态的数据，不落库的数据。<br>在该场景下可能会有一些数据丢失，但是Twitter的开发人员在这种高速访问和data loss做过trade-off，保留了该场景的使用。<br>一些典型的例子：</p><ul><li>Rate Limiter: 用来记录某个用户访问请求的次数，通常用于防止DoS攻击；<code>(UserID, Cap)</code></li><li>Deduplication Cache：用于去重。<code>(Key, Cap=1)</code><br>3 评估方法<br>收集分析日志<br>Twemcache 每一个集群都有一个<code>built-in</code> 的非阻塞日志系统：klog. klog的默认设置是每一百条请求生成一个日志，为了保证分析的覆盖性：研究者手动将采样率调整为100%，每一个集群中采样两个缓存实例进行分析。<br>作者从153个Cluster中的306个实例收集了 80TB的日志数据，每一个Cluster的QPS为1000。为了方便分析，作者筛选了54个流量最高的集群，他们处理了90%的QPS以及76%的内存。<br>3.1 缓存命中率 or Miss Ratio</li></ul><p>Miss Ratio<br>图a 展示了10个缓存缺失率最高的缓存服务，8 &#x2F; 10的缓存缺失率在5%以下，6 &#x2F; 10的缓存缺失率在1%以下。唯一的例外是最上面的点，它的缓存缺失率高达70%，它是一个write-heavy的缓存，相比其他的CDN缓存，明显缺失率要高很多。<br>Miss Ratio Stability<br>除了缓存缺失率，缓存缺失率的稳定性也十分重要，因为对于一个后端系统来说，往往是最高的缓存缺失率决定了系统需要提供的QPS能力，缓存缺失率稳定性定义为：$$\frac{mr_{max}}{mr_{min}}$$。很多时候一个稳定性更高的缓存服务比一个不稳定的缓存更受青睐。从图b可以看到，缓存不稳定性越高的服务，它的缓存命中率更低，说明后端系统对于Corner case的考虑不足，导致系统偶尔会出现一些spike。<br>3.2 请求率和Hot-Keys</p><ul><li><p>蓝线：请求量；</p></li><li><p>红线：访问的object数量；<br>大多数时候对于spike的理解是hotkey导致的，但是系统并不总是这种情况，因为从右图可以看到在request出现spike的时候，object 访问量也会上升，说明不是因为热key引起。某些可能的原因是：客户端重试，网络拥塞，scan-like请求，周期性任务。<br>3.3 Operation on Cache<br>Write Heavy Cache：写操作超过30% 的workloads。</p></li><li><p>从图b可以看出，超过35%的缓存集群都是write-heavy的，并且有超过20% 的缓存集群的写操作超过50%。<br>一些write-heavy的场景：</p></li><li><p>频繁更新的数据：机器学习的场景，用缓存保存一些不需要持久化的数据，所有的计算和更新都是在缓存上操作的。</p></li><li><p>预加载的数据：记录用户行为的场景，很多服务都是懒加载的，但在用户行为分析的场景中，根据局部性原理，可能会将用户的信息一次性加载到缓存中。但这些数据又不是可复用的（一个用户看完之后就应该要删除了），所以这些预加载的数据会被频繁更新。</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>Twitter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VM-FT</title>
    <link href="/2022/05/22/VM-FT/"/>
    <url>/2022/05/22/VM-FT/</url>
    
    <content type="html"><![CDATA[<p>VM-FT<br>本文是由VMWare于2010 年提出的一个虚拟机容错解决方案，使用的是一个Primary-BackUp的架构。</p><p>推荐阅读：为了更好的阅读体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccny051ESMjndx9mYC7wpg0cc">VM-FT</a></p><p>前言<br>在听取了MIT6.824的课程以及阅读了VM-FT的paper之后，笔者个人感觉要让VM级别的应用实现Fault Tolerance的难度会比DB (In-Memory &#x2F; Disk) 级别的应用大不少，主要是由于VM级别的机器需要关注更底层信息的一些复制，包括中断，异常等。<br>本文介绍方法的大致思想是使BackUp保留一份Primary的状态，当Primary出现故障的时候BackUP能够平滑接替Primary的功能，并且是对用户是透明的。<br>为了实现这个目标，首先在论文Intro部分介绍了全量复制和增量复制两种方法：</p><ul><li>State Transfer：全量地将所有信息，包括（CPU，Disk，I&#x2F;O）的变化都复制给BackUP，这种brutal force的方法简单粗暴，但是对网络资源的开销太大；试想一个数据库更新一行需要将整个数据库在网络上传输一次，这显然是不现实的。</li><li>Replicated State Machine: 将服务器抽象为确定状态机，让Primary和BackUP接受相同的Client Request，这种方法实现复杂，但是对网络带宽的占用很小，适合长距离的传输。<br>架构设计<br>暂时无法在文档外展示此内容<br>Component</li><li>VMM 是该文章复制的Node主体，他是建立在Hypervisor上的一个应用，运行在虚拟机中的操作系统是GuestOS，主机上的操作系统是HostOS。</li><li>Primary 作为一个逻辑概念，标识一台VMM节点，是Client唯一能够感知到的机器。所有的网络输入或者其他输入（磁盘、鼠标、键盘）都会进入Primary VM</li><li>Primary VM接收的所有输入都通过logging channel转发到BackUp进行同步，以保证两者的状态相同。Backup VM指令的结果与Primary相同，但是backup的返回结果会被hypervisor丢弃。<br>Deterministic Replay<br>对于一些不确定的指令或操作：</li><li>随机数生成指令</li><li><code>Time.Now</code>这样的获取时间戳指令</li><li>时钟中断指令</li><li>多核CPU之间的并发执行指令<br>对于不确定的操作，VM-FT中会添加一些额外的信息来保证BackUP对于这些操作的执行结果是与Primary一致的。VM-FT中会添加不确定的指令的执行结果，在backup中replay这些指令的结果。<br>一个logEntry大概需要包含以下内容：<br>Interupt Number &#x2F;&#x2F; 事件发生时的指令号<br>Type &#x2F;&#x2F; 事件类型：网络输入 &#x2F; 其他输入<br>Data &#x2F;&#x2F; 数据，对于不确定指令，此数据应该是Primary的执行结果</li></ul><p>Bressoud &amp; Schneider 提出使用了batch update的方式在Primary和Backup之间进行同步，他们提出将执行序列划分成若干个epoch，然后每次Primary-backup之间同步一个epoch的内容，来减少信息传递的次数。<br>FM Protocol</p><ul><li>输出要求<br>当Primary failover之后，Backup要能够立即接管Primary的任务，并且保证自身的信息和Primary是强一致的。</li><li>输出规则<br>Primary 在发出同步信息之后会等待Backup返回ACK之后，才把Output返回给Client。</li></ul><p>VM-FT不能保证<code>Exactly Once</code>语义，因为Primary在返回给Client Output之后发生了故障，backup在takeover之后也向Client发送故障，此时Client有可能会收到两个重复的数据包。<br>paper中说这个重复的数据包是通过TCP可靠传输来保证不重复的，但是笔者认为在此基础上，Client应该保证自身的幂等。</p><p>VM-FT和GFS的 Fault Tolerance</p><ul><li>VM-FT备份的是计算，它对于一致性要求的实现会更加复杂一些，主要体现在对于不确定性指令操作的处理上。</li><li>GFS备份的是存储，所以GFS的备份策略会比VM-FT更加高效和简单。<br>Reference</li><li>Paper</li><li>MIT 6.824</li><li>NUS CS5223 Lab-2</li><li>Chapter 5 - Replication</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>Distributed Storage</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Partition</title>
    <link href="/2022/05/22/Partition/"/>
    <url>/2022/05/22/Partition/</url>
    
    <content type="html"><![CDATA[<p>Partition的目标是用来提升读写的请求吞吐的，每一个Partition保留的整个数据集的一部分，针对不同数据的请求会被路由到不同的Partition中</p><ul><li>MongoDB，ES，SolrCloud中的Shard</li><li>HBase中的 Region</li><li>BigTable中的 Tablet</li><li>Cassandra和Riak中的 VNode</li><li>Couchbase中的 vBucket<br>1 Partition &amp; Replication<br>分区和复制通常是结合使用的，每一条记录落在特定的分区中，但是分区中有若干个Node形成一个Cluster，提升可用性 (Fault-Tolerance)。<br>Leader-Follower 架构的复制 加上 Partition 构成的整体架构如下图所示：<br>可以将每一个Partition理解为一个Database，Partition中的Replica可以分布在不同的Server Node上。</li></ul><p>推荐阅读：为了更好的阅读体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccn7ps0hMiJoJIw4DmRWn4HUd">Partition</a></p><p>2 Partitioning of KV Data<br>理想状态下：如果将一个完整的数据集分为10个子集，那么吞吐量理论上来说应该是划分前的十倍。如果划分的数据倾斜（Skew），在极端情况下，一台机器可能需要承担所有的数据请求（这种机器叫做HotSpot），此时读写瓶颈仍然收敛到单台机器的性能上，分区失去了意义。<br>最简单的分区策略是随机分配（Random），能够将数据均匀的分布到每一个Partition上，但是如果需要Read，则必须遍历所有的Partition，找到对应的Key之后才能读Value。事实上，有比随机分配更好的策略。<br>2.1 Partition By Key Range<br>将一段连续的Key划分到一个分区中，如果能够知道待查询key属于的范围，就能够确定key的分区；确定了key所在的分区之后，就可以直接向分区所在的Node发起请求。<br>直接按照字典序对Key进行分区的话，可能会造成数据倾斜，比如在下图的分区1中，包含的是a-b开头的key，而在分区12中包含的是t-z的key；假设a-b开头的单词大于t-z开头的单词，就会导致分区1中的数据量大于分区12中的数据量。因此，为了让数据更均匀的分布，需要让分区的边界能够根据Data进行自适应。<br>分区的边界设置通常由两种方法：</p><ul><li>DBA手动设置；</li><li>由DataBase自动设置（Reparation）；<br>应用实例：<br>包括：HBase, RethinkDB, MongoDB, BigTable。</li></ul><p>在每一个分区中，可以按序组织Key（SSTables、LSM-Trees），这样有利于范围查询。但是在某些场景下，存在一个弊端，比如：存储网络传感器数据，按照TimeStamp对Key进行排序，那么在追加写的时候，可能只会写当天的数据，那么保存当天数据的Partition就会成为HotSpot，而其他的Partition都是IDLE状态。<br>为了防止这种情况，可以考虑使用其他的Key作为“主键”，比如：按照传感器的ID进行分片，查询的时候，按照不同的传感器进行范围查询，然后聚合。<br>2.2 Partition By Key Hash</p><p>可以使用Hash函数对上面的String Key Range进行优化，使用Hash函数将一个String映射为一个无符号整型，然后将整数划分为不同的Partition。MongoDB，Cassandra 使用MD5作为哈希函数，Voldemort使用Fowler-Noll-Vo函数作为哈希函数。Java的hashcode和Rust的 Object#hash有一个问题：在不同的进程下返回的结果不同。<br>使用hash 做了散列之后，丢失了原本按照字典序划分的Partition的天然排序性，也即使用了Hash散列之后，Partition不再高效地支持范围查询了。在MongoDB中，所有的范围查询会被发送给所有Partition，Riak，CouchBase和Voldemort都不支持按照主键进行查询。<br>Cassandra中使用了一种复合主键 primary_key &#x3D; (key1, key2, key3)，第一项用于分片，其他几项用于索引来对数据进行排序（SSTable）。针对主键key1的范围查询是不支持的（因为分布在不同的shard上），但是如果指定了key1，就能定位到一个分片，然后根据其他列就能够支持范围查询。<br>2.3 Skewed Workloads &amp; Relieve Hot Spots<br>极端情况下的数据不平衡<br>实际中的场景：某一个大V的动作或者评论，可能引起很多粉丝去访问同一个ID（Action或者User），这个时候即使对ID做了Hash处理，但因为同一个ID的哈希值是一样的，所以粉丝们大量的请求还是会打到同一个Partition上。<br>解决：使用一些先验。<br>如果我们事先知道一些key可能会非常hot，我们可以在Key的前后添加一些随机值，只需要两个decimal<br>Random number就能让数据分布到100多个分片上。（将同一个Key的数据分布到不同的Partition上。）<br>3 Partitioning And Secondary Indexes<br>讲的主要是ES模型的分片原理<br>次级索引通常不是唯一定义一条元素的标识，DDIA 中指出次级索引是关系型数据库的bread and butter，许多KV数据库 (HBase, Voldemort) 没有实现次级索引，因为这会极大地增加实现复杂度。但有一些NoSQL如（Riak）开始使用次级索引，因为次级索引更有利于数据建模。次级索引的存在是为了支持更复杂的查询请求，它也是<code>Solr and ElasticSearch</code>引擎存在的意义。<br>针对次级索引的分片可以分为两种类型：</p><ul><li>基于document分片</li><li>基于term分片<br>3.1 Partitioning Secondary Key By Document<br>根据主键分片，次级索引在不同的Shard中是独立存在的，即每条次级索引只会关联到该Shard中的数据，而不会跨Shard关联。基于主键分片的次级索引的组织方式也称为：<code>Lcoal Index</code>。</li></ul><p>这种方式存在一个问题，如果仅根据颜色或者制造商进行查询，而不知道数据的主键，则需要对所有分片进行查询然后聚合，这种方式叫做: Scatter&#x2F;Gather（可以使用Map-Reduce）。<br>应用实例：<br>MongoDB, Riak, Cassandra, ElasticSearch, SolrCloud, VoltDB.<br>3.2 Partitioning Secondary Key By Term<br>按照次级索引进行分片，比如：color按照字典序划分：</p><ul><li><code>a-r</code>在第一个分区</li><li><code>s-z</code>在第二个分区<br>这种组织次级索引分片的方式叫做<code>term-based</code>，term这个名词来源于全文索引，其中的term是text中的每一个单词。term-based的方式更适合用于进行某些有实际意义的范围查询，因为主键通常是没有语义信息的，比如：想寻找价格在 (x, y) 范围内的车辆，如果希望流量在不同节点中分布的更加均匀，则可以使用Hash对term进行处理。</li></ul><p>Term-based 的组织方式有利于多读写少的场景，因为在读的时候，Client能够根据term定位到一个分片；然而在处理写请求的时候，可能需要涉及多个分片，这时候需要开启分布式事务，但大多数分布式数据库都不支持分布式事务。<br>在实际应用中，term-based 索引的更新通常都是异步地，如果对于一条索引的读请求距离上一次写请求很短，是有可能读到旧数据的。例如：Amazon-DynamoDB官方声明了他们global-index的更新会存在秒级延迟，如果基架出问题了则可能延迟更长的时间。</p><p>4 Rebalancing Partitions<br>Rebalancing (重分区 &#x2F; 重平衡)：指的是数据和请求从一个服务节点（Replicas）移动到另一个服务节点上的过程。</p><p>触发重分区的时机</p><ul><li>查询量上涨，所以需要更多的CPU来处理请求。</li><li>数据大小上涨，需要更多的Disk 和 RAM存储空间。</li><li>一个节点挂了，需要拉起另一个保存了数据副本的节点继续服务。<br>重平衡的目标<br>这三点基本在Implementation部分的设计文档中都实现了，负载均衡使用的是Hash，为了保证第三点，使用了多次平均算法。</li><li>保证负载均衡。假设重平衡之后出现了HotSpot，极端情况下，所有请求都往HotSpot打，HotSpot会被请求撑爆，然后进行重平衡，又出现HotSpot……一台机器一台机器被打挂，导致整个Cluster不可用。</li><li>在重平衡的过程中，仍然需要支持读写请求。</li><li>尽可能少的移动数据，使用多次平均算法来保证尽可能少的移动数据。<br>High-Level 思想是每次寻找包含Shard最多的节点Max和包含Shard最少的节点Min，然后从Max节点找一个Shard移动给Min节点。<br>while (true) {<br>  int source &#x3D; getGidWithMaximumShards(gID2ShardIDs);<br>  int target &#x3D; getGidWithMinimumShards(gID2ShardIDs);<br>  if (source !&#x3D; 0 &amp;&amp; gID2ShardIDs.get(source).size() - gID2ShardIDs.get(target).size() &lt;&#x3D; 1) {<br>  break;<br>  }<br>  Integer swapShard &#x3D; gID2ShardIDs.get(source).stream().findAny().orElse(null);<br>  gID2ShardIDs.get(source).remove(swapShard);<br>  gID2ShardIDs.get(target).add(swapShard);<br>}</li></ul><p>4.1 重平衡算法<br>4.1.1 Mod N<br>最容易想到的重平衡算法是使用Mod N算法，但是在这个场景下非常不推荐使用Mod N算法，因为Mod N算法在N改变的时候，大多数的Key都需要移动，带来的开销巨大。<br>所以一般都是将key的哈希值分配到一个范围中，将某一个范围分配给一个节点存储。</p><p>上图这种情况下，n从三增加到了四，需要移动两个节点，读者可以将k设置得更大，可以观察到这种 Mod N的算法，会在重平衡的时候移动很多数据。<br>对比之下，将key按照Hash值进行范围映射的方法，也是我们熟知的Consistency Hashing。</p><p>所以，经过以上分析，几乎不会有infra产品选择Mod N这种重平衡算法。<br>4.1.2 Fixed Partition Number<br>思路：在数据库创建的时候，就将数据库划分为N个Partition 并且之后不会再改变，当有新节点加入的时候，每一个老节点都会分配一定数量的Partition给它。当有一个节点failover的时候，它的Partition会被均匀的分配给仍然存活的Node。<br>应用案例：Riak, ElasticSearch, CouchBase, Voldemort都使用了这种重平衡方法。以及在我自己实现的KV Store中也是使用的这种方式，Partition的大小一开始就被设置为10，随着Server的进出，重新分配Partition 即可。</p><p>有什么问题？<br>Partition的数量不能设置的过大，也不能设置得过小。如果Partition数量设置得过小，未来随着数据增长，Partition也会随之膨胀。如果Partition数量设置得过大，那么ShardMaster 就需要维护更多的元数据信息，效率不是很高。<br>事实上，选择合适的PartitionNumber是一件十分困难的事情，因为数据量会根据业务的变化而变化（即：数据量是不固定的）。如果一个Partition承载了过多的数据，那么故障恢复和重平衡的成本将会很大（全量复制一个大Partition）。如果Partition设置得过小，有点不划算，类似用牛刀杀鸡的感觉。<br>4.1.3 Dynamic Partitioning<br>Fixed Partition Number对于使用Key Range (refer 2.1)进行分片的数据库是非常不友好的，因为某一个范围的Key只能被分配到特定的Partition上，在刚开始的时候0-100,000都被分配到Partition-1上，此时Partition-1满负载运行而其他Partition几乎没有流量。</p><p>思路：动态扩展和合并分区，当分区大小超过设定阈值的时候进行分裂，小于设定阈值的时候进行合并。（有点类似B树）。HBase 和 RethinkDB使用的是这种分区策略。<br>对于Fixed Partition Number出现的问题，HBase 和 MongoDB使用了一种叫做pre-splitting的策略对其进行划分，即将一部分key set分配到idle的机器上，但这种操作需要DBA对数据的分布有一定的先验。<br>4.1.4 Partition proportionally to Nodes<br>4.1.2的Partition 大小 正比于数据集大小<br>4.1.3的Partition 数量 正比于数据集的大小</p><p>Cassandra 和 Ketama 提出了另一种分区方式，让Partition的数量正比于Server Node的数量，即：每个Server Node上保存X个Partition，Server Node增加的时候Partition也增加，反之减少。<br>这个算法的神奇之处在于，如果Server Node不增加，那么Partition的大小会随着数据量的增加而增加，在DBA手动扩容之后，每个Partition的大小又会下降到平稳的水平。整体来看是结合了4.1.2和4.1.3的优势，并且能够使CPU和存储资源保持稳定的一种算法。<br>Cassandra中设定每个服务节点上拥有256个Partition，当一个新的服务节点加入后，会随机选择256个Partition进行分裂，然后由新节点管理分裂出来的Partition数据。这种方式要求Key 是按照Hash的方式进行分片的，整体的算法有点类似Consistency Hashing。<br>4.2 Automatic &#x2F; Manual Rebalancing<br>关于手动重平衡或是自动重平衡的问题，在我们系统中使用的是Manual 扩容的方式，可能考虑到重平衡是一项比较危险的操作，因为有可能一台HotSpot的Delay时间突然增加，此时其他节点将其判定为Dead并开启重平衡，这时候网络上会因为有更多的数据流动而变得更加拥塞。（TCP拥塞避免就是这么来的）。<br>所以笔者认为Rebalancing的操作是一定需要人工卡点的，即使系统自动判断应该进行重平衡，也应该加入人工校验的流程，比如：不能在直播的高峰期对Partition进行重平衡，否则整个系统的稳定性和可用性都会受到冲击。<br>4.3 Requesting Routing<br>问题是重平衡之后，Client怎么知道我的请求应该往哪个Socket上发呢？<br>服务发现：不仅是DB系统，基本上任何Software Application后台都需要服务发现的功能，以此来保证系统的高可用。基本上大厂都会有自己的服务发现系统，比如TikTok的Consul，阿里的Dubbo (? Not quite sure).</p><p>三种服务发现的方式：</p><ul><li>允许Client访问任一Node（Round Robin），如果路由到的节点能处理请求则直接处理，否则将Client的请求进行转发。</li><li>添加一层Routing Tier，来决定Client的请求应该发送给哪个Node</li><li>Client感知Partition信息，并且直接向对应的Node发起请求。</li></ul><p>问题是Node之间的一致性 -&gt; Distributed Consensus。</p><ul><li><p>一种方法是使用一个分布式协调服务来管理元数据信息，向Kafka，HBase，SolrCloud使用ZooKeeper。</p></li><li><p>另一种方式是在Cassandra 和 Riak中使用的gossip protocal，每次检测到集群状态变化的时候就进行广播，然后Client按照方式1进行请求。</p></li><li><p>CouchDB使用了方式2来进行请求路由，Routing Tier的实现组件为Moxi。<br>Conclusion<br>Partition 的目标是让Data和查询请求在不同的机器上更加均匀的分布，避免HotSpot的出现。这对系统设计提出的要求是：</p></li><li><p>选择合适的 Partition Schema：By Range, By Hashing. </p><ul><li>By Range:<br>当Key是按照某种顺序排序的时候，可以按照Key Range进行Partition，1-100映射到Partition-1,101-200 映射到Partition-2。但是By Range这种方式可能会产生HotSpot，假设高热主播都是注册较早的用户，他们都被划分在Partition-1中，当他们同时开启直播的时候，Partition-1 的请求量暴增，而其他的Server Node基本都处于Cold的状态，显然是不均衡的。</li><li>By Hashing:<br>先将Key进行一次Hash，将Hash值按照Range进行分区，这样做虽然破坏了有序性，但是能够使数据更加均匀的分布。<br>还可以使用以上两种分区方式并用的方式，按照某个Key Hashing进行分区，分区内按照另一个Key进行排序。<br>比如在我们Global Live的系统中，通常是按照User-ID进行分片，然后每个分区内，Primary-Key是有序的。</li></ul></li><li><p>Rebalancing Partition.</p><ul><li>Mod N：一般不用。将Key Hashing分配给固定的节点。</li><li>Fixed Partition Number：一开始设置好分片的大小，在使用过程中不可改变。</li><li>Dynamic Partitioning：当分区大小扩张当一定程度的时候进行分裂，当分区大小小于一定值的时候进行合并。</li><li>Partition Proportionally to Nodes: 每个Node管理固定数量的Partition。<br>Partition and Secondary Key<br>分区和次级索引的关系</li></ul></li><li><p>按照主键分区，次级索引伴随主键进行分片；适合写多的次级索引。</p></li><li><p>按照次级索引分区。适合读多的次级索引。<br>Implemention<br>笔者也按照NUS 5223的Syllabus实现了一个Sharded KV Store，能够支持分片在不同的Replicas之间移动，负载均衡算法使用的是普通Hash。设计文档可以参考：<br>Sharded Transactional KV 设计文档</p></li></ul><p>Some Question<br>看了一下Reference Title，抛出几个问题：<br>Java’s HashCode is not Safe for Distributed System. Why?<br>3% of Twitter’s Servers Dedicated to Justin Biber. Any Strategy to fix it?</p><p>Reference<br><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html#bp-general-nosql-design-concepts">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html#bp-general-nosql-design-concepts</a><br><a href="https://cassandra.apache.org/_/index.html">https://cassandra.apache.org/_/index.html</a><br>DDIA</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>DDIA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transaction</title>
    <link href="/2022/05/22/Transaction/"/>
    <url>/2022/05/22/Transaction/</url>
    
    <content type="html"><![CDATA[<p>Transaction<br>几乎所有的关系型数据库和部分NoSQL能够提供事务支持，早在1975年IBM提出的第一个SQL系统System-R中，介绍了事务的概念。虽然在过去的40年中，事务的实现方式改变了许多，但是其基本的思想与40年前的System-R几乎是一致的 ，在MySQL，PostgreSQL, Oracle, SQL Server中 事务的实现原理几乎是与System-R相同的。</p><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnNYVPZVwOG8KKcUirv37ELM">Transaction</a></p><p>新型的NoSQL数据中，大多都摒弃了事务的概念，因为在支持Partitioning 和 Replication的同时要支持事务的话，会对开发者和系统都提出很大的挑战。</p><ul><li>Spanner 架构的NoSQL中保留了事务的概念，并且系统是强一致（Serializable）的，按照CAP理论，它的Availablity有时候会因为网络分区而收到影响，表现为延迟较高，甚至系统不可用。<ul><li>BigTable，Chubby，ZK，GFS</li></ul></li><li>Dynamo 架构的NoSQL 已经抛弃了事务的概念，而且系统是弱一致（最终一致性）的，它的可用性较高，但是在网络分区的场景下，不同节点之间的一致性会收到影响，可能需要在用户侧执行Merge来处理冲突。<br>Riak (Bitcask Engine), Cassandra, and Voldemort (VoltDB) use leaderless replication models inspired by Dynamo.<br>目前不支持ACID的数据库系统，有时会被称为BASE (Base Availability, Soft State, Eventually Consistency) ，这是一个相对于ACID的概念，告诉用户“我的系统不支持ACID，是BASE的”，然后具体怎么理解交给用户，反正我的系统不支持ACID。<br>Overview<br>从下图中能够看到，在分布式事务中需要讨论的几个问题：</li><li>事务隔离级别</li><li>不同隔离级别的实现</li><li>某个事务隔离级别下的性能分析</li></ul><p>应用系统可能会面临的挑战</p><ul><li>数据库的软件或者硬件可能会在任何时候挂机，甚至是在读写进行到一半的时候宕机；（挑战了分布式系统的可用性）</li><li>网络分区：分布式集群中的一些节点可能无法与另一些节点进行通信，甚至集群与外部的联系也有可能被切断。</li><li>不同Client Propose不同的事务，导致Race Condition，或者Client可能读到一些部分更新的数据，这类数据是无意义的。这种问题在介绍索引的时候讨论过，对于MySQL这类直接覆盖记录的写法，如果不做保障，有可能更新到一半数据库突然挂机，这时候某一条记录中可能有一部分是新数据一部分是旧数据。但是对于Riak，VoltDB这样的数据库，都是采用Append的方式写入，后台使用一个守护进程定期对数据进行合并压缩，就不会出现部分更新的问题。<br>对于ACID一些有趣的解读</li><li>Atomic is not about Conccurency. Perhaps abortability would have been a better term than atomicity.</li><li>The word consistency is terribly overloaded: (Java 中overload表示重载)<ul><li>Replica Consistency and Eventually Consistency in async replicated system (Replication).</li><li>Consistent Hashing is a strategy used for rebalancing. (Partition)</li><li>In CAP theorum, C generally means linearizability.</li><li>In the context of ACID, C means “good state” in database system.</li></ul></li><li>ACID中的一致性其实是应用层的概念，而不是数据库层的概念。只有Client的应用能够定义什么是一致的什么是不一致的，而数据库只负责存储应用发过来的请求，并且只会进行相对较弱的校验（如：唯一索引校验，外键校验等）。所以ACID中，只有AID是在数据库层面保证的，而C是在应用层面保证的。<br>Transaction Isolation<br>Read Uncommit</li><li>脏读：读到一些基本不会存在的情况，比如：事务写了之后，但是回滚了。</li><li>脏写：两个事务同时写一个字段，导致partial update 的情况出现。<br>Read Commit<br>实现：通常通过行级锁实现，一个事物想要读&#x2F;写一个字段的时候需要先获得这条record的锁。<br>问题：一个很长的事务会阻塞大量的读请求，导致读请求堆积。<br>优化：数据库保留数据被加锁前的value，读请求在事务对record上锁的过程中可以读到old value。<br>SnapShot Isolation &amp; Repeatable Read<br>RC存在的问题是NonRepeatable Read 和 Read Skew<br>问题<br>场景一：可以容忍Read Skew，Alice看到自己两个account的钱总和不一致了，于是她刷新了一下界面，发现钱一致了，那么不会引起panic。</li></ul><p>场景二：BackUp如果读到了Master的old value，并且保存了Master的value，就会导致在应用副本的时候一直读取到旧数据，特别是如果Master挂了，BackUp take over之后，错误的数据会一直被使用下去。<br>场景三：OLAP系统中如果出现了不可重复读，会使数据分析产生偏差。<br>实现<br>SnapShot：Naive的思想是保证数据库在任何时候都能读到Consistent的数据，这样就不会产生Read Skew；在开启事务的时候打一个快照，之后所有的操作都在这个快照上进行。<br>MVCC的实现：每一行数据都有一个 <code>create_by</code> 和<code>delete_by </code>字段表示创建行的记录事务和删除行记录的事务。数据库通常都会有一个后台进程会不断地对产生的版本进行merge和GC的操作。</p><p>与SnapShot被一起定义的还有可见性原则，包括：</p><ul><li>在每一个事务开始的时候，数据库会标记此时所有正在执行的事务，所有正在执行事务的更新会被忽略。</li><li>任何被已回滚的事务提交的更新都会被忽略。</li><li>任何被大于当前事务ID的更新都会被忽略。</li><li>所有其他的更新都是立即可见的。<br>How is the index involved in MVCC?<br>理论上来说，可以让一条索引指向所有版本中的记录，当某条记录被删除之后，相应的索引项也会被删除。<br>在实际应用中，不同的数据库产品可能会采取不同的策略：</li><li>PostGreSQL中，禁止对某个多版本的object进行索引更新。</li><li>CouchDB中使用appendONLY + Copyonwrite的方式进行索引更新，由于这里数据库索引的底层使用的是B+树，所以从当前节点到根节点的page都会被copy一份。<br>More about Repeatable Read<br>Nobody knows the exact meaning of RR.<br>因为在System R诞生的时候，SnapShot Isolaiton还没被发明出来，而初始的数据库系统对于可重复读的定义特别模糊，导致后来的数据库产品对RR都有自己的定义。Oracle 把Snapshot 定义为可序列化级别，而MySQL将其定义为RR级别，但不管怎么定义，他们实现的功能都是能够保证一个事务内的读操作是一致的。<br>Dirty&amp; Write &amp; Lost Update<br>丢失更新个人理解是一种 <code>dirty write</code>的情况，一般来说，并发情况下，后一个执行的事务语句会覆盖掉前一个事务语句的更新。<br>可能会出现的场景：</li><li>对于用户充值和转账的并发情况，如果充值和转账出现了并发修改一个账户的情况，如果其中一个事务覆盖了另一个事务的执行结果，假设充值100，转账入账100元，用户的账户正常情况下应该是200元，但由于丢失更新少了100元，显然是不能接受的。<br>解决方案大体可以分为以下几种：</li><li>显式上锁（Manually）：<br>begin transaction;<br>select * from account where user_id &#x3D; xxx for update;<br>update amount where user_id &#x3D; xxx;<br>commit;</li></ul><p>显示上锁的问题是要求在业务层手动编码上锁：<code>for update</code>，如果程序员忘记对事务上锁的话，可能会引起丢失更新的问题。</p><ul><li><p>借助SnapShot (Automatically)<br>利用DB层提供的MVCC实现对丢失更新的自动检测，一旦事务检测到两个Write Version可能会引起冲突的时候，数据库会自动让后发的事务回滚。这样做的数据库实例有：PostgreSQL。<br>但是MySQL的InnoDB引擎并没有提供这种MVCC的功能，MySQL的RR级别是通过Gap-Lock来实现的。</p></li><li><p>CAS</p><ul><li>如果account_score 等于期望的old_value，那么事务可以更新account_score的值；</li><li>如果account_score 不等于期望的old_value，那么事务的更新操作是有可能失败的；<br>问题在于CAS的比较如果基于MVCC，那么数据库基于old-version返回给事务的结果是允许更新，但实际上new_version 的值是不等于old_value的。这样就会出现竞争情况，所以在使用CAS之前需要了解数据库在CAS条件下使用的隔离级别以及隔离级别的实现方式。<br>update account_score set account_score &#x3D; new_int<br>where user_id&#x3D;1 and account_score &#x3D; old_int</li></ul></li><li><p>Conflict Resolution &amp; Replication<br>Lock和CAS都是假设数据库在单机上工作的场景，但是对于分布式数据库，同一份data可能在不同的node上都有副本，并且副本的更新可能是并发或者异步的 (LeaderLess &#x2F; Multi-Leader)，这时候Lock和CAS可能没有办法很好的work。<br>对于Multi-Partition之间如何保证Atmoic Write，在Chapter 5 - Replication中提及过一些解决冲突的方式。即：DB允许在多副本上执行的顺序是不一致的，但是会通过一些手段来保证最终一致性，比如LWW (Last Write Win)，但LWW不能防止Lost Update。Riak2.0提供了多副本之间自动解决冲突并且能够防止Lost Update的方法。<br>总结：</p></li><li><p>Dirty Write &amp; Lost Update 都是为了解决对一个Key 的冲突，不涉及多个Key的竞争问题。<br>Serializable<br>Write Skew &amp; Phantom<br>Write Skew是MVCC无法解决的问题。<br>场景<br>必须保证至少有一个人在Oncall岗位上，Oncaller是可以短暂离开的，但是不论怎么样，必须保证一人oncall。<br>问题：A 和 B同时申请Leave，此时数据库根据当前值班人员有两人同时允许AB Leave，最终导致没人Oncall。—— Write Skew</p></li></ul><p>Write Skew 可以理解为：读同一条记录，更新不同的记录。在RR级别是无法防止Skew Write的，通常来说会采用以下两种方式解决上面的Write Skew问题：</p><ul><li>将隔离级别设置为Serializable，因为上面 Write Skew发生的场景是DB级别的事务同时被触发，但Serializable能够使事务串行执行，有了先后顺序之后，上面的Write Skew的问题可以得到解决。</li><li>如果无法使用Serializable这个隔离级别的话，需要手动上锁：<br>begin transaction;</li></ul><p>select * from doctors where on_call&#x3D;true and shift_id&#x3D;1234 FOR UPDATE;<br>update doctors set on_call&#x3D;false where name&#x3D;”Alice” and shift_id &#x3D; 1234</p><p>Commit;</p><p>其他场景分析</p><ul><li>预定场景：预定会议室：在预定之前先查询是否存在冲突的时间，如果不存在则插入一条记录，表示预约了该会议室。<br>begin transaction;</li></ul><p>select Count(1) from bookings where room_id&#x3D;1<br> and end_time&gt;&#x3D;’2022-05-01 12:00’ and start_time&lt;&#x3D;’2022-05-01 13:00’;</p><p>insert into bookings (room_id, start_time, end_time, user_id)<br>values (1, ‘2022-05-01 12:00’, ‘2022-05-01 13:00’, ‘A’ )</p><p>commit;</p><ul><li>消费场景：防止Double-Spending：在用户消费的时候，先check用户的account中是否有足够的coins，然后加入一条Transaction记录。但是在并发的情况下，可能会插入多条Transaction记录，这样会将用户的account扣成负数，并且多送出了一个礼物，公司赔钱。</li><li>注册场景：不能同一个用户名：先check数据库当前用户名是否使用过，如果没有，则将该用户名写入到数据库记录中。并发情况下，可能会出现重复的用户名；但是这个可以通过DB级别的Unique Key来保证。<br>Phantom<br>问题出现的过程：而且Insert 和 Delete 无法通过 For Update解决，因为库中根本不存在能够上锁的记录。MySQL中的解决方案是：<code>NextKey Lock</code>。<br>无法复制加载中的内容<br>Materializing Conflicting<br>Naive Idea是给不存在的row构造锁。<br>比如在会议室预定的场景中，可以新增一张表用于记录一个room的预约时间：每一行表示会议室被预约的时间(roomid, start_time, end_time)，初始化所有room所有可能的预约时间，当一个事务要预约的时候，需要先拿到对应room，对应时间段的锁（这一步操作是可以通过For Update实现的），因此可以解决Phantom问题。<br>缺点：</li><li>实现非常丑陋，需要新增一张Lock Table。</li><li>其次是需要将并发控制模型耦合到数据模型中，造成模型污染。<br>实在没有办法的话，才会选择这种实现方式，更常用的防止幻读的方式是通过：<code>Serializable</code>这个隔离级别来实现。<br>Serializable<br>所有并发事务的执行过程都和某一次顺序执行的结果一致，这种方法排除了所有事务并发的可能。<br>Serializable实现的方法大致分为以下三种</li><li>Actual Serial Execution</li><li>2PL</li><li>Serializable Snapshot Isolation<br>Actual Serial Execution<br>采用一种暴力回避并发冲突的方式，使用单线程一次只允许一个事务执行。虽然这是一个显而易见的方法，但是这个方法直到2007 年才被数据库专家们认可。因为在之前的30多年内，许多工程师都觉得使用单线程的方式会带来很多性能问题。</li><li>RAM变得更加便宜，使得机器的内存得到扩张，能够容量的数据集增长，使得事务能够存放在内存中，CPU能够更快的访问。</li><li>数据库工程师们发现：OLTP系统的事务总是特别短，通常只包含若干个 Read &#x2F; Write操作，而OLAP系统通常都是需要一系列较长的Read操作组成，所以其实只要保证不出现Partial Write的情况导致分析出错，业务方大多数情况下都还能够接受，因此OLAP是可以基于Snapshot来做事务的。<br>一些NoSQL如VoltDB, Redis, Datomic 都提供了对可序列化级别的事务支持，使用单核单线程的执行事务其实是一种tradeoff，一方面这样做能够减少上下文切换的开销，但是系统整体的吞吐量也因为单机而受到了限制。为了让单线程的事务得到更好的支持，需要将事务打包成一个特定的数据结构或者package传给DB层。<br>事务包装<br>购买机票场景：把用户的操作和交互封装在事务中，通过多次HTTP请求完成一个事务：（查询航班 (Read) - 查询航班票价和座位 (Read) - 选择航班和座位 (Write) - 填写旅客信息 (Write) - 付钱 (Write)），这一整个事务需要不断地通过HTTP请求与用户进行交互。</li><li>网络 和 DiskIO 能拖垮整个系统的响应时间。</li><li>用户的犹豫不决也让事务访问的对象迟迟得不到释放，使其他想要购票的旅客体验急剧下降。<br>所以，一种方法是将“交互”的模式转变为一次性的事务请求，即：一个HTTP只能关联一个事务。即便是这样，应用层和DB层也需要通过多次的交互来完成整个事务，如下图（上）所示。</li></ul><p>在可序列化级别下，为了保证性能甚至不允许应用层和DB层进行过多的交互，而是将整个事务包含的command一次性交给DB作为：”Store Procedure”。如上图（下）所示。<br>不同时代的系统对于Store Procedure有着不同的支持，早期 Store Procedure 还是存在很多问题的，比如 Metric &#x2F; Log等运维组件无法接入，或者一个 BadTx阻塞了整个CPU的执行也出现过。</p><ul><li><p>RDMS系统：</p><ul><li>Oracle, SQL Server, PostgreSQL 分别提供了 PL&#x2F;SQL, T-SQL, pgSQL&#x2F;PL 的Store Procedure的支持。这些实现都非常丑陋并且没有一个配套的生态，所以在开发中使用的也很少。</li></ul></li><li><p>NoSQL系统：</p><ul><li>VoltDB, Datomic, Redis 分别使用Java，Java，Lua来实现对Store Procedure的支持。由于Store Procedure能够在内存中执行，所以它也不会使系统的性能过度的退化；相反，它减少了网络和磁盘IO，使得系统的执行效率更高了。<br>总结对于Serial Execution的使用</li></ul></li><li><p>每一个事务必须是small and fast的，受到 <code>Single CPU Single Thread</code>的限制。</p></li><li><p>它要求数据集的大小能够存在于内存中，如果数据集太大或者事务涉及冷数据的时候，就需要与Disk进行交互，此时会拖慢整个系统。<br>这里可以使用一个叫做Anti-Caching的策略，将访问冷数据的事务挂起，执行另一个事务，并且在这个过程中异步地将第一个事务所需的数据加载到内存中，等加载完成后就能够加载第一个事务并执行。</p></li><li><p>如果对于写吞吐要求较高的场景，可能需要将数据进行Partition，每一个CPU负责若干个Partition上的事务。</p></li><li><p>如果事务涉及Cross-Partition，则需要额外的Coordinator和锁进行处理，又会进一步拖慢系统。所以总的来说，需要根据场景进行Trade-Off，如果能够支持Dynamo通过配置化定制不同的场景会是一个比较好的方案。<br>2PL<br>第一阶段：获取锁<br>第二阶段：释放锁<br>任何可能引起并发冲突的事务都会被串行化，通过并且读写会互斥，比Snapshot级别的读写并发效率低很多。<br>Predict Locking<br>在查询条件上进行上锁。<br>select * from booking where room_id&#x3D;123 and start_time&gt;’2022-05-12 12:00’ and end_time &lt; ‘2022-05-12 13:00’’</p></li><li><p>一个读请求必须保证查询条件上没有写锁，一个写请求必须保证查询条件上没有读锁。<br>Index Range Locks<br>Also Known as Next-Key Locking<br>Predict Lock对于性能有较大的损失，因为检查锁的过程非常耗时。<br>NextKey Lock会对事务请求的数据估计一个范围，然后对这个范围进行上锁，（通常需要配合索引使用，如果没有索引的话可能会锁住整张表）。比如：在查找会议室的时候：</p></li><li><p>如果通过room_id进行查找，则会在 room_id 进行上锁；</p></li><li><p>如果通过时间范围进行查找，则会对 时间范围 上锁。<br>Snapshot Isolation (SSI)<br>2PL 是一种极端悲观的并发处理方式，他会将检测冲突的行为前置于事务处理；SSI则是在乐观控制的基础之上加入了冲突检测机制，来判断两个事务的执行结果是否会存在并发问题，如果存在在的话，会挑选一个事务Abort。当然乐观控制也并不是完美的，在并发事务不多的情况下，能够work得很好，因为只需要回滚非常少量的事务；但是在事务并发很多的情况下，却会存在比较多的问题，因为一旦检测到事务并发冲突，就会回滚事务；这样在数据库已经接受很大流量的情况下，会使情况进一步恶化。<br>Decision based on outdated premise: 在预约会议室或者Doctor Oncall的例子中，并发问题的出现是由于两个事务同时基于读 数据Data，并且基于读到的Data进行分析，并且执行会影响数据Data的操作。如果数据库能够感知事务的执行结果会影响到事务过程中对Data 的分析，并对其做最坏的预估，就能够避免 Phantom &amp; Write Skew 的问题：</p></li><li><p>方法一：检测MVCC 对象的Version，如果有事务更新了当前Version的Data，就认为出现了事务并发问题；</p></li></ul><p>在MVVC这个隔离级别下，事务都是基于一个满足一致性的快照进行的。在SSI算法中，如果事务在即将提交的时候发现自己的premise不成立（SnapShot被修改），会立即Abort。</p><ul><li>为什么要等到事务提交才进行 Detect  + Abort操作？<br>事务有可能是只读的，一个只读的事务可以允许它基于一个Consistent的快照进行；在只读的事务中，只需要快照满足一致性即可，并不会出现 Write Skew的问题。而且一个事务在Read的时候，DB并不能预测事务未来是否会有写操作，因此不能 Read到快照被修改之后就立即Abort当前事务。</li><li>方法二：检测事务的Write操作是否会影响事务的Read操作结果。</li></ul><p>如果事务43在写执行Write操作的时候检测到事务42更新了它的读结果，此时会将事务回滚。Index-Range Lock 需要数据库在执行事务的过程中记录，这将带来额外的开销，但是由于其能够在事务提交或者回滚之后被删除，所以并不会对内存造成过大的占用。<br>SSI 相比 2PL最大的好处是 Read &#x2F; Write 事务能够并发，这样不会导致 Read事务因为 Write被阻塞，能够可观地增加系统的吞吐量以及降低延迟。Read 事务能够基于Snapshot进行。<br>SSI 相比 Serial Execution最大的好处是不用局限于单核单线程，能够最大限度地利用好多核的机器，增加系统的吞吐量。<br>Downside：SSL针对读写兼备的长事务不太友好，因为长事务需要一直执行到提交阶段才会进行并发检测，如果出现了Race Condition，会将执行了很长的事务回滚，这对于系统会造成不小的运行损失。</p><p>总结<br>Outline</p><ul><li>常见的并发问题<ul><li>脏读 &#x2F; 脏写</li><li>不可重复读</li><li>丢失更新</li><li>Write Skew</li><li>幻读</li></ul></li><li>事务隔离级别<ul><li>Read Commited</li><li>Read Repeatable</li><li>Serializable<ul><li>Serial Execution</li><li>2PL</li><li>SSI<br>总之，不论对于传统数据库还是新型的数据库来说，事务都是一个非常重要的Feature，因为它能够保证一系列操作的ACID特性，对于保持数据的一致性非常关键。</li></ul></li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>DDIA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamo</title>
    <link href="/2022/05/22/Dynamo/"/>
    <url>/2022/05/22/Dynamo/</url>
    
    <content type="html"><![CDATA[<p>Dynamo: Amazon’s High Available KV Store</p><p>DynamoDB是Amazon平台上构建的 “always on” availability 级别的NoSQL数据库，并且能做到无缝扩展 (high-scalability)。为了实现这个级别的可用性，它在某些故障场景中将不可避免的牺牲一致性。<br>最重要的两个核心点：High-Availability &amp; Scalability.<br>尽管牺牲了强一致性，Dynamo也是一个划时代的产品，他证明了去中心化也能够为系统提供高可用需求，同时Dynamo的成功也证明了最终一致性的DB基础架构也不失为高可用系统的一块基石。</p><p>推荐阅读：为了更好的体验，推荐移步至 <a href="https://lo845xqmx7.feishu.cn/docs/doccnAYOAFo5OQPUCapBWMqNAzb">飞书Dynamo</a></p><p>Introduction<br>亚马逊作为电商巨头，它对于用户能够添加购物车的场景尤为重视，Dynamo的提出很大程度上是受“加购”这个场景的驱动。它对于系统的要求是服务必须总是能够为用户提供 加购 和 查看购物车的功能，不论DC是否受到了灾难或者网络是否可达。<br>此外，Dynamo提供了可用性和扩展性的配置选项，因为在Amazon很多业务都需要根据自身的场景选择不同的存储系统，至少会有不同的存储需求。比如很多场景都只需要访问数据的主键，如果使用传统的关系型数据库则会出现性能瓶颈。<br>Dynamo 基于业界熟知的算法来满足对可用性和扩展性的需求：</p><ul><li>一致性Hashing：For Partition &amp; Replication</li><li>Object Version：For Consistency</li><li>Quorum Read &#x2F; Write</li><li>Gossip: Failure Detecting &amp; Membership Protocol<br>系统的假设 &#x2F; 要求</li></ul><ol><li><p>Query Model对数据项简单的读写是通过Primary Key做的状态存储为一个由唯一键确定的二进制对象。没有横跨多个数据项的操作，也不需要关系方案。<br>Dynamo主要用于小数据的存储，它的Store Server价格一定会高于GFS的Chunk Server。</p></li><li><p>ACID属性：在保证ACID的数据存储往往有很差的可用性，DynamoDB的目标应用程序是保证高可用性，弱一致性。<br>Dynamo 不提供任何隔离级别，因为它只允许单个Key上进行操作。</p></li><li><p>Efficiency：系统需运作在一般的Commodity Hardware上，服务必须能够通过配置Dynamo使他们不断达到延迟和吞吐量的要求。衡量Dynamo 性能的指标是 P99.9。<br>Design Consideration<br>很多传统的商务系统都会使用同步复制在多副本之间进行数据拷贝，为了实现这种级别的强一致性，系统会在网络不可达或者机器failover的时候被阻塞。<br>对网络延迟和服务宕机敏感的系统来说，可以通过乐观复制的技术来提升服务的可用性，即：数据的变化可以在后台更新，并发断网的操作也是可以执行的（并发断网：我理解是服务某个用户的Server和集群断连，多个用户的server可能同时面临这种情况；但是在该情况下仍然需要保证服务可用）。这样一来就会出现数据冲突Conflict， 设计时就需要考虑由谁在什么时候解决数据冲突的问题。</p></li></ol><ul><li>对于When解决冲突：很多传统的系统为了实现一致性，都使用了同步复制的方式，即：在Write的时候限制了Write不能达到所有的数据节点，则认为这个Write不能成功；而Read的时候可以向任一Server发起读请求，读到的都是最新的数据，简化了Read的逻辑。Dynamo采取了另一种思路：保证Write总是能成功，但是Read的时候需要做一些逻辑来保证一致性。（为了提升用户体验，如果用户发现自己无法将商品加入购物车，很可能就不会购买该商品，公司则会亏钱）。</li><li>对于Who解决冲突：Dynamo作者的思路是，如果将Reconciliation的逻辑放在DB层，那么用户可能没有自定义解决冲突的能力，因为DB层处理冲突通常都是采取LWW (Last-Write-Win) 的方式（可以想想MySQL是如何处理事务冲突的：Transaction）。从另一个思路来看，业务层其实是对某个场景的上下文理解比较全面的，所以Dynamo将冲突的处理留给了业务层。它向业务返回的时候，可能会返回多个版本，由用户来进行Merge。</li></ul><p>核心设计点：<br>动态增长的Server Node<br>对称：所有的节点一视同仁，没有特殊节点服务于特殊流量<br>去中心化：Peer-to-Peer 没有Leader决定Log Index<br>Heterogeneity：Server能够承担的流量要和Server主机的性能成线性关系</p><p>Related Works</p><ol><li><p>Peer-to-Peer (P2P) Systems<br>第一代P2P存储系统：Freenet and Gnutella 文件共享系统，特点是一次查询请求会通过洪泛法[2]将请求发送给系统中的其他节点，尽可能多的找到保存了查询数据的节点以返回。<br>第二代P2P存储系统：Pastry &amp; Chord，每一个节点保存一份路由信息，在O(1)的时间内找到系统中负责对应请求的节点。在此基础之上，OceanStore &amp; PAST也被工程师提出。OceanStore支持事务和持久化，它采用了为每一个事务赋予全局顺序的方式来解决冲突 (Conflict Resolution)。PAST则室友了一个抽象层来实现持久化存储。</p></li><li><p>分布式文件系统<br>P2P系统的路由表（也是命名空间的一种）只能保存简单的节点信息，而分布式系统的 NameSpace能够保存级联的路由信息。<br>产品：Farsite System (类似NFS的文件系统，去中心化)，GFS 是谷歌内部使用的中心化 (Single Master) 的文件系统 Google File System， Bayou 分布式RDB，提供最终一致性以及离线操作。<br>Bigtable 提供强一致的结构化数据存储，它的底层是使用多级的 <code>sorted map</code>实现。<br>这里想特别提一嘴的产品是 Antiquity，因为这个产品是笔者第一次见到能够抵御拜占庭故障的产品，它主要是为信托机构提供数据的强一致性。</p></li><li><p>讨论<br>Dynamo关注的点和传统的去中心化，面向强一致的存储系统不同，针对Amazon的电商场景，Dynamo更加注重用户的体验，为了让系统在网络分区的环境下仍然能够正常的运行。<br>Dynamo 使用场景的特殊性：</p></li></ol><ul><li>在 Failure &amp; Concurrent 的条件下提供 “Always Writable” 的数据存储服务。</li><li>应用在Amazon内部应用的基础架构层，所以可以保证所有节点都不是拜占庭节点。</li><li>使用Dynamo的应用不会像文件系统那样出现级联的命名空间，以及复杂的关联模型，而是简单的KV模型。</li><li>SLA级别：P99.9 要在几百毫秒内，提供给对延迟较为敏感的业务系统使用。<br> System Design</li></ul><ol><li>Dynamo API<br>get: key -&gt; ([value], context)<br>Exposes inconsistency: can return multiple values<br>Context is opaque to user (set of vector clocks)<br>Put: (key, value, context) -&gt; void<br>Caller passes context from previous get</li></ol><p>Context：包含了系统对于key的metadata，它是和key-value一起存储的，用户不需要对Context理解。Dynamo会将 key和value都视为一个字节序列，它会将key先用MD5 Hash进行加密得到128-bit的标识符，用于一致性Hashing决定哪个节点作为其Coordinator。<br>2. Partitioning<br>主要使用的是一致性Hashing，为了保证增加和删除节点的时候，系统每个节点的流量变化更加平均。更多Partition的信息可以参考：Partition is All You Need<br>3. Replication<br>Dynamo中三个可配置的参数: (N, R, W)<br>N 同一份数据需要复制到多少个节点上；<br>R 每次最少从R个节点中读取数据；<br>W 每次最少得到W个节点的Write数据；</p><p>通过一致性Hashing，可以计算出一个Key的Coordinator节点，这个节点除了将key写入到本地之外，还会在哈希环中顺时针寻找N-1个节点，同步到N-1个其他的节点中。一个Key对应多个Nodes，这些为同一个Key提供服务或者备份的节点称为节点的 “Preference List”，并且为了容错，preference list通常会包含多于N个节点，它只会包含物理节点信息，而不会包含虚拟节点信息（物理节点和虚拟节点的概念，需要读者自行了解一致性Hashing的原理）。<br>4. Data Versioning<br>Dynamo为上层应用提供最终一致性，因此所有write的操作都是异步地向Replica传递，一个Put操作可以在收到所有Replica的Ack之前向Caller返回结果（但必须大于超参W），所以Read操作是有可能读到Stale Data的。<br>在Amazon场景中可以允许Read到旧数据的，但是必须保证用户的Write操作不能被覆盖，比如：“加入购物车”和“删除购物车”，即使网络分区或者部分服务不可用的情况下，也要保证用户的写一定会生效，比如加入购物车的商品一旦加入，在未来的某个时刻用户必须能读到这个商品。过程中产生的冲突可以通过Reconciliation修复。<br>为了实现以上系统要求，Dynamo会将每一个更新操作后的数据视为不可变对象，并且允许系统中存在一个对象的多个版本，比如：Node-A上存在商品A，Node-B上存在商品B，Node-C上存在商品C。在大多数情况下，Dynamo本身就能在DB-Level对数据冲突进行修复（使用LWW），但如果同一个对象的状态因为并发写 &amp; 网络问题出现了Branch，就需要在Application-Level进行Merge。</p><p>应用层在使用Merge策略的时候，能够保证Add的商品一定不会丢失，但是Delete的商品不一定能够删除，提升用户的购买率。</p><p>为了使系统中能够存在多个版本的数据，Dynamo为每个对象添加了Vector Clock，VC用来表示系统中两同一个Object两个Branch的逻辑先后顺序，如果两个Branch是可比较的那么DB层保留最新的版本；如果两个版本是冲突的，则需要应用层进行Merge，每个Vector Clock表示为：<br>struct ClockNode{<br>    Node ServerNode,<br>    Counter int64,<br>}</p><p>struct VectorClock{<br>    VectorNodeItem list<ClockNode><br>}</p><p>在PUT的时候，Dynamo要求Client指定Version，Application包含在Context中传递给DB。如果Dynamo遇到了DB层无法合并的Branch，则会将所有状态都返回给Client，Client必须Issue一个新的Put请求，来解决冲突。理论上，如果存在Conflict的Branch越多，需要存储的Version Data越多，那么Context会膨胀，很吃内存。但Dynamo Paper中说实际上这种情况十分罕见，因为Writes总是会被分配到Preference List中Top N个节点，当出现网络分区的时候会造成Branching的情况，这种情况下需要对Vector Clock的大小进行限制（限制List的length），假设只允许存储10个ClockNode，当达到阈值时会采用FIFO策略进行淘汰。<br>Author们说在实际生产环境中很少遇到这种问题，因此也没有深入研究过这块算法。</p><ol start="5"><li>Get 和 Put操作的执行<br>负载均衡的方式：</li></ol><ul><li>使用LB中间件对Client的请求进行转发，这样Client不用感知Cluster中的节点。</li><li>让Client感知所有Partition，并直接对合适的Partition发起请求，这种方式会更快，因为不用LB转发。<br>Coordinator：处理Client请求的节点，通常是Preference List中Top N中的第一个节点。如果通过LB的方式选择Node，则需要考虑LB指定的Node是否在Preference List的Top N中，如果在的话，则成功成为Coor；如果不在的话，则需要将请求转发给Preference List中第一个节点。Top N个节点是Healthy的节点，即不存在网络分区的节点；当网络分区出现的时候，lower ranked的Node也会被Client访问到。<br>Dynamo Quorum：(N, R, W) 通常的配置是 (3, 2, 2)。<br>Dynamo Sloppy Quorum: R + W &gt; N （和Paxos类似，Paxos中使要求任何请求都获得大多数节点的认可，Dynamo只需要 R+W&gt;N 就能保证每次读到最新的结果，即使这些结果有冲突）。 </li><li>“Never Block Waiting for Unreachable Nodes”；</li><li>“Each Key Has Replicas &gt; N”</li><li>Hinted  Handoff: 当Failover出现的时候，Dynamo并不总是会选择TopN of Preference List。比如Consistency Hashing Ring如下图所示，如果A不可达，那么A上的Replica会被转移到D上，并且被转移的Replica上会携带一个HintNode的标志，表示它原先是属于A节点的Replica，当系统检测到A节点恢复之后D会把Replica返回给A，然后从本地存储中删除。<br>通过Hinted Handoff，Dynamo保证了系统总是能够保证Read &#x2F; Write能够正常执行，即使出现了暂时的Node Failover或者网络不可达。<br>Dynamo Cross DC Strategy: 由于Dynamo天然的Leaderless架构，支撑了它能够很好的扩展到不同的DC中，在Dynamo中一个Key的Preference List总是由不同DC的节点组成，并且每一份副本都会保存在不同的DC上，为了应对极端情况下DC断电或者网络短连，以及其他自然灾害。</li></ul><ol start="6"><li>Membership and Failure Detection<br>Ring Membership<br>Gossip-Based：每一个Node定期与其他节点的通信，感知到系统中存在的节点，有点类似（Rip路由算法），每个Node随机地和两个节点通信获取他们c的信息，这样系统中不断的进行信息交换最终能够得到完整的“路由信息”，缺点和Rip一样，就是每次删除或者更新节点的时候，其他节点需要反应一段时间才能意识到新加入了节点。<br>Join And Leave<br>在早期的Dynamo架构中使用了Gossip-Based的方法对新加入的节点或者移除的节点进行检测，而后期加入了显示的Node Join and Leave。其实这是对Node Failover 和 Node Addition做了不同的策略，对于节点的Failover，系统认为这是一种短暂的状态，所以会通过gossip的方式进行传播；而对于永久加入或者移除一台Server，系统会认为这是一种永久的操作，如：在DC中加入了Server不会在短期内弃用，所以是通过显示通知的方式告知所有节点。<br>Implementation<br>无法复制加载中的内容<br>Local Persistence<br>Dynamo的持久化层是可插拔的，可以针对不同的业务适配不同的持久化层，比如：BDB (Berkerly Data Store)适用于10-100KB的数据，MySQL适合更大的数据等。<br>Read Coordination<br>笔者理解Read Coordination是使用行为者模式设计的一种事件驱动模型，这一部分是整个Dynamo架构中的组织协调者，它将一个工作流拆分为不同的子阶段：<br>无法复制加载中的内容<br>在Coordinator得到所有Node的返回结果之后，会通过Version判断出最新的结果，并且会尝试更新那些保存着过期数据的节点，该过程被称为 Read-Repair。</li></ol><p>如果每次都选择Preference List中第一个节点作为Coordinator，可能会造成流量分布不均匀。为了解决这个问题，Dynamo中允许Preference List的Top N任意一个节点成为Coordinator。并且系统总是认为“每一个Write操作之后总会紧跟一个Read操作”，比如：（Write, A）, (Read, A)，那么前一个Write的Coordinator会自动成为后一个Read 的Coordinator，以此来增加“Read Your Write”成功的机会。</p><p>Anti-Entropy<br>Dynamo 使用了Merker Tree来检测不同副本之间的一致性以及最小化需要移动的增量数据。Merker Tree一种哈希树，它的叶子节点是每一个 Key的Hash值，非叶子节点是它子树节点的Hash值的和。Merker Tree最大的好处是在判断两个副本之间数据是否相同的时候，不一定需要加载整棵树结构。例如：如果两个副本根节点的Hash值相同，说明他们子树的值都是一致的，可以提前结束判断。如果副本根节点的Hash值不同，说明肯定存在至少一个数据副本是不一致的，此时需要遍历到树的根节点，找出所有不一致的数据副本，然后选择最新的内容进行同步。</p><p>在Dynamo中，每一个节点会为它所负责的 每一个Key-Range (不同Virtual Nodes会产生不同的Partition) 维护一棵Merker Tree，然后使用树的遍历算法计算两棵树是否相同。</p><p>Business Logic<br>Dynamo最大的优势是作为Infra Storage 为上层提供了自定义解决冲突的方式。</p><ul><li>购物车场景；使用Merge的方式解决Version Conflict。</li><li>User Session场景使用：LWW的方式解决冲突。</li><li>Adjust Params：<ul><li>对于Heavy Read场景可以将R设置为1，W设置为N。</li><li>对于Heavy Write场景，并且允许读到stale data的场景，可以将W设置为较小值，R响应增大。<br>Performance Analysis</li></ul></li><li>SLA: P99.9 &#x3D; 300ms。<br>2006 年Dynamo读写请求值的分布情况：</li><li>Write请求的延迟总是大于Read请求：Write请求通常需要Disk Access</li><li>Avg 总是低于 P99.9，因为P99.9 对流量洪峰、访问对象大小和网速更为敏感。</li></ul><p>Client Driven &#x2F; Server Driven Dynamo</p><ul><li>Client Driven<br>Client保留一份集群中节点的分区信息副本，每次Write &#x2F; Read 直接向对应的Node发起请求。</li><li>Server Driven<br>Client 每次将Write &#x2F; Read请求发送到LB，由LB选择Coordinator处理Client请求。</li></ul><p>Client Driven 的方式减少了IP包转发的次数，所以能够减少延迟。但是Client-Driven方式的效率很大程度取决于Client的local cache能够拿到多新的Cluster Image，在该方法中Client会周期性地 (10s) 随机地向 Dynamo Node 发起Pull请求，获取集群当前的信息。所以最坏的情况下，Client会保持10s的过期配置信息，但是Client会有一个热更新的操作当它检测到Node不可达会立即Pull一次Cluster Config。<br>补充内容：Dynamo Data Model</p><ul><li>Table, Items, Attributes</li><li>Primary Key</li><li>Secondary Key</li><li>DynamoDB Data Types</li><li>Item Distribution<br>Table Item Attribute<br>在RDB中，一个table有一个预先定义好的结构，比如：表名，主键，列名。而DynamoDB只需要一个主键，不需要提前定义各种属性或者数据类型，DynamoDB中的独立item可以拥有任何数量的属性，但每个item的size不能超过400KB。<br>Items中的每个属性都是一个 (K, V) pair 这里的value可以是集合、列表、String等数据结构。<br>Primary Key<br>用于唯一标识一个item，DynamoDB中支持两种类型的主键。<br>Partition Key（分区键）一种简单的Primary Key，仅有一个属性组成的Key，DynamoDB利用Key的Hash值计算Item应该被存储的分区 (Partition)。<br>Partition Key &amp; Sorted Key：由分区键和排序键组合的主键，DynamoDB利用Value确定存储的分区，对于在同一个分区中的Key按照Sorted Key进行排序。<br>注意：</li><li>Partition Key可以理解为 Hash Attribute，通过Hash计算，可以实现让Items 根据Key值平均分散到不同的位置上。<br>在DynamoDB中，Partition Key的属性只能为String, Number, Binary</li><li>Sort Key 可以理解为 Range Attribute，利用Sorted Key可以确保拥有相同Partition Key的item在物理结构上按照顺序紧密的存储在一起。<br>Secondary Index</li><li>Global Secondary Index：一种由Partition Key 和 Sorted 以外的Key组成的索引。</li><li>Local Secondary Index：一种由Partition Key和不同的Sorted Key组成的索引。<br>Item Distribution<br>DynamoDB 利用Partition 存储数据，Partition是基于SSD的并且会自动创建三个副本。</li></ul><ol><li>在只有Primary Key的情况下：<br>如果table中有一个Primary Key，DynamoDB会基于这个Key确定这个Key的分区；读写过程都是先基于Primary Key找到对应的Partition，然后Load和Store Data。</li></ol><p>注意：Partition Key最好选择那些差异程度最大的属性，来最大程度上避免collision。<br>2. Partition Key &amp; Sorted Key<br>写入的时候先根据Partition Key找到对应的物理分区，然后根据Sorted Key顺序写入；<br>读取的时候可以根据Partition Key进行读取，并且能够在Sorted Key上添加condition条件。</p><p>总结 &amp; 启发<br>Dynamo给我最大的启发就是它利用了一些业界的现有技术，打造了一个新概念的DB产品，很多Follow-UP的数据库都是基于Dynamo产生的，包括：RocksDB，VoltDB，以及字节内部使用的Abase，美团使用的TiDB。(A + B + C + D) 这种模式其实更多的被使用在学术界，笔者之前短暂从事过AI的研究工作，发现不少出名的工作都是基于“炒烂饭”被提出的。其实，借鉴前人的经验和工作并不是什么坏事，能够把握时机，在正确的时代背景下运用正确的技术手段也是一件伟大的工作。</p><p>Reference</p><ul><li>Dynamo Paper</li><li>OSPF</li><li>Vector Clock</li><li>Antiquity</li><li>Bayou</li><li>Google File System</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
      <tag>Amazon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GFS</title>
    <link href="/2022/05/22/Google%20File%20System/"/>
    <url>/2022/05/22/Google%20File%20System/</url>
    
    <content type="html"><![CDATA[<p>Google File System</p><ul><li>分布式文件系统，弱一致性模型 (Relaxed Consistency)；</li><li>谷歌三驾马车之一，与MapReduce和BigTable开启了新的大数据时代，引领工业界向NoSQL发展；</li></ul><p>推荐阅读：为了更好的体验，推荐移步至<a href="https://lo845xqmx7.feishu.cn/docs/doccncmo8iqkFFn424B8lZd44gh">飞书文档</a>。 <a href="https://lo845xqmx7.feishu.cn/docs/doccnadQrN9vNCbIGJh5YtTOGxh">GFS FAQ</a>。</p><p>Intro<br>单机系统的存储容量始终是有限的，随着数据量不断增大，如果只对单台机器进行scale up也会逐渐达到上限，所以scale out to multi-Server是发展的必然。<br>GFS主要支持谷歌内部的软件系统，比如：YouTube视频，Google Drive等等，谷歌并不对外开放GFS的接口，只对外提供上层服务。Bigtable 和 MapReduce 都是建立在GFS之上的应用。并且Hadoop生态中的HDFS也是在GFS系统之上发展出来的一个文件系统分支。</p><p>系统的主要目标和大多数分布式存储系统一致，主要包含三个方面：</p><ul><li>Reliable, High Performaces</li><li>Scalable</li><li>Available<br>系统设计时考虑的几个方面：</li><li>Normal Failure<br>文件系统由上百台廉价的存储机器组成，对外部大量的Client 提供服务。</li><li>Huge File<br>系统需要存储TB级别的数据集，由十亿级别的KB的数据组成，所以需要重新对磁盘IO和文件块大小进行评估。</li><li>Append rather than Overwriting<br>在GFS中几乎不存在随机写，并且一旦写入了文件，所有的读操作都是顺序读，系统的优化主要集中在这个点上。<br>Assumption<br>谷歌的工程师针对自家的业务场景对系统的情况规定了一些前提与假设：</li><li>系统中每一台Sever都是廉价的主机，所以经常会fail，需要对系统进行监控和容错。</li><li>GFS主要用于存储大文件，相比Chubby 主要是用来存储小文件的场景，GFS存储的文件量通常为几百万的100MB的文件甚至Multi-GB 的文件大小。</li><li>读场景主要有两种方式：<ul><li>大规模的顺序读写：顺序读写上百个KB大小的内容。</li><li>小规模的随机读写：几KB的在随机位置的读写。</li></ul></li><li>写场景主要是 大 和 顺序写，以Append的方式追加到文件末尾，并且GFS规定一旦文件写入就极少会被修改。</li><li>系统需要执行原子的高并发写入。</li><li>高持续带宽比低延迟更重要。<br>作为一个分布式文件系统，GFS在乎的是整个系统的吞吐量，由此可能会因为Replication 造成一些延迟的开销。</li></ul><p>Interface<br>不完全支持标准POSIX API接口，但提供了<code>create, delete, open, close, read, write</code>这些方法。<br>POSIX，Portable Operating System Inferface，可移植操作消停接口，是UNIX系统设计的一个标准。Linux和Windows部分支持该协议。</p><ul><li>Snapshot：可以对file 或者 目录打快照</li><li>Record Append：允许多个Client 同时向同一个文件追加记录，并且不用加锁。<br>Architecture<br>重点来了：GFS集群是一个经典的Master-Worker架构，Master充当任务分配和元数据管理的角色，而Worker是真正存储数据的角色。<br>整个系统的架构如下图所示。</li></ul><p>角色</p><ul><li>Chunk<br>数据块，每一个chunk的大小为64MB并且每一个chunk由一个uint64类型的 chunk handler 唯一标识，在创建的时候由master指定。为了保证容错，通常一个chunk会包含三个副本。<br>目前这种设计会造成hot spot，极端情况下，所有的Client都去访问某一个chunk，然而该chunk只有三个副本，会导致chunk所在的机器 IO次数急剧增大。<br>GFS的解决方案是给这些chunk更多的Replica，Author们也提到了可以让Client 向另一个 Client 请求数据。</li></ul><p>可以看到在GFS文件系统中使用了比Linux操作系统中更大的块，能够带来如下优点：</p><ul><li><p>减少Client 和 Master的交互次数<br>  一次请求就能获取所有chunk的位置信息，并且这个信息是可以在TTL规定的时间内缓存的 。</p></li><li><p>减少Client 和 ChunkServer的交互次数<br>  利用空间局部性和TCP的长链接，减少网络IO。</p></li><li><p>减少了元数据的大小<br>  如果chunk size设置的比较小，那么在master上就需要管理更多的元数据信息。一旦RAM装不下，就会使用Disk存储，这样磁盘IO又成为了性能瓶颈。<br>每一个文件会被split成不同的chunk，这些chunk可以存放在一台机器的磁盘上，也可以存放在不同机器的磁盘上。在GFS的视角看来，我需要存储和管理的文件就是chunk而不会关心file到底有多大；而从用户的视角来看，每一个file被切分出来的chunk1-n都是逻辑连续的。</p></li><li><p>Client<br>GFS Client 可以认为是依赖于 GFS lib 进行类库函数调用的线程，也是一个逻辑上的概念 [2]。<br>Client Side 不会缓存文件数据，因为数据太大了。</p></li><li><p>Master<br>提供In-Memory的元数据管理支持，主要包含：<br>无法复制加载中的内容<br>表示Master会通过operation log持久化到Disk上。对于chunks的位置信息，通过Heartbeats 周期性获取。<br>Master 在内存中存储了两个表：<br>Table 1:<br>Key<br>Value<br>Filename<br>An array of chunk handler (nv)</p></li></ul><p>Table 2:<br>Key<br>Value<br>Chunk handler<br>List of chunk servers<br>Chunk version number<br>Primary node<br>Lease expiration</p><ul><li>Chunk Server</li></ul><p>Consistency Model<br>Guarantees by GFS<br>文件名namespace的变化（文件创建）是由master在内存中进行的，整个操作通过namespace lock确保操作的原子性，master的operate log定义了全局创建操作的顺序。</p><p>这张表描述了在Append Record之后系统的状态：</p><ul><li>Consistent: 不论Client从哪一个Replica中读取数据，Client都能看到一致的信息。</li><li>Defined：当一个文件数据修改后，它能够保持Consistenct的状态并且所有Client能够看到全部修改的内容。是一种级别更高的一致性。</li><li>Consistent bu undefined: Client虽然能够看到相同的数据，但是不能及时看到其他进程的修改。</li><li>Inconsistent：不同Client能够在不同的时间看到不同的数据。<br>此外，表格中也显示了GFS中两种更新操作：</li><li>Write<br>Write是修改原来文件中的数据，相当于是一种overwrite的操作。</li><li>Record Append<br>Record Append是一种追加操作，它包含着“原子 + AtLeastOnce” 语义，因为没有AtMostOnce的语义，所以GFS是有可能存在Duplicate Record的。GFS为了保证写入的顺序，通常会由Primary Replica决定一个Chunk的写入顺序，并且由Master向Client返回一个Offset代表Record写入的起始位置。<br>Master 会通过心跳的方式与Chunk Server进行定期的通信，并且使用Version Number来判断Replica是否过期；一旦Master判定某个Replica是过期的，会对过期的Replica 进行GC。</li></ul><p>Client会对GFS的数据进行缓存，并且Client有可能缓存到旧数据，由于GFS中的数据都是以Append ONLY的方式写入的，所以Client 缓存的数据可以理解为只是部分不完整的数据，而不是错误的数据。<br>System InterActions<br>Read Process<br>Client 向Master请求文件所在的Replica，Master返回Chunk Handler和Location；<br>Client向最近的Replica发起请求，读取指定范围的数据。</p><p>Write Process<br>Lease &amp; Mutation Order：Master会给每一个Mutation的操作（每一个Mutation操作只对应一个Chunk，即不能超过chunksize）指定一个Primary Replica，给它赋予一个Lease，并且由这个Primary决定每chunk的写入顺序。<br>在Lease的时效 (60s) 内Primary可以不用与Master通信，而自行决定文件Append的执行顺序。<br>Master通过心跳的方式判断Primary的liveness，并且决定是否需要延长Lease的时效。</p><p>整个写操作可以分为如下7个步骤：</p><ol><li>Client向Master询问哪一个Chunk Server持有要进行写操作的Chunk的Lease</li><li>Master向Client返回Primary的Handler以及其他Replica节点的地址；Client收到Master的回复并进行缓存，当Primary不可达或者TTL后，会向Master按照Step1重新请求。</li><li>Client向所有的Replica推送数据，并且Replica会将数据写入到Buffer中。</li><li>当Replica都接收到了Client的数据，Client会向Primary发起Commit请求，Primary对多个写生成执行计划（主要是决定写入顺序），然后将该执行计划先应用于本地IO操作。</li><li>Primary将写操作转发给其他两个Replica，使其按照Primary的执行计划进行IO操作。</li><li>Follower给Primary 返回写入成功或者失败的Response。</li><li>Primary响应Client，并且返回该过程中发送的错误，Client如果收到了error，则会Reissue这次Write操作。（📢 这里会引起问题…）。<br>SnapShot<br>这个操作由于比较常见，所以在内容中增加一下。Redis持久化和一些新型的数据库引擎 (In Memory: BitCask; Disk: InnoDB-MVCC) 都使用了SnapShot来提升并发度保证一致性。<br>目标：尽量减少对正在进行写操作的影响。<br>GFS使用标准的Copy-On-Write技术来实现快照：</li></ol><ul><li>当Master收到SnapShot的请求后，首先会revoke所有Primary Replica的Lease，保证后续的Write操作都会经过Master。</li><li>当Lease撤回或者过期后，Master首先会将操作日志记录到磁盘，然后通过复制源文件以及目录树的metadata来将日志记录应用到内存中的状态。</li><li>当Client请求进行Write操作的时候，Master发现chunk上的引用技术大于1，便会重新命令chunk Server 创建一个新的chunk，在新的chunk上进行写操作。<br>Master’s Operation<br>Master主节点负责的工作有：</li><li>所有namespace的管理工作</li><li>管理整个系统中所有的chunk Replicas<ul><li>做出将chunk 实际存储在哪里的决定，创建新的Chunk和 Replica；</li><li>协调系统的各种操作（比如：读、写、快照），用于保证chunk正确且有效的进行备份。</li><li>管理chunkserver之间的负载均衡，回收没有被利用的存储空间。<br>Namespace Management and locking<br>不同于其他传统的文件系统，GFS并没有为每一个目录创建一个用于记录当前目录拥有哪些文件的数据结构，也不支持文件和目录的别名。GFS逻辑上把namespace当做一个查询表，用于将full pathname（目录或者是具体的文件）映射为metadata，并且使用前缀压缩使得这个表可以在内存中被高效的表示。<br>Master节点的每一个操作都必须先获得一系列的锁才能够真正的运行，当Client并发的对 <code>/home/user/bar</code> 和<code>/home/user/foo</code>进行修改的时候，会对bar和foo两个文件上写锁并对父节点上读锁。</li></ul></li></ul><p>一般来说，最底层的文件修改都需要上写锁，为了防止对文件进行并发的修改，而对于其父目录通常只需要上读锁，为了防止将目录重命名、删除和SnapShot。<br>并且所有的锁获取都需要按照顺序获取，以此来防止死锁。</p><p>Replicas Create &amp; Rebalance<br>Replicas 会在 Create, Re-replication, Rebalance的时候被创建。<br>创建<br>当Master需要创建Replicas的时候, Master会按照以下策略选择将 Replicas 放置到哪个位置上：</p><ul><li>Master希望选择一个磁盘使用率低于平均值的Server。</li><li>Master不希望所有最新创建的Replicas都集中在一台机器上，因为刚创建的Replicas可能隐含着即将进行大量的读写操作。</li><li>Master希望尽可能地把Replicas放到同一个DC中，不同的 racks，即不同的层上。<br>Re-Replication</li><li>某些Replicas不可用导致Replicas的数量低于预先设置的数量</li><li>程序员手动增加了Replicas的数量<br>Master会按照一定的优先级进行Re-Replicatoin：</li><li>首先，如果ChunkA的Replicas数量少于chunksB 那么优先对chunkA进行Re-Replication；</li><li>如果某个 chunk阻塞了Client请求的执行，那么优先对其进行Re-Replication；</li><li>最近活动 (Read &#x2F; Write) 的文件比最近删除的文件具有更高的优先级。<br>为了防止过多的clone操作占用系统的带宽，Master节点既会限制chunkserver的clone数量也会限制整个GFS集群同时进行clone的数量。<br>Rebalancing<br>Master会检查Replicas的分布，然后将Replicas移动到更合适的位置。通过这个机制，Master可以逐渐对新加入的Node进行Replicas填充，而不是瞬间加入大量的Replicas将其打爆。总的来说，重平衡是为了使磁盘利用率和Server QPS更加均匀的分布到各台机器上。<br>Garbage Collection<br>GFS使用了一种惰性删除的操作，当一个文件被应用删除时，Master节点会将此删除操作立即写入日志系统，但是Master不会立即将文件空间回收，而是将其文件名设置为一个不可见的name，并且包含删除指令的时间戳。<br>在Master定期对namespace扫描的过程中，如果发现一个chunk的删除时间已经超过三天，会将这个chunk的空间进行回收。在一些特殊的情况下，可能会出现orphaned chunks，chunkserver如果检测到orphaned chunk会立即将他删除。</li></ul><p>优点：这种删除方式简单可靠</p><ul><li><p>chunk可能在一些replicas上成功，在一些Replica上失败，因此会留下一些节点没有记录的chunk数据。当master发现其没有记录某个chunk信息的时候，会告知chunkserver，chunkserver会对那个空位置进行垃圾回收。</p></li><li><p>删除指令不一定能够送达chunkserver，在GFS的方案中，因为会有定时的心跳，因此master 最终一定会告知chunkserver删除相关的chunk。<br>缺点<br>这种方式在资源紧张情况下，无法及时回收存储空间。不过GFS提供了其他机制来确保快速的删除操作，比如动态地加快存储回收策略，在GFS不同区域采用不同的删除策略。<br>High Availability<br>Fast Recovery<br>Master 以及 Chunk Server不管出于何种原因的故障，都能在秒级时间恢复故障前的状态。<br>Chunk Replication<br>一个chunk复制到三台Replica上。<br>Master Replication<br>Shadow Master：在Master节点宕机后，Shadow Master能够对外提供只读的操作。Shadow Master与Master的通信类似MySQL PB 的Binlog操作。<br>Shadow Master允许略微滞后于Master，但是不会读到错误的旧数据，而是读到不完整的旧数据，Append Only保证的。</p></li></ul><p>Some Questions For MySelf &amp; Reader:</p><ul><li>GFS prohibits the size of a record append to exceed the chunk size. Use a sequence of events to demonstrate why a larger record append may lead to correctness issues.</li><li>When GFS can show some duplication?</li><li>Why can Client-B read Client-A’s update even though Client-A is returned an error?</li></ul><p>GFS FAQ<br>GFS Supplementary </p><p>Reference<br>[1] <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf</a><br>[2]<br><a href="https://spongecaptain.cool/post/paper/googlefilesystem/">https://spongecaptain.cool/post/paper/googlefilesystem/</a><br>[3] <a href="https://en.wikipedia.org/wiki/Google_File_System">https://en.wikipedia.org/wiki/Google_File_System</a><br>[4] MIT 6.824<br>无法复制加载中的内容<br>[5] Google Slides</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>File System</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
