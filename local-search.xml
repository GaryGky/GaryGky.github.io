<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>complete_system</title>
    <link href="/2022/12/11/complete-system/"/>
    <url>/2022/12/11/complete-system/</url>
    
    <content type="html"><![CDATA[<p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=Yzc4MmE3NGQ4ZjEwZjg3MWRlYzhiZTBlYTliMWY2YzRfYjZla1FJSHhYTnJZZkJVZEQ2S0R4SUM3TWJLdDBjWkdfVG9rZW46Ym94Y25aZkp5V0hGdDdOQjlZWXNwZDk4bmVmXzE2NzA3NjQxMDU6MTY3MDc2NzcwNV9WNA" alt="Auto complete system overview"></p><p>The requirement for auto complete system should be:</p><ul><li><p>Lightening fast reads. When users type in keywords, the service responds in a very fast way. Facebook found that an autocomplete needs to return suggestions in <a href="https://www.facebook.com/notes/10158791367817200/">less than 100 ms</a>.</p></li><li><p>Relavancy of suggestions: should return the most frequent query result from the database for one keyword.</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGE5MTkyYThkODhjMDRlNWJlZGNiZmI2OTk4NjU3NDhfS2NTR0NvSUJ3N3hOVk01ZVJ5dmY4R1pCUXNNRVJVbk1fVG9rZW46Ym94Y251aEx3blo3eEI0eWV2OWJoanFIR29lXzE2NzA3NjQxMDU6MTY3MDc2NzcwNV9WNA" alt="Google Search engine&#39;s auto complete"></p><h1 id="Understand-the-problem-and-Establish-design-scope"><a href="#Understand-the-problem-and-Establish-design-scope" class="headerlink" title="Understand the problem and Establish design scope"></a>Understand the problem and Establish design scope</h1><ul><li><p>Requirement Detail: Search Top-5 frequent suggestions for a keyword</p></li><li><p>User Scope: Assume there are 10 million DAU</p></li><li><p>No spell check</p></li><li><p>One of the important features of this system is that the suggested result is not user-specific. While on Facebook there is a lot of user-specific data such as <strong>feed</strong>.</p></li></ul><h2 id="Back-of-the-envelope-estimation"><a href="#Back-of-the-envelope-estimation" class="headerlink" title="Back-of-the envelope-estimation"></a>Back-of-the envelope-estimation</h2><p><strong>Traffic</strong>: Assume that on average, one user will search 10 times a day and in each query there are 20 characters  </p><p>QPS: 10,000,000 * 10 * 20 &#x2F; (24 * 60 * 60) &#x3D; 24000</p><p>Peak QPS: 48000</p><p><strong>Storage</strong>: each query contains 20 characters, so the size of a query is 20 bytes. It generates (10, 000, 000 * 10 * 20) bytes per day and assumes that the new query takes up 20% of the total queries.</p><p>So the storage space increases 0.4 GB per day.</p><h1 id="High-Level-Design"><a href="#High-Level-Design" class="headerlink" title="High Level Design"></a>High Level Design</h1><p>The client sends a request to the API gateway. The load balancer will choose one web server to handle the query request. </p><p>The Web Server will first compute the top-5 frequent keyword queries from the query history and then increase the querying keywork count by 1.</p><p>The storage layer could be NoSQL because the storage model does not need relational information and the query can be tolerated for <strong>eventually consistency. Availability weighs more in this scenario.</strong> Because we always expect the search server to respond to the user query and if the suggestion is not the up-to-date order, it will be acceptable.</p><h2 id="System-Architechture"><a href="#System-Architechture" class="headerlink" title="System Architechture"></a>System Architechture</h2><p>You should clarify if the search service is for an active online search application like Twitter or an offline search like Google Search.</p><p>The former one may require you to update the trie immediately, the latter one may tolerate the result is not up to date.</p><p><img src="/../img/tech/system%20design/complete%20system/architecture.jpg" alt="system high level architechture"></p><p>The update can not be simply done with incrby, because you must maintain the trie tree as well. It should not update the words but should also update the prefix. It would be too time-consuming if you wanted to do this in real time.</p><h1 id="Dive-into-Deep"><a href="#Dive-into-Deep" class="headerlink" title="Dive into Deep"></a>Dive into Deep</h1><h2 id="Trie-Tree"><a href="#Trie-Tree" class="headerlink" title="Trie Tree"></a><a href="https://leetcode.com/problems/implement-trie-prefix-tree/">Trie Tree</a></h2><p>For the <strong>auto complete</strong> problem, it is easy to recall that Trie tree fit the scenario well. In a view, the data structure would be like:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trie</span>&#123;<br>    <span class="hljs-comment">// assume the qeury is converted to lower-case word,</span><br>    <span class="hljs-comment">// there are 26 characters at most</span><br>    Trie[<span class="hljs-number">26</span>] children;<br>    <span class="hljs-comment">// false : the node is not the end of any word</span><br>    <span class="hljs-comment">// true : the node is the end of some words</span><br>    bool isEnd;<br>    <span class="hljs-comment">// only valid when isEnd == true, record the frequency of a keyword</span><br>    <span class="hljs-type">int</span> frequency;<br>&#125;<br></code></pre></td></tr></table></figure><p>A picture from <a href="https://www.amazon.sg/System-Design-Interview-insiders-guide/dp/B08CMF2CQF/ref=asc_df_B08CMF2CQF/?tag=googleshoppin-22&linkCode=df0&hvadid=404658063268&hvpos=&hvnetw=g&hvrand=1585535126343690537&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9062543&hvtargid=pla-934212337151&psc=1">Alex Xu’s book</a>:</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NWY1YjUwNWNiOGVhODY0YmU5ZDY3OGFhMWU5NzZiZWJfeUx4TmFTUE5sNEhVNGdNWUlUUmtBZXJDeWQ1RGhHZXNfVG9rZW46Ym94Y25nVkU5cnhDTDk0UTk0aG4zTXNackxlXzE2NzA3NjQxMDU6MTY3MDc2NzcwNV9WNA" alt="Trie Tree example"></p><p>Query Process: Query the word is to query the trie tree. If we find prefix in the trie tree, then we have to find all the children under the node and sort them within a heap to generate the top-5 results.</p><p>The total time consists of three parts: O(L) + O(N) + O(KlogK)</p><ul><li><p>The first part is raised by the length of type-in keyword</p></li><li><p>The second part is all the completions in the Trie leafnodes</p></li><li><p>The third part is sorting out raw results</p></li></ul><p>However, the process is really time-consuming, assume that the user only enters ‘s’ and you have to traverse all the leaf nodes starting with ‘s’. </p><p>Quick Calculation:</p><p>The time complexity is <strong>O(N)</strong> for querying the suggestion where N is the total number of keywords (completions) in the Trie. </p><p>This is really a bottleneck of the system which increases the responding time.</p><p>There are some optimization we can do to remove the above time bottleneck.</p><h3 id="Remove-O-N"><a href="#Remove-O-N" class="headerlink" title="Remove O(N)"></a>Remove O(N)</h3><p>One possible way to improve the query process is to cache the <strong>top-5</strong> result for a node. In Google Search scenario, the top-5 result can read stale data for the reason that the suggestion can be a little bit outdated. While in the Twitter scenario, the top-5 results should be as accurate as possible.</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NmVjMzViZmNlNTNiMjlmZTk0ZDYxZTQ2ODM4MDQ5MjBfY3pMWEFpRjVxNmIzaDNvdTJsZVZKbGJZU0draW1oYlFfVG9rZW46Ym94Y25wTzZhUURUTWxpNm1xSUJzOU5WOGRnXzE2NzA3NjQxMDU6MTY3MDc2NzcwNV9WNA" alt="cache the completion for trie nodes"></p><p>Trade-offs</p><p>For the keyword ‘bee’, we have to store it multiple times in the node (‘b’, ‘be’, ‘bee’). <strong><a href="https://medium.com/@prefixyteam/how-we-built-prefixy-a-scalable-prefix-search-service-for-powering-autocomplete-c20f98e2eff1">But since speed of reads was our priority, we were happy to make these trade offs</a>****.</strong></p><h3 id="Remove-O-L"><a href="#Remove-O-L" class="headerlink" title="Remove O(L)"></a>Remove O(L)</h3><p>If we can limit the length of keywords, like we define the max length of L as 50 chars. For searching more than 50 chars, we can simple block the auto complete service. </p><p>This is what Google Search does. You can try to paste different things into the search bar. And you may find that if you type in a sentence which is too long, there won’t be any suggestions.</p><h3 id="Remove-O-KlogK"><a href="#Remove-O-KlogK" class="headerlink" title="Remove O(KlogK)"></a>Remove O(KlogK)</h3><p>To most of the auto- complete suggestions, it only requires us to generate the top5 or top10 related results. So the K can be regarded as a constant and O(KlogK) is nothing more than O(1).</p><h2 id="How-to-Store-Trie-Tree-in-the-DB"><a href="#How-to-Store-Trie-Tree-in-the-DB" class="headerlink" title="How to Store Trie Tree in the DB"></a>How to Store Trie Tree in the DB</h2><h3 id="Stored-in-Document-DB"><a href="#Stored-in-Document-DB" class="headerlink" title="Stored in Document DB"></a>Stored in Document DB</h3><p>From <a href="https://www.amazon.sg/System-Design-Interview-insiders-guide/dp/B08CMF2CQF/ref=asc_df_B08CMF2CQF/?tag=googleshoppin-22&linkCode=df0&hvadid=404658063268&hvpos=&hvnetw=g&hvrand=1585535126343690537&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9062543&hvtargid=pla-934212337151&psc=1">Alex Xu’s book</a>, the idea is to take a snapshot of the DB weekly and store the data into Document NoSQL like MongoDB.</p><h3 id="Stored-in-KV-Store"><a href="#Stored-in-KV-Store" class="headerlink" title="Stored in KV Store"></a>Stored in KV Store</h3><table><thead><tr><th><strong>Key</strong></th><th><strong>Value</strong></th></tr></thead><tbody><tr><td>c</td><td>[car: 30, cat:20, curry:10]</td></tr><tr><td>ca</td><td>[car:30, cat:20, canada:10]</td></tr></tbody></table><p>This gives us a few advantages, such that we can access any prefix in O(1) time. This eliminates the tree traverse process in the memory. But it also increases the storage space because it loses the conventional trie’s ability to share common prefixes. A hashmap also requires more space to store the meta data such as load factor and map size.</p><p><strong><a href="https://medium.com/@prefixyteam/how-we-built-prefixy-a-scalable-prefix-search-service-for-powering-autocomplete-c20f98e2eff1">But again, we were more than willing to trade space for speed of reads.</a></strong></p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NDhmNTYxYTVmM2RlODEzMzM5ZTY0N2JkNzFmZTU3YmZfUGFPOVlFQ2pMa0hKdzJSdld5WGpVTGRYRnF0NktXVWtfVG9rZW46Ym94Y25rMFlYdk51SDJ6YXR5UHBNYXpvVXloXzE2NzA3NjQxMDU6MTY3MDc2NzcwNV9WNA" alt="tradeoffs choosing the data structure"></p><p>We can use <strong>Redis</strong> <strong>&#x2F; Abase</strong> to store the trie data. To store the completion data, we could use Redis list or <strong>Redis Zset</strong>.</p><table><thead><tr><th></th><th>Redis List</th><th>Redis Zset</th></tr></thead><tbody><tr><td>Performance</td><td>Search for O(1)Update for O(K)</td><td>Search and Update for O(logK)</td></tr><tr><td>Operation Complexity</td><td>Need to take care of sorting and de-duping. When updating, it requires us to take two round trips with Redis, load all the lists and increase and insert the new entry in-place.</td><td>Not require to take care the sorting and deduping logic.  The ZSET will maintain the order and uniqueness through skiplists.</td></tr><tr><td>Commands</td><td>LRANGE: Load all the list LREM: Remove the updated entryLINSERT: Insert new entry into the list</td><td>ZRANGE: for search  ZINCRYBY: for update</td></tr></tbody></table><h3 id="Cache-by-Redis-and-Persist-By-MongoDB"><a href="#Cache-by-Redis-and-Persist-By-MongoDB" class="headerlink" title="Cache by Redis and Persist By MongoDB"></a>Cache by Redis and Persist By MongoDB</h3><p>MongoDB is a key document NoSQL which fits well with the Redis KV model. Having similar models of abstraction among data stores is convenient, because it makes the persistence process easier to reason about.</p><p>For the cache miss handling, we can implement Cache Aside Pattern. Note that the logic for searching in the event of cache miss only happens quite a few times, and when there’s cache miss it can quickly load the data into Redis so that the next query can hit the cache. Thus, the slower and more complicated logic for <strong>searching persistent stores</strong> only happens in a relatively small number of cases.</p><h2 id="How-to-update-the-Trie-Tree"><a href="#How-to-update-the-Trie-Tree" class="headerlink" title="How to update the Trie Tree"></a>How to update the Trie Tree</h2><p>The query service can store the query history and periodically generates the analysis log based on the historical data. With this analysis log, we can build the Trie once and use it for a long time.</p><p><img src="/../img/tech/system%20design/complete%20system/update_trie.jpg" alt="offline update trie by OLAP"></p><p>But note that this mechanism is only feasible for the Google Search scenario where the auto complete suggestion does not require realtime update. For the Twitter auto complete scenario, you may want to figure out some faster way to update the Trie store.</p><h2 id="How-to-partition-the-data-efficiently"><a href="#How-to-partition-the-data-efficiently" class="headerlink" title="How to partition the data efficiently"></a>How to partition the data efficiently</h2><p>The words starting with ‘s’ are more than the words starting with ‘x’ and this might cause sharding inbalance. To mitigate the problem of unevenly distributed characters.  We may introduce a shard manager to do the routing.</p><p><img src="/../img/tech/system%20design/complete%20system/sharding.jpg" alt="partition mechanism"></p><h1 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h1><p>Some questions to think about:</p><ul><li>How do you extend your design to support multi-language?</li></ul><blockquote><p>Hint: Unicode</p></blockquote><ul><li>What if we want to do auto-complete based on different countries?</li></ul><blockquote><p><a href="https://medium.com/@prefixyteam/how-we-built-prefixy-a-scalable-prefix-search-service-for-powering-autocomplete-c20f98e2eff1">Multi tenant solution</a></p></blockquote><ul><li>How can we support real-time search queries?</li></ul><blockquote><p>Update immediately</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.amazon.sg/System-Design-Interview-Insiders-Guide/dp/1736049119/ref=asc_df_1736049119/?tag=googleshoppin-22&linkCode=df0&hvadid=405606626615&hvpos=&hvnetw=g&hvrand=5683234730197475962&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9062543&hvtargid=pla-1645933631661&psc=1">Alex Xu’ System Design Book</a></p><p><a href="https://www.facebook.com/notes/10158791367817200/">FaceBook: The life of a typehead query</a></p><p><a href="https://medium.com/@prefixyteam/how-we-built-prefixy-a-scalable-prefix-search-service-for-powering-autocomplete-c20f98e2eff1">Medium: Auto completion system design</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>system design</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>chat_system</title>
    <link href="/2022/12/11/chat-system/"/>
    <url>/2022/12/11/chat-system/</url>
    
    <content type="html"><![CDATA[<h1 id="IM-系统的历史"><a href="#IM-系统的历史" class="headerlink" title="IM 系统的历史"></a>IM 系统的历史</h1><p>传统的消息系统是推送消息，不进行持久化，即便是传递离线消息，当接收方收到了发送方的消息之后，也会从数据库中删除对应的消息。在传统的IM系统中，没有消息回溯的能力。</p><p>现代的消息系统中，消息是先存储后同步。消息存储库保留全量的会话消息，主要用于支持<strong>消息漫游（消息的离线推送）</strong>。消息同步库保存的是所有未同步到接收方的消息，如果接收方在线，则这是一个更优的同步路径，消息能够直接传递到接收方，如果接收方离线，下次登录的时候会从消息同步库中拉取未同步的消息。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NGVmMTY1NWIzOTNjOTUxMTI4M2VmNjQ2NGQ5OTdmZWZfbk1yNzEyQkhURjhlck9wdnNtZVJhSzdOdTZsMXQ3VVpfVG9rZW46Ym94Y25rTExVWDR5cFRVR09Xa3VDVTllemxjXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><p>Chat System 就是一个IM即时通讯软件，用户A向用户B发送消息，或者用户A向群组发送消息；用户B或者群组内的其他成员能够即时地获取到A发送的消息。</p><p>所以从High Level的角度看一个消息系统应该具备以下功能：</p><ul><li><p>消息存储：存储所有用户发送的消息，以便用户的立即读取和历史消息的回溯；</p></li><li><p>消息推送 (Online)：A 发送了消息之后，要立即推送到好友B的消息列表中，并且延迟要求尽可能的低；</p></li><li><p>消息通知 (Offline)：A发送了消息之后，B不处于在线的状态，所以需要发送系统通知提醒B去阅读新的消息</p></li><li><p>多端同步：不同的设备能够读到的消息内容是一致的</p></li></ul><p>暂时无法在飞书文档外展示此内容</p><h1 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h1><p><strong>接入层</strong>：接入层提供客户端接入服务：包括与客户端保持短连接和长链接，对于用户在线的情况 （A和B正聊得热火朝天），此时 A B都会与服务建立长连接，WS协议提供了全双工推送的能力，由服务端将消息推送给用户。</p><p><strong>逻辑层</strong>：逻辑层提供业务逻辑的封装，包括：服务发现，<strong>消息推送</strong>，和数据持久化，用户状态更新，好友关系维护等。</p><p><strong>数据存储层</strong>：数据存储服务应该采用 RDB + NoSQL混合的方式，关系型数据库可以提供用户设置，用户Profile，好友列表等数据模型的存储，NoSQL可以用于存储消息的实体，以追求更极致的性能和更好的Scalability。</p><p>暂时无法在飞书文档外展示此内容</p><h2 id="接入服务"><a href="#接入服务" class="headerlink" title="接入服务"></a>接入服务</h2><p>用户A在登录之后，由LB路由到正确的 机房&#x2F;集群&#x2F;服务节点上，请求 ZooKeeper来发现当前可用 Chat Server，并从中选择一台Assign给User A，之后User A与这台Server建立WS协议；（此时另一个User B 可能已经连接了同一台或者另一台 Chat Server），双方进行实时地聊天。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NzM2MDBiNjhiODE5ZDQ3ODg4YzUyMjNiYWFkMDA0YzBfOFF4TWwwVlNDUGhtSVN6eTdtaUFTZlJSSW01dHVyVERfVG9rZW46Ym94Y25Mc2hmN2YwZ3RqYW5nY1ZxaklrY0JlXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><h3 id="Naive-数据模型"><a href="#Naive-数据模型" class="headerlink" title="(Naive) 数据模型"></a>(Naive) 数据模型</h3><p>大多数IM系统的 Chat Message都是使用 K&#x2F;V 的NoSQL数据库进行存储的，数据模型如下，Primary Key定义了用于消息查询的键。</p><p>暂时无法在飞书文档外展示此内容</p><p>使用NoSQL作为存储层的主要考虑是：</p><ul><li><p>NoSQL 具备较好的 scale out特性</p></li><li><p>NoSQL 的可用性较高，延迟低</p></li></ul><h3 id="现代IM系统存储模型-Timeline"><a href="#现代IM系统存储模型-Timeline" class="headerlink" title="现代IM系统存储模型 Timeline"></a>现代IM系统存储模型 Timeline</h3><p>Timeline 是一个类似 Message Broker的模型，有一个生产端 (PUB) 和 多个订阅端 (SUB): 同一个用户的不同设备。</p><ul><li><p>每一个消息有一个独立的SeqID，严格递增，新的会话消息通过Append的方式追加到timeline 末尾。</p></li><li><p>接收端会维护消息位点 cursor，可以根据cursor的值定位到 Timeline中某个消息。</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=OWQxNTFmMGRkNzgxY2ExNjViMzQ1OWJhOTgxOTcyMzhfRmxPNGVzZkRwZG11Y1hGRzdNRGpWUWlPdkNma0lUU2hfVG9rZW46Ym94Y25La1BBQ3lPUGFGaHoxWTVXRE1sbTliXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><table><thead><tr><th></th><th>消息同步库</th><th>消息存储库</th></tr></thead><tbody><tr><td>数据模型</td><td>Timeline 模型</td><td>Timeline 模型</td></tr><tr><td>写能力</td><td>高并发写，峰值十万级别TPS</td><td>高并发写，峰值十万级别QPS</td></tr><tr><td>读能力</td><td>高并发读，峰值TPS为十万</td><td>少量范围读</td></tr><tr><td>存储规模</td><td>保留一段时间的同步消息，TB级</td><td>保留全量的消息，百TB级</td></tr></tbody></table><p>对于数据库的要求包含以下几点：</p><ul><li><p>不要求关系型，但是需要支持队列模型，并且能够支持自增的SeqID</p></li><li><p>能够支持高并发的写和范围读取</p></li><li><p>能够保存海量数据</p></li><li><p>能够为数据定义生命周期</p></li></ul><p>Facebook Messenger 使用 HBASE作为消息存储NoSQL，Discord 使用Cassandra作为底层的存储。阿里试用TableStore进行存储，是基于LSM引擎的分布式NoSQL，支持自增列，能够完美实现 Timeline这样的逻辑模型。</p><h2 id="消息同步流"><a href="#消息同步流" class="headerlink" title="消息同步流"></a>消息同步流</h2><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=ZWNmNjk0ZTljZTNhMDUxZGViYzA0YjFmZjVjYWFhZGNfYUl5a0t1MFVxV0lERTB0dmIyYnFzakxXZnBDN3VmMHNfVG9rZW46Ym94Y25mTkNXV09SWkhRSE5ES05vaTZ0SkhkXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><p>消息的同步模型分为读扩散和写扩散两种方式</p><ul><li><p>读扩散：仅写入到会话的timeline中，每个用户在登录后或者打开会话的时候去会话的timeline中拉取全量的数据。</p></li><li><p>写扩散：写入到会话的timeline中，并且写入到会话中每个用户的timeline中。</p></li></ul><h1 id="飞书IM-Cloud"><a href="#飞书IM-Cloud" class="headerlink" title="飞书IM Cloud"></a>飞书IM Cloud</h1><ul><li><p>用户之间的消息传递依然是通过云端进行传输，云端需要负责消息的存储</p></li><li><p>每一个Receiver有一个 Inbox (用户维度)</p></li><li><p>服务端维护维护device维度的cursor，每次使用cursor去读取inbox中的数据</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=YmZkN2VmMWExYTA2OTU2YTUyYmFiYjdmNGRlMTA1YzlfWnh0cFNxZ3VhaDBmb3hrdTlUdHh4VmRzNzRrQVFSOG9fVG9rZW46Ym94Y25TY3BwNUNtZUFESURYMHp0SUw1a01lXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><h2 id="消息收取"><a href="#消息收取" class="headerlink" title="消息收取"></a>消息收取</h2><p>消息的收取由inbox完成，inbox需要支持以下两种操作</p><ul><li><p>Scan 通过cursor一次性拉取所有未读消息</p></li><li><p>Append 将消息顺序写入到 inbox 中存储消息的数据结构，为了保证inbox写入的顺序性，所有对于一个inbox的写入应该路由到一个线程上</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=MmQ0MTA1YWU2ZDY3Mjk4YWUzM2EwOWU5YTE3N2Q5MmFfcWhtUWFydXVLbG9JVlFJYkJNRkhhQjBWSjVvTzlXSkNfVG9rZW46Ym94Y243RmJibE1qWHhDenZmaTA2d09zQkVnXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><h2 id="消息分发"><a href="#消息分发" class="headerlink" title="消息分发"></a>消息分发</h2><p>整个过程可以抽象成会话分发和成员分发两步：</p><ul><li><p>会话分发使用 会话ID作为Partition Key保证一个会话的消息能够顺序写入到第二级的MQ中</p></li><li><p>用户级别的MQ以UserID为Partition Key 保证从会话MQ接收到的MQ能够顺序写入一个用户的 Inbox</p></li></ul><p>注意：这里使用了两级MQ，任意一级的MQ中如果有一个Partition阻塞，就会触发Kafka的扩容，导致消息乱序，例如：消息cursor已经读到了X，但是新写入的消息会因为Rebalance被放到X之前的某个位点，此时这条消息就读不到了。</p><p>原因是在当前的设计中是先生成index，然后再Append到inbox末尾的，Append相当于NoSQL的一个set操作。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NDUyYjJjMjFjZTg2YmNlNWYwNWEyNTE3N2JkZTNjNjNfejBwZGUxa21BMkxza3RaYTN3REFYb21yMVFUeGh1MzhfVG9rZW46Ym94Y25ZVDJRYklzbWtwclA1NEFXSHk1R2piXzE2NzA3NjQwNzU6MTY3MDc2NzY3NV9WNA" alt="img"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>现代 IM的架构主要是基于 Timeline这一逻辑模型展开，Timeline模型对于存储层没有关系型的要求，但是需要实现SeqID的自增。Table型的数据库能够较好地支持这种场景的需求。</p><p>基于timeline的消息存储和推送模型，还可以应用在Feeds流、实时消息同步、直播弹幕等场景。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://developer.aliyun.com/article/253242">阿里云现代IM系统</a></p></li><li><p><a href="https://www.amazon.sg/System-Design-Interview-insiders-guide/dp/B08CMF2CQF/ref=asc_df_B08CMF2CQF/?tag=googleshoppin-22&linkCode=df0&hvadid=404658063268&hvpos=&hvnetw=g&hvrand=11142147491480852710&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9062543&hvtargid=pla-934212337151&psc=1">System Design Interview Volume-1 Chapter 12</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>system design</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>troubles_in_distributed</title>
    <link href="/2022/11/15/troubles-in-distributed/"/>
    <url>/2022/11/15/troubles-in-distributed/</url>
    
    <content type="html"><![CDATA[<h2 id="Faults-amp-Partial-Failures"><a href="#Faults-amp-Partial-Failures" class="headerlink" title="Faults &amp; Partial Failures"></a>Faults &amp; Partial Failures</h2><p>对于单机系统来说，正常情况下，相同的程序执行结果都是一致的，一旦CPU执行失败就会以Crash 的方式返回错误；(<strong>Deterministic</strong>)</p><p>而在分布式系统中，如果一个操作需要涉及多个节点，可能会出现部分节点正常而部分节点异常的情况。由于网络延迟的存在，Client 有时候并不知道在某个节点上的操作是否成功或者失败了。(<strong>Non-Deterministic</strong>)</p><h2 id="Cloud-Computing-amp-SuperComputing"><a href="#Cloud-Computing-amp-SuperComputing" class="headerlink" title="Cloud Computing &amp; SuperComputing"></a>Cloud Computing &amp; SuperComputing</h2><p>构建大规模系统的两种方式：</p><ul><li><p>超算：HPC，一台超算能够同时配备上千块CPU，用于执行一些<strong>计算密集型</strong>的任务：如：深度学习中的矩阵计算或者是一些模拟任务。（Shared Everything 架构）</p></li><li><p>云计算：云计算是通过公网IP和以太网进行通信的，它具备动态扩容和极致高可用的特点。(Shared Disk 架构）</p></li></ul><p>这两种系统的区别在于：</p><ul><li><p>互联网服务基本都是在线实时的服务，主要是为降低延迟，提升用户体验而设计的。而超算则是服务于一些离线的服务，这对于用户的响应要求不是很高。</p></li><li><p>超算通常都是使用专门的网络拓扑进行通信，并且每台单机的性能非常高，保证它节点直接通信的可靠和高效；而云服务基本都是建立在普通节点上的，这些服务节点的宕机率远远高于HPC的节点，同时云服务通过以太网进行通信，这样的网络链路也是不可靠的。</p></li><li><p>在一个由成千上万个节点的组成的云服务中，可以认为在每一个时刻都有failover发生，这对系统的 fault-tolerance 提出了更高的要求（Google File System）。一旦系统的恢复时间拉得很长，那么有可能导致服务的雪崩。</p></li><li><p>HPC的计算节点都是聚集在一起的；而云服务的节点为了得到更低的延迟，通常会部署在靠近用户的边缘节点上，这样系统中每个节点的通信开销将大大增加。</p></li></ul><p>分布式系统的目标：基于不可靠的节点建立可靠的服务。</p><p>在传统的CS 系统中，有许多这样的例子：</p><ul><li><p>在操作系统中，CRC或者其他校验算法能够检测出 bit级别的错误 ( checksum )。</p></li><li><p>在计算机网络中，建立在不可靠的IP层之上的TCP 协议能够对应用层提供可靠的通信 (TCP Reliable Commucation)。</p></li></ul><h2 id="Unreliable-Network"><a href="#Unreliable-Network" class="headerlink" title="Unreliable Network"></a>Unreliable Network</h2><p>Shared-Nothing 架构（Spanner），每一台节点都是平等地，独立地拥有自己的CPU，内存和磁盘，节点中的通信只能通过 网络请求 (<strong>RPC</strong>) 完成。Shared-Nothing 架构过去成为构建分布式系统的主流方法，主要是因为这个方案比较<strong>经济</strong>，可以利用普通的节点来实现高可用的分布式服务。</p><p>在不可靠的网络下，如果Client 没有收到请求，是无法辨别出错的情况的，以下任一种情况都是有可能的：</p><ul><li><p>请求没有到达 Server</p></li><li><p>Server Failover</p></li><li><p>Server 已经执行了Request 但是Response丢失。</p></li></ul><p>处理响应不可达情况的一种常用的方式就是 <strong>Timeout，</strong>只要超过一定时间 Server 没有给出响应，那么就会认为这次请求失败。</p><h3 id="Detecting-Faults"><a href="#Detecting-Faults" class="headerlink" title="Detecting Faults"></a>Detecting Faults</h3><p>许多系统都需要网络故障探测：</p><ul><li><p>Load Balancing: 检测集群中机器的活性，如果一个Node failover了，就不应该再将请求路由到该Node上。</p></li><li><p>在Single-Leader的服务中，如果Leader Failover了，需要从剩余的节点中选出新的Leader。</p></li></ul><p>但是由于网络延迟总是存在的，系统并不能完全掌握节点的存活情况，在一些系统中，会对failover的情况给Client 一些 提醒：</p><ol><li><p>如果能够检测出某个节点存活，但是TCP端口上没有绑定进程，操作系统会自动给发往那个进程的请求返回一个 FIN 或者 RST 报文。</p></li><li><p>如果一个进程 Crash了，但是操作系统仍在运行，可以用一个脚本主动将进程Crash的消息Push到其他节点上。（<strong>HBase</strong>）</p></li><li><p>网络管理员能够通过DataCenter提供的管理接口，监控DC 中节点的存活情况；</p></li><li><p>如果IP层不可达，可以通过ICMP 报文的返回值判断是否有不可到达的package，进而判断Server 进程的存活情况。</p></li></ol><h3 id="Timeouts-and-Unbounded-Delays"><a href="#Timeouts-and-Unbounded-Delays" class="headerlink" title="Timeouts and Unbounded Delays"></a>Timeouts and Unbounded Delays</h3><p>如果timeout 是解决网络故障的唯一方法，工程师也不能完全确定 timeout的时长：</p><ul><li><p>如果timeout 时间设置得过长，那么Client需要一直阻塞直到该节点declare failover，在等待的这段时间内服务不可用；</p></li><li><p>如果timeout时间设置得过短，(timeout之后立即认为节点failover) ，这样就会出现节点本身是健康的，而因为网络延迟 &#x2F; Spike等原因触发了timeout的机制，所以被动地被认为failover。这样会造成的后果就是在网络已经非常<strong>overloaded</strong>的情况下，需要进行请求的重新路由，进而引发更多的机器崩溃。</p></li></ul><p>理想情况下，一次网络的RTT上界 + 一次请求处理时间的上界就应该是timeout的合理阈值，但是大多数异步的网络（分布式系统），网络的delay 都是 <strong>unbounded</strong>的，即：没有明确的时间范围。</p><p>在公有云和共享数据中心中，实例通常是用容器来部署的，此时不同的服务可能共享的是一台机器上的资源（CPU &#x2F; 内存 &#x2F; Disk），如果 A Noisy Neighbourhood 占用了过多的资源，就很有可能导致当前服务的网络delay增加。</p><h2 id="Unreliable-Clock"><a href="#Unreliable-Clock" class="headerlink" title="Unreliable Clock"></a>Unreliable Clock</h2><p>分布式系统的时钟是一个非常tricky的设计，每台计算机中都会包含一个<strong>石英震荡</strong>的元件，基于此来产生时间，但是每台机器的石英中并不是完全精确的，因此有些机器可能会震荡得快，另一些机器可能会震荡得慢，导致Cluster中的机器Local Time不一致。</p><p>现代系统除了使用石英钟之外，还会使用 NTP来对系统的时钟进行调整，即通过<strong>网络授时</strong>来对节点当前的时间进行统一。</p><h5 id="Time-of-Day-Clocks"><a href="#Time-of-Day-Clocks" class="headerlink" title="Time-of-Day Clocks"></a>Time-of-Day Clocks</h5><p>Time-Of-Day Clocks 返回的时间就是Wall Clock Time，与 (midnight, 1, Jan, 1970 UTC) 对齐的时间。</p><p>Linux: <code>clock_gettime(CLOCK_REALTIME)</code></p><p>Java: <code>System.currentTimeMillis()</code></p><p>Linux 和 Java的这两种时间调用 call 的都是 Time-Of-Day Clocks，Wall Clock通常都是由网络授时服务产生的，所以理想情况下是一种同步时钟。但是由于不同节点到NTP节点的距离不同，所以肯定会存在网络延迟导致的误差。</p><h5 id="Monotonic-Clocks"><a href="#Monotonic-Clocks" class="headerlink" title="Monotonic Clocks"></a>Monotonic Clocks</h5><p>Monotonic Clocks 通常用来丈量 Duration，比如：timeout 或者 service response time。Linux中的 <code>clock_gettime(CLOCK_MONOTONIC)</code> 以及 Java 中的 <code>System.nanoTime()</code> 获取的都是 Monotonic Time。</p><ul><li><p>当一个节点距离NTP服务较远的时候，它的 TimeofDay Clock是有可能被往前拨。</p></li><li><p>Monotonic Clock的特点是它永远是向前走的，而不会被回拨。</p></li></ul><p>在分布式系统中，可以用Monotonic Clock来准确的计算 Elapsed Time (e.g Timeout) ，但是用Monotonic time 的来衡量时间绝对值是没有意义的。</p><h3 id="Clock-Synchronization"><a href="#Clock-Synchronization" class="headerlink" title="Clock Synchronization"></a>Clock Synchronization</h3><p>Wall-Clock Time 需要通过网络授时或者本地原子钟进行校对，但是由于时钟的偏移，会产生较大的误差，Google 曾经公布过对服务器墙钟时间的调查结果：每30秒同步一次的时钟会产生6ms的偏移，每天同步一次的时钟会产生17s的偏移。</p><p>到此，讨论了分布式系统不同于单机系统的点：没有共享内存以及只能通过network上的事件进行通信，由于时钟不确定性和操作系统中断的随机性，会出现Unbouded的Delay，导致Client 无法明确的感知Server端出现的问题，进而进入选择的窘境。</p><h2 id="Knowledge-Truth-and-Lies"><a href="#Knowledge-Truth-and-Lies" class="headerlink" title="Knowledge, Truth and Lies"></a>Knowledge, Truth and Lies</h2><h3 id="The-Truth-is-defined-by-Majority-of-Nodes"><a href="#The-Truth-is-defined-by-Majority-of-Nodes" class="headerlink" title="The Truth is defined by Majority of Nodes"></a>The Truth is defined by Majority of Nodes</h3><p>分布式系统的原则就是避免单点问题的出现，不能依赖一个 Node 的信息做出决定，常用的方法是使用系统中大多数节点的投票（<strong>QUORUM</strong>）来决定。</p><h4 id="The-Leader-and-the-Lcok"><a href="#The-Leader-and-the-Lcok" class="headerlink" title="The Leader and the Lcok"></a>The Leader and the Lcok</h4><p>ONLY One的场景：</p><ul><li><p>Leader base的分布式系统中只能存在一个Leader，否则就会出现脑裂；</p></li><li><p>同一把锁在任一时刻只能有一个lock holder；</p></li><li><p>数据库层面的 ONLY Once，一个用户只能有一个用户名（例如：工作邮箱）</p></li></ul><p>在一些情况下，即使某个节点认为它是leader，实际情况却不是这样，比如一个处于网络分区的leader实际上不是整个集群的leader，或者一个持有分布式锁的节点持有锁后，因为 Long GC而过期了，导致它仍然认为自己持有锁，而继续进行操作。</p><p>如果其他的节点相信了错误的 leader &#x2F; lock holder ，整个系统就会产生错误的行为。例如，在分布式锁的场景下，Client 1获取到锁之后进行了一段很长时间的GC，当Client 1恢复之后，继续向DB发起写请求，此时便会造成数据的不一致。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=OWJlNWQ0MWNmOWY3OTJmYzUzZDIwYWVkODE3NTIxNmRfU1lIblRtYWFubkxaZnZheFBRRnlseU1ka0c2a3hKWkVfVG9rZW46Ym94Y25LV3ZyeW5UUnRnM0hIVzI4MGhObHhoXzE2Njg1MTk4NDA6MTY2ODUyMzQ0MF9WNA" alt="img"></p><p>上图这种情况是实际发生过的，HBase就曾经因为这种问题发生过事故。</p><h4 id="Fencing-Token"><a href="#Fencing-Token" class="headerlink" title="Fencing Token"></a>Fencing Token</h4><p>Fencing Token 是用来防止上述问题发生的，一个典型的 fencing token generator就是ZooKeeper 的 zid，因为它能够保证<strong>自增</strong>。除此之外，下游系统也需要配合fencing token完成任务，下游系统（DB 或者 其他微服务）要能够鉴别 fencing token，拒绝掉过期的 token。对于不支持fencing token校验的系统来说，需要想出一种校验方式，例如 文件系统可以将 fencing token 编码到文件名中。</p><p>Server 端校验 Client 的token 是非常有必要的，因为server 不能假设Client总是正常工作的，所以对Client 进行校验能够对服务产生一定的保护作用。</p><h3 id="Byzantine-Fault"><a href="#Byzantine-Fault" class="headerlink" title="Byzantine Fault"></a>Byzantine Fault</h3><p><strong>拜占庭将军问题</strong></p><p>拜占庭将军问题在分布式系统中指的是，一个叛军节点向集群中其他节点传递错误的信息，导致整个系统无法达成共识或者达成错误的共识。在 Byzantine Fault环境下达成共识的方法通常被称为  Byzantine General problem</p><ul><li><p>在太空环境下，计算机的内存或者CPU寄存器可能受到宇宙辐射的干扰产生一些随机的行为，但太空飞船一旦发生错误将产生十分严重的后果，所以飞船的控制系统通常都是拜占庭容错的。</p></li><li><p>在一个Multiple Participant 的环境中，可能会存在一些Participant 故意传递一些错误的信息。因此，直接相信另一个节点传过来的信息是不准确的。例如在比特币交易系统中（或者其他区块链系统），它们的共识是由peer-to-peer产生的，而不需要中心节点达成共识，这种情况下出现拜占庭问题的概率会更大。</p></li></ul><p>Web Application的现实场景：在大多数情况下，服务端基本不需要考虑拜占庭将军问题，因为公司的机器通常都是部署在数据中心当中，很少受到辐射的干扰，而且有专业的运维同学负责监控，叛军出现的概率很小。</p><h3 id="Weak-Form-of-Lying"><a href="#Weak-Form-of-Lying" class="headerlink" title="Weak Form of Lying"></a>Weak Form of Lying</h3><p>即使在非拜占庭的系统中，也会存在一些bug 导致服务节点在集群中通信的时候传达错误的信息，比如：由于硬件问题导致的非法消息，软件的bug，以及错误的配置。一些典型的例子包括：</p><ul><li><p>网络包有时候因为硬件或者操作系统的出错而产生错误，TCP和UDP 能过滤掉大多数被污染的包，但也有少部分被污染的包能够<a href="https://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html">通过以太网CRC校验加TCP的checksum</a>。一个简单有效的方法就是在应用层加上校验来进一步防止corrupt package的出现。</p></li><li><p>DoS攻击，攻击方通过在请求中加入超大的string导致服务器内存发生OOM。</p></li><li><p>NTP (Network Time Protocol) 在同步的时候会与集群中大多数的节点进行通信，校验本地的误差级别，只要集群中有大多数节点能够工作，就能够将出错的 NTP Server检测出来。</p></li></ul><h2 id="System-Model-And-Reality"><a href="#System-Model-And-Reality" class="headerlink" title="System Model And Reality"></a>System Model And Reality</h2><p><strong>Safety and Liveness</strong></p><p>Safety is defined as nothing bad happening and liveness as something good eventually happens.</p><p><strong>System Models</strong></p><p><strong>Synchronize Model</strong></p><p>这个model假设 网络延迟、进程终止、时钟偏移总是有边界的，并且 RPC的调用方能够感知到以上这些延迟，基于正确的时钟做出决策。 <strong>Partial Synchronize Model</strong></p><p>半同步系统在大多数情况下都表现的和同步系统一致，但是偶尔会出现超过 bound 的网络延迟、进程终止和时钟偏移。</p><p>这种模型符合现实中大多数系统，大多数情况下，网络和进程都能够正常工作，但一旦发生了延迟，则延迟的上界可以是无限大的。 <strong>Asynchronized Model</strong></p><p>在这个模型中，系统不会对环境做出任何假设，unbounded network delay, process pause  随时都会发生。因此建立在异步模型上的分布式共识算法往往更加复杂和严格。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>分布式系统中常见的三种问题：</p><ul><li><p>Network Delay</p></li><li><p>Clock Drift (NTP server and local quartz may suffer problems)</p></li><li><p>Process Pauce (e.g Due to GC, the process has to pause for large amount of time)</p></li></ul><p><a href="https://ucare.cs.uchicago.edu/pdf/socc13-limplock.pdf">Limping Lock</a>：这篇文章中讲述了如何处理<strong>残废节点</strong>，一个节点没有完全宕机，但是性能急剧下降（Throughput 1GB -&gt; 1KB），这种节点往往比 dead node 更加难以处理。</p><p>分布式系统和超算系统的不同：超算系统假设系统中节点都是可靠的，如果一个节点宕机，则整个任务需要重新运行。分布式系统可以永远运行下去，如果在没有其他因素对服务造成干预的话。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>DDIA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>refactoring</title>
    <link href="/2022/11/03/refactoring/"/>
    <url>/2022/11/03/refactoring/</url>
    
    <content type="html"><![CDATA[<h1 id="Bad-Smells-in-code"><a href="#Bad-Smells-in-code" class="headerlink" title="Bad Smells in code"></a>Bad Smells in code</h1><h2 id="Long-Function"><a href="#Long-Function" class="headerlink" title="Long Function"></a>Long Function</h2><p>If you have a function with logs of parameters and temporary variables, they get in the way of extracting.</p><h3 id="Extract-Function"><a href="#Extract-Function" class="headerlink" title="Extract Function"></a>Extract Function</h3><p>Looking at a fragment of code and figuring out what it’s doing, then you should extract it into a function and name the function after “what”. Programmers should take practice naming the function that can make the function more self-documenting.</p><p><strong>Mechanism</strong></p><ul><li>Create a function and name it after the intent of the function (by what it does instead of how it does). If the code is as simple as a function call, it should be extracted if the new function name can reveal what the function is doing. If it can’t come with a more meaningful name, that’s sign of don’t extract.</li></ul><p>If the language supports nested functions, nest the extracted function inside the source function that will reduce the amount of out-of-scope variables to deal with.</p><h3 id="Replace-Temp-with-Query"><a href="#Replace-Temp-with-Query" class="headerlink" title="Replace Temp with Query"></a>Replace Temp with Query</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">const</span> basePrice = <span class="hljs-variable language_">this</span>.<span class="hljs-property">quantity</span> * <span class="hljs-variable language_">this</span>.<span class="hljs-property">itemPrice</span>;<br><span class="hljs-keyword">if</span> (basePrice &gt; <span class="hljs-number">1000</span>)<br>    <span class="hljs-keyword">return</span> basePrice * <span class="hljs-number">0.95</span><br><span class="hljs-keyword">else</span> <br>    <span class="hljs-keyword">return</span> basePrice * <span class="hljs-number">0.98</span><br></code></pre></td></tr></table></figure><blockquote><p>Could be refactoring to</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">get</span> <span class="hljs-title function_">basePrice</span>() &#123;<span class="hljs-variable language_">this</span>.<span class="hljs-property">quantity</span> * <span class="hljs-variable language_">this</span>.<span class="hljs-property">itemPrice</span>;&#125;<br>...<br><span class="hljs-keyword">if</span> (<span class="hljs-variable language_">this</span>.<span class="hljs-property">basePrice</span> &gt; <span class="hljs-number">1000</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">this</span>.<span class="hljs-property">basePrice</span> * <span class="hljs-number">0.95</span><br><span class="hljs-keyword">else</span> <br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">this</span>.<span class="hljs-property">basePrice</span> * <span class="hljs-number">0.98</span><br></code></pre></td></tr></table></figure><p>Using function instead of variables also allows us to avoid duplicating the calculation logic in similar functions. <strong>Whenever I see variables calculated in the same way in different places, I look to turn them into a single function</strong>.</p><p><strong>Mechanism</strong></p><ul><li><p>Check the variables are determined entirely before they ‘re used and this code that calculates them does not yield a different value whenever it is used</p></li><li><p>If the variable isn’t read-only and can be made read-only, do so</p></li></ul><h3 id="Replace-function-with-command"><a href="#Replace-function-with-command" class="headerlink" title="Replace function with command"></a>Replace function with command</h3><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NGM4YTQ3MWUwNDQzNDQ0MDNjMzJkZjY1NmMyYWY2ZDdfWm5SazZpdE5JYVg1ZWFPaTdsVVJUaEx2c3o5ZWhyaXlfVG9rZW46Ym94Y25Zd0VCc3BFOHpDdmtDTWpHYWRpb0xnXzE2Njc0NDE2NDI6MTY2NzQ0NTI0Ml9WNA" alt="img"></p><p>Functions are one of the fundamental buildings. But there are times when it’s useful to encapsulate a function into its own object (command object). (Like runner in <code>compliance.kyc`). A commander offers more flexibility than normal function. For example, the commander object could provide </code>undo&#96; operation. It can provide methods to build parameters thus supporting a richer lifecycle.</p><blockquote><p>Commander is tool that trades flexibility off complexibility.</p></blockquote><p>Given the choice between function and commander, I will pick function 95% off the time. I only use a command when I specifically need a facility that simpler approaches can’t provide.</p><p><strong>Mechanism</strong></p><ul><li><p>Create an empty class for the function. Name it based on the function</p></li><li><p>Move function to the empty class and keep the original function as a forwarding function until at least the end of the refactoring. Follow any convention the language has for naming commands such as “execute”, or “call”</p></li></ul><h2 id="Long-Parameter-List"><a href="#Long-Parameter-List" class="headerlink" title="Long Parameter List"></a>Long Parameter List</h2><blockquote><p>If one parameter can be computed by another parameter, then <strong>replace that parameter with query</strong></p></blockquote><h3 id="Replace-Parameter-with-Query"><a href="#Replace-Parameter-with-Query" class="headerlink" title="Replace Parameter with Query"></a>Replace Parameter with Query</h3><p>The function parameters should avoid any duplication as the statement in the code.</p><blockquote><p>Rather than pulling lots of data out of an existing data structure, you can use <strong>Preserve whole Object</strong> to pass the original data structure instead.</p></blockquote><h3 id="Preserve-whole-Object"><a href="#Preserve-whole-Object" class="headerlink" title="Preserve whole Object"></a>Preserve whole Object</h3><p>Pulling several field values from object to do some logic them alone is a smell (Feature Envy) and usually a signal that this logic should be moved into the whole itself. </p><p>One case that many people miss is when an object calls another object with several of its own data values. If I see this, I can replace the whole value with a self-reference (&#96;&#96;this&#96; in Java).</p><blockquote><p>If several parameters always fit together, combine them with <strong>introduce parameter object</strong></p></blockquote><h3 id="Introduce-Parameter-Object"><a href="#Introduce-Parameter-Object" class="headerlink" title="Introduce Parameter Object"></a>Introduce Parameter Object</h3><p>If the groups of data often travel together, appearing in function after function, such a group is a data clump, and I like to replace it with a single data structure.</p><p>It makes explicit the relationship between data items and reduces the size of parameter of a function. It helps consistency since all functions that use the structure will use the same names to get at its elements.</p><blockquote><p>If a parameter is used as a flag to dispatch different behavior, use <strong>Remove Flag Argument</strong></p></blockquote><h3 id="Remove-Flag-Argument"><a href="#Remove-Flag-Argument" class="headerlink" title="Remove Flag Argument"></a>Remove Flag Argument</h3><p>A flag argument is a function argument that the caller uses to indicate which logic the called function should execute. </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">boookConcert</span>(<span class="hljs-params">aCustomer, isPremium</span>)&#123;<br>    <span class="hljs-keyword">if</span>(isPremium)&#123;<br>        ...<br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;<br>        ...<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>The flag argument complicates the process of understanding what function calls are available and how to call them. My first route into an API is usually the list of available functions and flag arguments hide the difference in the functions. Boolean flags are even worse because they don’t convey their meaning to the reader. </p><p>To remove the flag argument, we need to create an explicit function for each value of the parameter. If the main function has a clear dispatch conditional, use Decompose Conditional to <strong>create the explicit functions</strong>. Otherwise, create <strong>wrapping functions</strong>. And then for each caller that uses a literal value for the parameter, replace it with a call to the explicit function.</p><p>Examples</p><p><strong>Caller</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript">aShipment.<span class="hljs-property">deliveryDate</span> = <span class="hljs-title function_">deliveryDate</span>(anOrder, <span class="hljs-literal">true</span>);<br>aShipment.<span class="hljs-property">deliveryDate</span> = <span class="hljs-title function_">deliveryDate</span>(anOrder, <span class="hljs-literal">false</span>);<br></code></pre></td></tr></table></figure><p><strong>Function</strong></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">deliveryDate</span>(<span class="hljs-params">anOrder, isRush</span>) &#123;<br>  <span class="hljs-keyword">if</span> (isRush) &#123;<br>    <span class="hljs-keyword">let</span> deliveryTime;<br>    <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;MA&quot;</span>, <span class="hljs-string">&quot;CT&quot;</span>]     .<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;NY&quot;</span>, <span class="hljs-string">&quot;NH&quot;</span>].<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">else</span> deliveryTime = <span class="hljs-number">3</span>;<br>    <span class="hljs-keyword">return</span> anOrder.<span class="hljs-property">placedOn</span>.<span class="hljs-title function_">plusDays</span>(<span class="hljs-number">1</span> + deliveryTime);<br>  &#125;<br>  <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-keyword">let</span> deliveryTime;<br>    <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;MA&quot;</span>, <span class="hljs-string">&quot;CT&quot;</span>, <span class="hljs-string">&quot;NY&quot;</span>].<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;ME&quot;</span>, <span class="hljs-string">&quot;NH&quot;</span>] .<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">3</span>;<br>    <span class="hljs-keyword">else</span> deliveryTime = <span class="hljs-number">4</span>;<br>    <span class="hljs-keyword">return</span> anOrder.<span class="hljs-property">placedOn</span>.<span class="hljs-title function_">plusDays</span>(<span class="hljs-number">2</span> + deliveryTime);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>Since the branch condition is not complicated, we can explicitly decompose the two branches: </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">rushDeliveryDate</span>(<span class="hljs-params">anOrder</span>) &#123;<br>    <span class="hljs-keyword">let</span> deliveryTime;<br>    <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;MA&quot;</span>, <span class="hljs-string">&quot;CT&quot;</span>]     .<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;NY&quot;</span>, <span class="hljs-string">&quot;NH&quot;</span>].<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">else</span> deliveryTime = <span class="hljs-number">3</span>;<br>    <span class="hljs-keyword">return</span> anOrder.<span class="hljs-property">placedOn</span>.<span class="hljs-title function_">plusDays</span>(<span class="hljs-number">1</span> + deliveryTime);<br>&#125;<br></code></pre></td></tr></table></figure><p>And</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">rugularDeliveryDate</span>(<span class="hljs-params">anOrder</span>) &#123;<br>    <span class="hljs-keyword">let</span> deliveryTime;<br>    <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;MA&quot;</span>, <span class="hljs-string">&quot;CT&quot;</span>, <span class="hljs-string">&quot;NY&quot;</span>].<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ([<span class="hljs-string">&quot;ME&quot;</span>, <span class="hljs-string">&quot;NH&quot;</span>] .<span class="hljs-title function_">includes</span>(anOrder.<span class="hljs-property">deliveryState</span>)) deliveryTime = <span class="hljs-number">3</span>;<br>    <span class="hljs-keyword">else</span> deliveryTime = <span class="hljs-number">4</span>;<br>    <span class="hljs-keyword">return</span> anOrder.<span class="hljs-property">placedOn</span>.<span class="hljs-title function_">plusDays</span>(<span class="hljs-number">2</span> + deliveryTime);<br>&#125;<br></code></pre></td></tr></table></figure><p>So the previous caller would be like below and we successfully remove the flag argument</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript">aShipment.<span class="hljs-property">deliveryDate</span> = <span class="hljs-title function_">rushDeliveryDate</span>(anOrder);<br>aShipment.<span class="hljs-property">deliveryDate</span> = <span class="hljs-title function_">rugularDeliveryDate</span>(anOrder);<br></code></pre></td></tr></table></figure><p>When the condition is too complex to decouple, like</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">deliveryDate</span>(<span class="hljs-params">anOrder, isRush</span>) &#123;<br>  <span class="hljs-keyword">let</span> result;<br>  <span class="hljs-keyword">let</span> deliveryTime;<br>  <span class="hljs-keyword">if</span> (anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;MA&quot;</span> || anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;CT&quot;</span>)<br>    deliveryTime = isRush? <span class="hljs-number">1</span> : <span class="hljs-number">2</span>;<br>  <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;NY&quot;</span> || anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;NH&quot;</span>) &#123;<br>    deliveryTime = <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">if</span> (anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;NH&quot;</span> &amp;&amp; !isRush)<br>      deliveryTime = <span class="hljs-number">3</span>;<br>  &#125;<br>  <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (isRush)<br>    deliveryTime = <span class="hljs-number">3</span>;<br>  <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (anOrder.<span class="hljs-property">deliveryState</span> === <span class="hljs-string">&quot;ME&quot;</span>)<br>    deliveryTime = <span class="hljs-number">3</span>;<br>  <span class="hljs-keyword">else</span><br>    deliveryTime = <span class="hljs-number">4</span>;<br>  result = anOrder.<span class="hljs-property">placedOn</span>.<span class="hljs-title function_">plusDays</span>(<span class="hljs-number">2</span> + deliveryTime);<br>  <span class="hljs-keyword">if</span> (isRush) result = result.<span class="hljs-title function_">minusDays</span>(<span class="hljs-number">1</span>);<br>  <span class="hljs-keyword">return</span> result;<br>&#125;<br></code></pre></td></tr></table></figure><p>We can layer functions over the delivery-date:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs JavaScript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">rushDeliveryDate</span>   (anOrder) &#123;<span class="hljs-keyword">return</span> <span class="hljs-title function_">deliveryDate</span>(anOrder, <span class="hljs-literal">true</span>);&#125;<br><span class="hljs-keyword">function</span> <span class="hljs-title function_">regularDeliveryDate</span>(<span class="hljs-params">anOrder</span>) &#123;<span class="hljs-keyword">return</span> <span class="hljs-title function_">deliveryDate</span>(anOrder, <span class="hljs-literal">false</span>);&#125;<br></code></pre></td></tr></table></figure><blockquote><p>If there are several functions sharing the same parameter list, then you can use Combine function into class to capture those common values as fields.</p></blockquote><h3 id="Combine-functions-into-Class"><a href="#Combine-functions-into-Class" class="headerlink" title="Combine functions into Class"></a>Combine functions into Class</h3><h2 id="Feature-Envy"><a href="#Feature-Envy" class="headerlink" title="Feature Envy"></a>Feature Envy</h2><p>When we modularis a program, we try to separate the code into zones that maximize interaction inside a zone and minimize interaction between the zones. A classic case of Feature Envy is when a function in one module spends more time communicating with functions or data inside another module that it does within its own module. For example, a function invokes half-a-dozen getter methods on another object to calculate some value. It means that the function needs some data, so we can use Move Function to get it there.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Refactoring</tag>
      
      <tag>Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>interview-guide</title>
    <link href="/2022/11/03/interview-guide/"/>
    <url>/2022/11/03/interview-guide/</url>
    
    <content type="html"><![CDATA[<h1 id="程序员面试"><a href="#程序员面试" class="headerlink" title="程序员面试"></a>程序员面试</h1><h3 id="了解面试流程"><a href="#了解面试流程" class="headerlink" title="了解面试流程"></a>了解面试流程</h3><h4 id="第一步：通过初筛，获得面试资格"><a href="#第一步：通过初筛，获得面试资格" class="headerlink" title="第一步：通过初筛，获得面试资格"></a>第一步：通过初筛，获得面试资格</h4><p>这一阶段，最重要的就是<strong>写好简历</strong>。大多数程序员简历被挂不是因为他们做的项目不行或者能力不行，而是因为简历中对于曾经做过的Project包装不到位。关于如何写好简历，可以参考YangShun大佬的： <a href="https://www.techinterviewhandbook.org/resume/">step-by-step guide here on software engineering resume preparation</a>。</p><p>在面试前通常会有一些前置的流程，包括 OA，QuiZ等等，这些通常是在HC比较紧张的时候，用于快速筛选候选人的一种方式吧。我得出该结论的推理是：我参加过一次秋招和一次春招：</p><ul><li>在秋招的过程中，海投了十几家大大小小的互联网公司，大多数时候都是HR直接约面的，一些OA我放着没做后来HR也主动打电话过来约面了。（此时在中国）</li><li>在春招的过程中，海投了5-6家海外的互联网公司，除了TikTok是内推直接约面的之外，其他的公司基本都有OA。（此时在新加坡）。</li></ul><p>虽然可能国内国外的互联网招聘流程不太一样，但我推测影响这一流程的因素包括三个方面：</p><ol><li>候选人是否是通过内推进入的招聘流程（能够得到职级越高的员工内推，后续的流程也更加顺利）；</li><li>部门的HC是否充足，秋招的HC 应该会远远大于春招。</li><li>候选人的能力，推测该因素的优先级是远远小于前两个。</li></ol><p>基于以上，对于OA 和 Quiz 的频率定义为：<strong>Occasionally</strong>.</p><h4 id="第二步：进入电面环节"><a href="#第二步：进入电面环节" class="headerlink" title="第二步：进入电面环节"></a>第二步：进入电面环节</h4><ul><li><p>手撕代码</p><p>不要依赖编译器来调试自己的代码，而需要经过详细的推敲和耐心的编码，在短时间内写出正确、高效的代码。</p></li></ul><p>Refer：<a href="https://www.techinterviewhandbook.org/coding-interview-cheatsheet/">https://www.techinterviewhandbook.org/coding-interview-cheatsheet/</a></p><h4 id="第三步骤：进入线下面环节"><a href="#第三步骤：进入线下面环节" class="headerlink" title="第三步骤：进入线下面环节"></a>第三步骤：进入线下面环节</h4><ul><li>到了这一步已经快成功了，通常公司会支付海外候选人的机票和住宿。</li><li>这一步常见的面试问题包括：Coding、System Design 和 Behavior，而且因为是线下可能需要花费较长的时间，并且需要自带电脑，在Whiteboard 上做设计，并且在短时间内编码。<ul><li>准备好开发环境。</li></ul></li><li>选择一门合适的编程语言：建议选择常用的，对于数据结构有较完整封装的语言，包括：Python、Java、C++。</li></ul><h3 id="面试前的准备"><a href="#面试前的准备" class="headerlink" title="面试前的准备"></a>面试前的准备</h3><ul><li><p>Code Interview: 最重要的一环，抱紧力扣大腿，充分了解对于所选编程语言的应用。比较建议的时间是 2-3个月准备编程应试，每天2-3小时。对于我而言，每天保持力扣的刷题更新，面试前针对性的刷目标公司的题目即可。</p></li><li><p>Mock Interview (With Google &amp; FaceBook Engineers): <a href="https://iio.sh/r/DMCa">interviewing.io</a></p></li><li><p>System Design</p><ul><li><a href="https://bytebytego.com/?fpr=techinterviewhandbook">ByteByteGo</a></li><li><a href="https://designgurus.org/link/kJSIoU?url=https://designgurus.org/course?courseid=grokking-the-system-design-interview">Grokking the System Design Interview” by Design Gurus</a></li></ul></li><li><p>Behavior Interview</p><p>这一环节主要是HR为了了解候选人在上一家公司的经历，以及候选人对于一些工作上遇到的问题的解决方法（冲突处理），或者职业生涯规划等。</p><ul><li>STAR Format:<ul><li>Situation: 关于做某个任务的BackGround。</li><li>Task：需要解决什么具体的问题，关注这个Task 的 影响面、挑战、产出。</li><li>Action：做了什么事情去实现这个Task，并且描述在解决Task中遇到的一些选择。</li><li>Result：描述Action 的产出，以及从这个Action中学习到了什么。</li></ul></li><li>资源：<ul><li><a href="https://www.techinterviewhandbook.org/behavioral-interview-questions/">Top-30 Behavior Question</a></li><li><a href="https://www.techinterviewhandbook.org/behavioral-interview/">BI Guide</a></li></ul></li></ul></li><li><p>学习如何Negotiate：</p><ul><li><a href="https://www.techinterviewhandbook.org/negotiation/">negotiation strategies</a> </li><li><a href="https://www.techinterviewhandbook.org/understanding-compensation/">software engineer compensation</a>.</li></ul></li></ul><h3 id="如何写好简历"><a href="#如何写好简历" class="headerlink" title="如何写好简历"></a>如何写好简历</h3><p>简历不是一次性的，它就像一个工程，有了良好的架构之后，扩展性和可用性都能持续提升，每一次完善简历、投递简历的过程就是一次 CI &#x2F; CD。</p><h4 id="建立-ATS-Applicant-Tracking-System-友好的简历模板"><a href="#建立-ATS-Applicant-Tracking-System-友好的简历模板" class="headerlink" title="建立 ATS (Applicant Tracking System) 友好的简历模板"></a>建立 ATS (Applicant Tracking System) 友好的简历模板</h4><blockquote><p>大多数公司都有对简历的自动解析系统，并且会根据解析出来的选项自动过滤掉一些候选人的简历。</p></blockquote><p>简历中内容的顺序</p><table><thead><tr><th align="center">Section</th><th>Heading Name</th></tr></thead><tbody><tr><td align="center">Professional Summary</td><td>Use Resume Headline as section title, e.g: Senior Software Engineer at Google with 5 years of experience leading teams.</td></tr><tr><td align="center">Contact Information</td><td>Contact infos</td></tr><tr><td align="center">Skills</td><td>Programming Language, Frameworks</td></tr><tr><td align="center">Experience</td><td>Work Experience</td></tr><tr><td align="center"><strong>Education</strong></td><td>Less than 3 years, should put school info <strong>First</strong></td></tr><tr><td align="center">Projects</td><td>Projects</td></tr></tbody></table><h4 id="写好简历的Summary"><a href="#写好简历的Summary" class="headerlink" title="写好简历的Summary"></a>写好简历的Summary</h4><blockquote><p>Summary 就像一篇论文的 Abstract，目标是抓住Manager的眼球，主要是围绕自己的 ”卖点“ 和 JD的匹配度 来写。</p></blockquote><ul><li><p>写大纲：描述自己对于JD中要求具体满足&#x2F;契合的点。</p></li><li><p>写标题：(Example)</p><h5 id="Software-Engineering-Lead"><a href="#Software-Engineering-Lead" class="headerlink" title="Software Engineering Lead"></a><a href="https://www.techinterviewhandbook.org/resume/#software-engineering-lead">Software Engineering Lead</a></h5><p>Software Engineer with X years of experience in back end, scaling complex distributed systems, and various cloud platforms. Led over 5 engineering teams with an average size of 6 members across two companies and mentored over 20 junior members.</p><h5 id="Senior-at-University-X"><a href="#Senior-at-University-X" class="headerlink" title="Senior at University X"></a><a href="https://www.techinterviewhandbook.org/resume/#senior-at-university-x">Senior at University X</a></h5><p>Senior Year student at University X with a focus on Artificial Intelligence and Machine Learning (ML). Interned at X companies and worked on full stack development and ML engineering roles.</p></li></ul><h4 id="写清楚简历的Contact-Infomation"><a href="#写清楚简历的Contact-Infomation" class="headerlink" title="写清楚简历的Contact Infomation"></a>写清楚简历的Contact Infomation</h4><p><strong>Must Have</strong></p><ul><li>Name</li><li>Personal Phone Number</li><li>Location</li><li>Email Address</li><li>LinkedIN</li></ul><p><strong>Recommend</strong></p><ul><li>GitHub Page</li><li>Personal Website Page</li><li>Competitive Coding Profile</li></ul><h4 id="关于工作经历的描述"><a href="#关于工作经历的描述" class="headerlink" title="关于工作经历的描述"></a>关于工作经历的描述</h4><p>按照熟悉程度和 倒序时间顺序来写就职的公司，需要包括以下信息：</p><ul><li>The company, location, title, duration worked following this structure</li></ul><blockquote><p>[Company or Organization], [Location] | [Job Title] | [Start and end dates formatted as MM&#x2F;YYYY]</p></blockquote><p>Example</p><blockquote><p>Facebook, Singapore | Front End Engineering Lead | 08&#x2F;2018 - Present</p></blockquote><p>列出在公司中最主要的贡献</p><ul><li><p>Scope of job and skills required</p></li><li><p>Accomplishments listed following this structure</p><ul><li><blockquote><p>[Accomplishment summary] : [Action] that resulted in [quantifiable outcome]</p></blockquote></li></ul></li></ul><h4 id="润色简历"><a href="#润色简历" class="headerlink" title="润色简历"></a>润色简历</h4><ul><li>Less is More! 内容尽量局限于一页纸，高亮一些重点的产出比盲目列举一些平庸的产出更重要。</li><li>从 JD 中摘取一些关键词应用到简历中，可以将关键词放在 Summary 和 教育经历中。进一步地，需要根据JD 分析对于候选人能力的优先级顺序，优先级高的关键词应该更多地出现。<ul><li>一些TIPs：<ul><li>多找几份相关岗位的JD 从中提取共同的信息；</li><li>可以Paste到 MarkDown中，通过关键词搜索来找到最常用的关键词；</li></ul></li></ul></li></ul><h4 id="Last-But-Not-Least"><a href="#Last-But-Not-Least" class="headerlink" title="Last But Not Least"></a>Last But Not Least</h4><ul><li>认真对待申请表，有些公司需要填写完申请表格之后，候选人的简历才能被 HR 看到；</li><li>不要申请一家公司的多个职位。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>stream-processing</title>
    <link href="/2022/11/03/stream-processing/"/>
    <url>/2022/11/03/stream-processing/</url>
    
    <content type="html"><![CDATA[<h1 id="Stream-Processing"><a href="#Stream-Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h1><p><a href="https://lo845xqmx7.feishu.cn/docx/doxcnOr5SPHoYCqX6uQZdPTxyle">Batch Processing </a>: The Principle behind MapReduce</p><p>流处理与批处理任务不同点：</p><ul><li><p>批处理任务通常都是离线任务，它要求输入是有限的，比如使用MapReduce任务进行排序，则要求文件中已经完整保存了所有需要被排序的 item。而流处理的任务通常都是无限的，比如：用户的行为记录，消费记录，这些数据每时每刻都在产生。</p></li><li><p>如果使用批处理处理流式数据，则需要将数据按照某个维度进行划分（例如：时间），例如每天或者每小时对数据进行一次划分。这样做会使数据的反馈延迟一定的时间，不适用于一些实时的任务。如果使用流式的方案，那么每当产生一条数据记录的时候就会消费这条数据记录。</p></li><li><p>批处理任务的<strong>输入输出</strong>通常都是分布式文件系统上的文件，而流处理任务的输入通常为一条数据记录，或被称为一个事件 (<strong>event</strong>)。</p></li></ul><p>在计算机系统中，<strong>流数据</strong>表示源源不断产生的数据，比如：Unix的 STDIN 和 STDOUT, Jave FileInputStream API, TCP Connection 等。</p><p>流式数据通常有一个生产方和一至多个消费方，将生产者或消费者连接起来的桥梁可以是一个文件或者一个Database，消费者通过<strong>轮训</strong>文件或者DB来检测是否有新的event产生。但对于大部分系统来说，轮训必然增加系统额外的开销，所以如果消费者在event产生的时候能够被通知 (<strong>notify</strong>)到就好了…</p><p>数据库系统通常不会提供notify的机制，传统的RDB中提供了一个trigger的方案用于在某个表被更新的 时候执行某个操作，但是功能十分有限而且不好维护。</p><h2 id="Message-System"><a href="#Message-System" class="headerlink" title="Message System"></a>Message System</h2><p>一个常用来 notify 消费者工具就是消息系统 (<strong>消息队列</strong>) 。Unix 管道和TCP连接都是使用直接的通信管道连接双端的，并且只允许在两个进程之间进行通信。消息队列扩展了其功能，使得多个生产者可以向一个topic发送消息，以及多个消费者可以从消息队列中消费消息，形成一种<strong>发布&#x2F;订阅</strong>的模式。</p><p>为了区别不同消息系统的能力，通常会考虑以下两个维度：</p><ul><li>如果生产者的生产速率大于消费者的消费速率怎么办？</li></ul><p>为了应对生产消费速率不匹配的问题，通常可以使用三种方案：1. 限制生产者的生产速率，2. 对消息进行缓冲（Buffer），3. 直接丢弃消息。TCP和Unix管道采用Buffer的方式限制速率，在网络系统中，这个过程通常被称为流量控制。</p><p>对于采用buffer 限流的系统，一个重要的问题就是如果buffer容量超出了内存限制，Queue是否会Crash 或者 Queue中的数据是否会被持久化的磁盘</p><ul><li>如果消息队列中途Crash了，消息是否会丢失。</li></ul><p>在数据系统中为了保证数据不丢，需要对数据创建副本或者本地持久化，这样会对系统的性能造成一定的影响。批处理为开发者提供了一个良好的可靠性保证，因为一个批处理任务是原子的，要么全部成功要么全部失败，不会存在Partial Write。</p><h4 id="Direct-Messaging"><a href="#Direct-Messaging" class="headerlink" title="Direct Messaging"></a>Direct Messaging</h4><p>消息系统通常都会在生产者和消费者之间建立一个直接的通信链路，不会经过中间节点：</p><ul><li><p>UDP多播在金融系统中广泛应用，例如：要求低延迟的股票推送。尽管UDP是一种不可靠的协议，但是应用层可以通过记录哪些包丢失了，进而重传这些包。Brokerless的消息队列，向ZeroMQ, nanomsg等采用了类似的方案，它们使用了TCP + IP 多播的方式进行通信。</p></li><li><p>如果消费者在网络上暴露了一个服务接口，那么生产者可以通过HTTP或者RPC 请求的方式将消息推送给消费者。</p></li></ul><p>仅仅依靠传输层的可靠传输是不够的，在消费者Crash的情况下，消息无法被接受；生产者Crash的情况下，丢失的消息无法进行重传。一个常用的方案是通过broker节点来发送和存储消息。</p><h2 id="Message-Broker"><a href="#Message-Broker" class="headerlink" title="Message Broker"></a>Message Broker</h2><blockquote><p>Broker 才是消息系统的核心，也是狭义的消息队列。</p></blockquote><p>消息队列类似一个特殊的DB系统，它本身作为一个服务的提供方，生产者和消费者作为服务的Client 连接到消息队列的服务上，消息可以被无限持久化在Broker上或者有限存储于内存中。</p><p>和传统DB相比，MQ不同之处在于：</p><ul><li><p>对数据的<strong>持久化</strong>：大多数MQ对于数据的存储都是临时的，一旦消息被消费，会被立即或者延期删除；而DB对于数据记录的保存是永久的，除非用户显示对数据进行了删除。</p></li><li><p>因为MQ对于数据的临时保存机制，所以在设计MQ的时候，通常都会认为其工作集较小，如果broker需要存储大量的上游消息，那么就需要将消息持久化到磁盘上，因为磁盘IO的限制，所以整个系统的吞吐将受到影响。</p></li><li><p>DB系统支持二级索引等更加丰富的查询方式，而MQ只提供了一些topic 匹配的查询方式。</p></li></ul><h3 id="Multiple-Consumer"><a href="#Multiple-Consumer" class="headerlink" title="Multiple Consumer"></a>Multiple Consumer</h3><p>当系统中存在多个Consumer消费同一个topic的情况时，需要考虑以下两种消费模式：</p><ul><li><p><strong>Load Balance</strong>：Broker负责均匀地将消息传播给下游的Consumer</p></li><li><p><strong>Fan Out</strong>：broker将消息广播给所有的Consumer，每一个Consumer独立地对消息进行消费。</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjYxNGNjMGRlMzUzMzkwYjM5Nzk1OGU1NmVlMmJhYjRfaEluZXhHUTloQUg3MzNScmpkWk1abTNrWWhnM0VkaTNfVG9rZW46Ym94Y25Ec0Q5Skt5SVhNcE94c2RCa2huRzVkXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><p>在真实的消息队列中，通常一条消息会广播给所有的消费者组，每一个消费者组内只会有一个消费者实例接收到消息。</p><h3 id="ACK"><a href="#ACK" class="headerlink" title="ACK"></a>ACK</h3><p>ACK机制是为了防止Consumer处理消息失败而设计的，消息的持久化保存是由Broker内部的持久化机制保证。如果Broker给Client发送消息之后，Client 因为Crash 没有给Broker返回ACK，此时Broker就会选择另一个实例对消息重新投递，以此来保证At Least Once语义；这里会出现ACK在网络链路上丢失的情况，所以消息有可能被重复投递，需要业务下游对消息做幂等处理。</p><p>在Load Balance的情况下，会出现消息乱序，如下图所示，Broker 给 Consumer 2 发送的 m3消息丢失后，重传给 Consumer 1，但是在此过程之间 m1 接收了消息 m4进而导致消费顺序变为 (m4, m3, m5)。乱序消费在event 独立的时候不会对系统产生影响，但是如果消息之间存在 <strong>casual dependency</strong>，就会产生问题。</p><p>为了保证顺序消费，一种方法是 只使用一个Consumer 实例，并且维护一个队列，按顺序处理消息。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjcwYTUxYzhhMjY3YWE2NmI0YTg0ZTg0YzYzNTUyYmRfRTV4TFdRbDY4alZnNHpMMTdkSDBudEhvOEJSeFAyTVpfVG9rZW46Ym94Y25NYlZmSXVqN1FwNk1FUDMxWGc5Y2ZiXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><h3 id="Partition-Logs"><a href="#Partition-Logs" class="headerlink" title="Partition Logs"></a>Partition Logs</h3><p>Log 是一种Append Only的数据结构，存储系统中的LSM Tree 和 WAL 都是基于Log实现的方案。在消息系统中，生产者可以将消息以Append ONLY的方式顺序写入Log，消费者可以从Log中顺序的读写。与 Unix&#x2F;Linux 系统中，<code>tail -f</code> 的效果类似。</p><p>Apache Kafka, Amazon Kinesis Streams 都是基于Log的方案实现，这些消息系统在满足消息持久化的同时仍然能够保持百万级吞吐背后的原因是因为 Log Partition，并且Replication 也保证了系统 Fault Tolerance 的能力。值得注意的是，消息在Partition内有序，Partition之间无序；Partition内的消费进度由Consumer保存，如: Consumer Group 中维护的 <code>offset for xxx</code>。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NzMxNmU3MmFjODkyN2Y1ZDRlMTYyZmQ0ODBhNzU0NTRfTzRVcGRES1RBT2ZKUzJrSEZybkN2WVpHMFBONzlGTm1fVG9rZW46Ym94Y25ub2pQNUtqeldSVWxDMUdrd1V0SXpmXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><p>Log-Based 消息系统不需要使用Fan-Out的模式进行通信，Broker通过将某个Partition 分配给消费者实例的方式实现Load Balancing，每一个消费者实例只需要去消费被分配到的Partition中的消息，而无需关心其他的Partition。这种做法存在两个弊端：</p><ul><li><p>消费者实例数的上限为Partition的数量，因为一个Partition只会被分给指定的Consumer。</p></li><li><p>如果一个Partition内出现了Slow Message，这个Partition有可能出现消息堆积的问题。</p></li></ul><p>关于MQ选型：</p><ul><li><p>不需要消息有序 &amp; 单条消息消费时间很长： 选择JMS&#x2F;AMQP的方式。</p></li><li><p>消息有序 &amp; 单条消息消费时间短：选择 Log-Based &amp; Partition的方式。有序性保证：将消息往同一个Partition中投递。</p></li></ul><h3 id="Consumer-Offset"><a href="#Consumer-Offset" class="headerlink" title="Consumer Offset"></a>Consumer Offset</h3><p>Consumer Offset 用来标记消费者的消费进度，Broker无需关心每条消息的ack，只需要定期check消费者的消费进度。这里的 Consumer offset 和 存储系统中的 Log Sequence Number 类似，用于Follower 与Leader之间进行State Replication，在消息队列中表现为：Broker &#x3D; Leader, Consumer &#x3D; Follower，Follower 需要定期获取Leader的日志来同步自身与Leader的状态。</p><p>如果消费某个Partition 的Consumer A Crash了，则系统需要将这个Partition分配给其他的Consumer B，并且从最后一个记录的offset开始消费。此时如果Consumer A消费了某个区间的消息，但是Broker还没来得及记录；Consumer B就会重复消费该区间内的消息。</p><h3 id="Disk-Space-Usage"><a href="#Disk-Space-Usage" class="headerlink" title="Disk Space Usage"></a>Disk Space Usage</h3><p>如果不对Log进行回收，任其无限增长的话，最终将会占满所有的存储空间（内存 &#x2F; 外存），所以消息系统需要定期对Log进行回收。从存储的视角看，MQ中的Log是按照Segment进行组织的，系统也会按照Segment的维度对 Log进行回收和清理。</p><p>在磁盘上，系统会维护一个环形的Buffer用于存储历史 Input Log，当Buffer达到full size的时候，将会对消息进行清理，通常这个环形的buffer能够保存1-2周的时间。</p><h3 id="When-Consumer-cannot-keep-up-with-Producer-Message-Accumulation"><a href="#When-Consumer-cannot-keep-up-with-Producer-Message-Accumulation" class="headerlink" title="When Consumer cannot keep up with Producer - Message Accumulation"></a>When Consumer cannot keep up with Producer - Message Accumulation</h3><p>之前讨论过如果消费速率低于生产速率的三种处理方式：<strong>Drop</strong>, <strong>Buffer</strong>, <strong>BackPressure。</strong>Log-Based的消息系统通常采用Buffer的方式处理此问题。如果Consumer 消费速率低于生产速率，那么消息的消费位点会离生产位点越来越远，如果没有人工干预，最终会导致该消费者无法消费到消息。因为buffer无法无限增长，达到 fullsize后，系统便会对其进行清理。因此，需要对消费进度进行监控，一旦发现消息堆积，则需要人工介入排查问题。磁盘上buffer的容量足够大，因此为RD提供了充足的时间去排查问题。</p><p>与传统MQ相比，Log Based MQ 在消息被消费后不会删除消，所以消费可以被视为一个<strong>Read ONLY</strong>的操作。因此可以通过Consumer获取任意时间的Offset对MQ的消息进行<strong>重放，</strong>这与Batch Processing 的处理过程非常类似，下游与上游的数据不耦合，所以中间的处理过程可以无限执行。</p><h2 id="Database-and-Streams"><a href="#Database-and-Streams" class="headerlink" title="Database and Streams"></a>Database and Streams</h2><h3 id="CDC-Change-Data-Capture"><a href="#CDC-Change-Data-Capture" class="headerlink" title="CDC: Change Data Capture"></a>CDC: Change Data Capture</h3><p>一个大型应用底层通常需要多个数据系统来支持上层的功能，比如：OLTP数据库用于支持事务型查询，Cache用于加速访问率较高的资源，Search Index用于支持搜索，OLAP数据库用于支持分析型查询。不同数据系统之间通常使用ETL（Batch Process）进行同步。</p><p>如果全量同步数据较慢的话，通常会使用 <strong>dual write (双写)，</strong>由应用层显示更新两个不同的数据源。但是双写存在一些数据不一致问题：</p><ul><li><p>两个Client 同时更新DB 和 ES的同一个Key，但是更新的顺序为 DB: (A -&gt; B), ES: (B -&gt; A) 这样，DB和ES的数据将会永久不一致。</p></li><li><p>Partial Write：如果更新ES成功，但是更新DB失败，两边数据也会永久不一致。如果需要保证强一致，则需要使用 2PC，并且承受 2PC带来的巨大性能损失。</p></li></ul><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=NjQ1ZWNhOTkzYWEwNzA5MTVhNmEwMTBjMGUxNzkzOThfcm1oWlBKYlJESVRmRmZqUWJWbHRidE5INm9zclNFSFBfVG9rZW46Ym94Y240c1RaZ0ZvcEVDaFByMGhpQ2FFOGdkXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><p>从图11-4来看，DB和ES是两个不同的集群，集群由多个服务节点组成，每个集群只存在一个Leader，那么上述系统将表现为一个MultiLeader的系统，为了保持数据一致，需要引入额外的 CR (Conflict Resolve) 层。</p><p>如果能够将上述系统架构成一个 Single Leader的系统，Leader产生Log，Follower消费Log与Leader进行状态同步，那么系统将变得简单许多。</p><p>以MySQL为例，可以通过Binlog监听数据库的变化，并且将数据库的变化以相同的方式应用于其他的数据源。如果能够保证不同的数据源按照相同的顺序执行数据更新的操作，即可保证不同数据源上数据的一致性。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=YzgwZTI2ZjE5ZjU5MWRjYTg3NjhiYWI0ZTRhMWQ0YTdfUW5pUXV5TVZ1QWp2WEZWYmE1c290V1RzU1JJNmU3QXZfVG9rZW46Ym94Y254aUJNUWp5UXRwOGE1NjdiNEVPVHlnXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><p>另一种方式是使用数据库触发器来实现对数据变化的监听，但是触发器的方式存在较大的性能瓶颈而且支持的功能也相对有限，所以大部分场景下，还是会通过监听 Binlog的方式进行<strong>数据同步</strong>。</p><p>这种方案被广泛使用在：<strong>LinkedIn Databus, Facebook Wormhole, Yahoo! Sherpa</strong></p><p>注意：BinLog只是为 MySQL 实现CDC (Capture Data Change) 的一种形式，Maxwell 和 Debezium 在其上开发了Binlog与Stream结合的机制；在其他数据库中表现类似，但是有不同的名称，<a href="https://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/">Bottled Water</a> 给PostgreSQL 实现的一个用于解析WAL Log的API，MongoRiver 可以读取MongoDB 的<strong>opLog</strong>，GoldenGate为Oracle 实现了类似的机制。</p><h4 id="Log-Compaction"><a href="#Log-Compaction" class="headerlink" title="Log Compaction"></a>Log Compaction</h4><p>和数据库引擎类似，Kafka 也提供了对于日志压缩存储的功能，这使得日志能够持久化的存储在磁盘上，当系统接入一个新的Consumer后，可以从 index-0开始读取日志并重放，以此来获得上游数据源的最新数据。</p><h3 id="Event-Sourcing"><a href="#Event-Sourcing" class="headerlink" title="Event Sourcing"></a>Event Sourcing</h3><ul><li><p>CDC将DB看做是一个可变的table，每次更新操作都是通过底层的 Replication Log同步到另一个数据源，Stream的过程保证了数据变化的过程是同步的。</p></li><li><p>Event Sourcing：Application对于DB的更新由App自身基于不可变的table产生，在Event Sourcing中，event store是追加写的，没有update 和 delete的操作。Event反映上层应用发生的事件，而不是底层数据库状态的改变。</p></li></ul><p>单条event log本身不是非常有用，因为用户总是希望能够看到系统的最新状态而不是系统的修改记录，所以基于event log设计的应用需要将event log流转化为应用的state展现给用户，比如：从一系列购物车加减操作中计算出当前购物车中有什么商品。这种计算方式对于 log compaction的操作与CDC略有不同：</p><ul><li><p>CDC的操作日志中越新的日志保存的状态也是越新的，所以当获取到更新的状态之后，记录之前的状态的Log就可以被系统回收了。</p></li><li><p>Event Sourcing 基本上需要将所有状态变化保存下来，通过历史event 构建系统当前的状态。一个可以优化的地方是使用 checkpoint (snapshot)机制，定期对系统的state打快照，之后对于最新状态的计算则能够从checkpoint位置进行恢复。</p></li></ul><h3 id="State-Streams-Immutability"><a href="#State-Streams-Immutability" class="headerlink" title="State, Streams, Immutability"></a>State, Streams, Immutability</h3><p>从 Batch Process的角度来看，如果程序将Input文件视为 immutable的数据，那么不论执行多少次MP任务，得到的结果都是一致的（Fault Tolerance）。<strong>Immutable</strong> 这一特征在CDC和Event Sourcing中也十分有用。</p><p>但通常，我们会把DB看做是应用的最新状态，状态它自然是会改变的，因为DB必须支持 write 操作，这一看起来DB就是immutable的。但注意，DB 的State也是由一系列 event创造的状态，而event是追加写入 immutable file的，所以State 和 event 的关系可以用下式表达。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=Yjc2Y2Q4NmM0NmE4MGQ5M2Y3N2E2ODJmYmNjYjdiZDZfOVdwQlV5RWpObkdCVnlBakxhRzI5MWlQd1RjRHVYWUlfVG9rZW46Ym94Y256bWdpaUpkSjNzSU5mTmI5dGxoMVdkXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><p>可以这样理解，DB只是cache了event中计算出来最新的状态信息。在交易场景中，DB可以视作一个不可变对象，每一笔交易产生的时候都会向DB中创建一条记录 (<strong>Ledger</strong>)，用户账上的余额是由一系列Ledger计算出来的结果。如果交易过程中出现了问题，也不会修改交易的信息，而是通过追加一条新的交易记录来弥补错账。</p><p>Cons： 需要额外的存储空间保存海量的events</p><p>Pros：</p><ul><li><p>对金融系统的Audit (审计) 需求，监管者可能需要了解每一笔金融发生的原因和详情。</p></li><li><p>推荐：用户将商品A加入购物车一段时间后又删除，由于DB只保存了最新的State所以看不到购物车中有商品A，但是加购这个行为却能够说明用户对商品A 其实是喜爱的。</p></li></ul><h3 id="Messaging-and-RPC"><a href="#Messaging-and-RPC" class="headerlink" title="Messaging and RPC"></a>Messaging and RPC</h3><p>消息和RPC是分布式服务间两种通信方式，RPC通常应用于Actor 模式（请求-响应），这与基于Message 通信的不同点在于：</p><ul><li><p>RPC框架是基于分布式系统的并发和通信模式创建的，而流式处理是一种数据管理的手段</p></li><li><p>Actor 之间的通信通常都是一次性的且一对一的模式，而流式处理可以有多个消费者消费一个事件</p></li><li><p>Actor模式的通信可以是有环的，而流式处理通常都是pipeline的模式</p></li></ul><h2 id="Reasoning-About-Time"><a href="#Reasoning-About-Time" class="headerlink" title="Reasoning About Time"></a>Reasoning About Time</h2><p>流式处理引擎通常都需要处理事件发生的时间，比如：过去五分钟内的最大值&#x2F;平均值，但是这里的时间比较是不明确的，到底是broker的local 时钟还是event发生的时间。</p><p>在批处理系统中，如果涉及到时间窗口的问题，批处理任务通常都会根据event 发生的时间进行划分，看broker本地的时间是没有意义的，因为事件发生的时间与消息队列broker的时间没有任何关系。使用event的时间戳能够保证determinitic 每次计算出的结果都是一致的。</p><p>但是也有一些流式引擎使用的是本地时间计算window，这样做的好处是简单，省去了解析event的步骤，并且在事件产生时间和事件处理时间lagging很小的时候能work，一旦出现了event 积压，导致处理事件远小于event time，这种方式还是会有问题。</p><h3 id="Event-Time-V-S-Process-Time"><a href="#Event-Time-V-S-Process-Time" class="headerlink" title="Event Time V.S Process Time"></a>Event Time V.S Process Time</h3><p>导致process time 落后于 event time的因素有很多，比如：network 延迟，broker 重启，event积压ddeng，将process time 等效为event time使用存在很多问题。在流量计算的场景中，如果下游统计服务重启之后，去消费Web Server的事件会发现在重启这段时间内出现一个流量的异动，但实际上Web Server的流量是均匀的，是 Stream Processor 内部的计算逻辑有误导致的。</p><p><img src="https://lo845xqmx7.feishu.cn/space/api/box/stream/download/asynccode/?code=MGNjZjkwMDM5OTk4YWI2ODgwNjM0ZjM0Y2I1MjZiMWRfWDdFUXNVclVmOTFhMnRSSkpmZFNJa1o1S21Yd25pRzNfVG9rZW46Ym94Y25SUzM4cmZIdnJUMU56UEZ2VnlpRTViXzE2Njc0NDA3OTk6MTY2NzQ0NDM5OV9WNA" alt="img"></p><h3 id="Knowing-when-you-are-ready"><a href="#Knowing-when-you-are-ready" class="headerlink" title="Knowing when you are ready"></a>Knowing when you are ready</h3><p>在决定流处理窗口的时候存在一个比较tricky的问题：如果一条消息在网络中延迟了，导致其错过了计算窗口的下沿，此时应该如何处理这条event？可以考虑两种解决方案：</p><ul><li><p>直接丢弃该消息，因为正常情况下，延迟到达的概率很小；此时需要对被Drop的消息加一个监控，当大量消息drop时，使系统产生报警</p></li><li><p>生产一个correction消息</p></li></ul><p>在某些情况下，生产者可以追加一条记录通知消费者“从现在开始，不会再有比t时刻更早的消息产生”。但如果不同的生产者处于不同的机器上，且每一个生产者维护其最小的时间戳阈值，则下游消费者需要保存每个生产者的时间位点。</p><h2 id="Stream-Joins"><a href="#Stream-Joins" class="headerlink" title="Stream Joins"></a>Stream Joins</h2><h3 id="Stream-Stream-Join"><a href="#Stream-Stream-Join" class="headerlink" title="Stream-Stream Join"></a>Stream-Stream Join</h3><blockquote><p>场景：搜索</p></blockquote><p>假设我们需要计算网站上URL 最近某个时间段的 trend（微博热搜）。每当用户搜索一个关键词的产生一条event，每当用户点击一个关键词产生一条event，为了计算CTR，则需要将两条event基于&#96;&#96;session_id&#96;做join操作。</p><p>注意，将搜索结果存放到点击的请求中的结果和Join的意义是不一样的，因为第一种做法只能告诉系统用户在某个搜索结果中点击了某个关键词，如果用户没有做点击操作的话，则统计不到。</p><p>为了实现这种类型的Join操作，Stream Processor需要维护一个状态，比如：<strong>过去一小时</strong>中发生的所有events，由&#96;&#96;session_id&#96;建立索引。当一个搜索event 到来的时候存入对应的index 中，当一个click event 到来的时候存入对应的index中，当检测到 search index 和 click index 中有匹配的记录就可以emit一个事件表示某个搜索结果被点击。如果超过了时间窗口（此处为<strong>一小时</strong>），则认为该搜索结果没有被点击，也产生emit一个事件表示这个搜索结果无人问津。</p><h3 id="Stream-Table-Join"><a href="#Stream-Table-Join" class="headerlink" title="Stream-Table Join"></a>Stream-Table Join</h3><blockquote><p>场景：推荐</p></blockquote><p>假设需要为抖音的用户基于它的浏览记录和好友信息进行推荐，则可以将好友信息看做是一个静态的Table，User的浏览记录看做是Stream。Stream中每一个Event包含了UserID和这个用户浏览的内容。为了实现Stream和Table的Join，一种做法是对于每一个新进的Event，将其与Table做一次Join，比如用UserID和 Friends做一次Join操作，但这类DB 的Query对性能开销比较大；另一种做法是将Table缓存在Stream Processor上，省去了远程的读DB操作。这种做法和Batch Processing中 Map-Join的思想类似，Map Join也是将一个比较小的表存入Batch Processor的缓存中，与 Input 做Join。</p><p>Stream-Table Join 与 Map Join不同的是，Map Join只需要任务执行时刻的Table 快照，而Stream Processing 需要 Table的最新信息，可以用Capture Data Change的方式获取Table的更新信息，Stream Processor 订阅Table的变化 (Binlog) 。</p><p>这样我们需要维护两个Stream，一个是User在APP上的浏览记录，一个是用户好友更新的信息。</p><h3 id="Table-Table-Join"><a href="#Table-Table-Join" class="headerlink" title="Table-Table Join"></a>Table-Table Join</h3><blockquote><p>场景：朋友圈</p></blockquote><p>当用户打开微信朋友圈时，如果全量的去查询用户所有好友最新的朋友圈内容，展示给当前用户的话开销太大。所以可以为每个用户创建一个朋友圈缓存，可以想象成一个双向链表，当好友发布朋友圈的时候往链表中push，于是用户的打开朋友圈进行查看就可以变成一个 Single Look Up的操作。</p><p>维护这个缓存需要包含以下四种操作：</p><ul><li><p>当 User A 发布了一条朋友圈，则所有关注了 UserA的用户应该收到A的朋友圈 内容；</p></li><li><p>当A删除了朋友圈，则所有关注UserA的用户应该看不见A的朋友圈内容</p></li><li><p>当B 添加 A为好友，B需要能看到A所有最近发布的内容</p></li><li><p>当B删除好友A，B不该看见A所有的内容</p></li></ul><p>其实每个用户能看到的朋友圈内容应该是基于Event Stream计算出来的某个时刻的状态（最新状态），事件基于两个Stream（Friend &#x2F; UnFriend 和 Publish &#x2F; UnPublish）。</p><p>另一种方式看这个场景：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> friends.from_user_id <span class="hljs-keyword">as</span> timeline_id, agg(posts.<span class="hljs-operator">*</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> create_time <span class="hljs-keyword">desc</span>)<br><span class="hljs-keyword">from</span> posts <span class="hljs-keyword">Join</span> friends <span class="hljs-keyword">on</span> posts.creator <span class="hljs-operator">=</span> friends.to_user_id<br><span class="hljs-keyword">Group</span> <span class="hljs-keyword">by</span> friends.from_user_id<br></code></pre></td></tr></table></figure><p>每次朋友圈更新或者好友列表更新的时候，重新计算SQL的结果，投放到对应用户的Channel中。</p><h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><p>批处理任务的Fault Tolerance 是通过 Recompute 实现的，例如一个MapReduce 任务失败了，则Worker可以重新冲HDFS中读取文件，重新执行任务；因为文件是不可变的，所以每次执行的结果都是一致的。这和Exactly Once的结果是一样的，更准确的说这是一种 Effectively Once。但流处理中的fault tolerance和批处理不一样，批处理是一次任务完成之后产生结果，但是流处理基本上不会结束，所以不可能等到它结束才产生结果。</p><h3 id="Microbatching-and-checkpointing"><a href="#Microbatching-and-checkpointing" class="headerlink" title="Microbatching and checkpointing"></a>Microbatching and checkpointing</h3><ul><li>Spark Microbatching</li></ul><p>Spark Stream 采用了microbatching的机制，使用tumbling window将Stream划分为小Batch，时间窗口通常设置为1秒，如果窗口时间设置的过大，则意味着需要更长的时间才能产生结果，如果窗口时间设置得过小，则<strong>调度 (Scheduler) 和协调 (Coordinator)器</strong>会成为瓶颈。</p><ul><li>Flink checkpointing</li></ul><p>Flink采用了打快照的方式进行容错，如果Stream Processor在某个时间Crash了，则可以加载最近一次的checkpoint并且丢弃这段时间内所有状态的更新，重新计算 state，checkpoints是由Stream中的barrier触发的，它没有固定的时间间隔。</p><p>仅依靠以上两种技术是无法实现fault tolerance的，因为它无法保证原子性：如果event的计算结果离开了Message System进入了下游系统（DB system, Downstream Service），则消息产生的影响无法被回滚。</p><h3 id="Atomic-commit-revisited"><a href="#Atomic-commit-revisited" class="headerlink" title="Atomic commit revisited"></a>Atomic commit revisited</h3><p>为了保证 exactly-once 的语义，我们规定：一个事件能够产生Output的充要条件是这个事件能够产生的所有影响同时执行成功，比如：Consumer Offset 增加，DB成功写入，成功发送邮件等。这些effects需要保证原子性，要么同时发生要么都不发生。在分布式系统中，需要使用两阶段提交来保证原子性，但是在Stream Processing中，可以使用比 2PC 更高效的一种方式实现原子性。</p><p>在消息系统<strong>内部</strong>维护其他系统状态的变化和将要发给其他系统的消息，用一个Transaction包含多个消息这种方式抵消2PC带来的瓶颈。</p><h3 id="Idempotence"><a href="#Idempotence" class="headerlink" title="Idempotence"></a>Idempotence</h3><ul><li><p>所有节点按照相同顺序执行一致的 Log </p></li><li><p>为了防止过期的节点重复执行 Log，需要使用 <strong>Fencing Token</strong></p></li></ul><h3 id="Rebuilding-State-after-a-failure"><a href="#Rebuilding-State-after-a-failure" class="headerlink" title="Rebuilding State after a failure"></a>Rebuilding State after a failure</h3><p>把State保存在本地内存中，周期性刷到持久化存储或者复制到远端的机器上</p><ul><li><p>Flink 周期性给State 打 Snapshot并且刷到HDFS上</p></li><li><p>Kafka &#x2F; Samza会把state投放到一个特殊的 topic中，用于日志压缩，和CDC原理类似</p></li><li><p>VoltDB 通过在多个节点上执行Input Message的方式来实现fault tolerance （和 Raft类似）</p></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇主要讲述了Stream Processing的是如何服务于大型应用的，Stream 和 Batch System的原理类似，但是Stream 的实时性要求更高，并且Stream处理的数据是源源不断的。Message Broker 和 Event Logs对于Stream 来说就像Batch Processing中的文件系统。</p><blockquote><p>两种风格的 Message Broker</p></blockquote><ul><li><p>AMQP &#x2F; JMS Style：broker会将一条消息发送给一个Consumer，当Consumer处理完成并且返回ACK之后，Broker会将消息删除。这有点类似异步的RPC调用， RPC调用完成后，这条消息就没有意义了（不用revert back或者读历史消息），所以可以从本地删除</p></li><li><p>Log Based Message Broker：Broker将消息投放到对应的Partition中，每个Partition会分给一个Consumer Node，Partition内的消息有序。Consumer通过维护offset记录消费进度，同时broker会将消息持久化到磁盘上，所以消息能够被追溯</p></li></ul><p>Stream Processing的几种应用场景：</p><ul><li><p>对于event的复杂查询</p></li><li><p>时间窗口内做聚合（搜索过去一天最长搜索的关键字：微博热搜）</p></li><li><p>用于同步不同的下游数据系统 (DB -&gt; ES &#x2F; Cache &#x2F; OLAP)</p></li></ul><p>Stream Processing中三种 Join场景：</p><ul><li><p>Stream-Stream：统计一段时间窗口内，不同Stream的信息。（搜索和推荐场景）</p></li><li><p>Stream-Table: 类似MapReduce中的MapJoin，本地维护Table的最新版本，与进入Processor的消息做Join操作。</p></li><li><p>Table-Table：输入是两个Table的更新Event，将这两个Stream Join之后用于更新Table Join的 Materialized View</p></li></ul><p>Fault Tolerance：</p><ul><li><p>Microbatching and checkpointing</p></li><li><p>Transactions</p></li><li><p>Idempotent Writes</p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Designing data-intensive applications: The big ideas behind reliable, scalable, and maintainable systems. Stream Processing.</p><p>[2] <a href="https://kafka.apache.org/documentation/s">Apache Kafka Documentation</a></p><p>[3] [Bottled Water: Real-time integration of PostgreSQL and Kafka](</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>Big Data</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scalable SQL and NoSQL Data Stores</title>
    <link href="/2022/10/04/nosql/"/>
    <url>/2022/10/04/nosql/</url>
    
    <content type="html"><![CDATA[<p>Motivated by <strong>Web 2.0</strong>, the application was designed to support thousands of millions of users write and read concurrently. In contrast to traditional DBMS, NoSQL accepts BASE when designing, which means that they sacrifice some of the dimensions like transaction or strong consistency in order to maintain high availability or scalability.</p><p>Generally, there are several key features of NoSQL:</p><ul><li><p>Horizontally Scale.</p></li><li><p>Fault Tolerant and increasing throughput by partition and replication.</p></li><li><p>A weaker consistent model compared to ACID.</p></li><li><p>Efficient use of RAM and distributed index.</p></li></ul><p>Among the various NoSQL products, Google Bigtable, Amazon Dynamo and Memcached provide a “proof of concept” that inspired many deriving data stores.</p><ul><li><p><strong>BigTable</strong> pioneered the idea of partitioning persistent data into thousands of nodes. </p></li><li><p><strong>Memcached</strong> demonstrated that in-memory indexes can be highly scalable, distributing and replicating objects over multiple nodes.</p></li><li><p><strong>Dynamo</strong> is the first one to design a DB with eventually consistency and increase the service scalability and availability.</p></li></ul><p>NoSQL has a key feature called <strong>shared-nothing</strong> horizontal scaling by replicating data over many servers. But now NewSQL (e.g Amazon Aurora <a href="https://lo845xqmx7.feishu.cn/docx/doxcnLtzOhq6AwnL8BIbNdc5pGb">Amazon Aurora: Design for HighThroughput Cloud-Native Relational Databases.</a>) has started the trend of shared-storage architecture where server nodes don’t store data locally but in Distributed File System.</p><h2 id="Category"><a href="#Category" class="headerlink" title="Category"></a>Category</h2><p><strong>Key-Value Store</strong>: These systems store values and an index to find them, based on a programmer defined Key. (Redis, Memcached, Voldemort.)</p><p><strong>Document Store</strong>: The documents are <strong>indexed</strong> and a simple query mechanism is provided. (MongoDB, CouchDB)</p><p><strong>Extensible Record Stores</strong>: The systems store extensible records that can be partitioned vertically and horizontally across nodes (Wide-Column-Stores). (BigTable, HBase, Cassandra)</p><p><strong>Relational Database</strong>: (MySQL, PostgreSQL)</p><h2 id="Key-Value-Stores"><a href="#Key-Value-Stores" class="headerlink" title="Key Value Stores"></a>Key Value Stores</h2><p>Suitable for Query only by the primary key.</p><table><thead><tr><th></th><th>Data Model</th><th>Concurrence Control</th><th>Consistency</th><th>Partition</th><th>Sync Partition</th><th>Logic Clock</th></tr></thead><tbody><tr><td>Voldemort</td><td>KV Pairs</td><td>MVCC</td><td>Eventually Consistency</td><td>Cons Hashing</td><td>Async</td><td>Dynamo Vector Clock</td></tr><tr><td>Riak</td><td>Json</td><td>MVCC</td><td>Quorum (R, W, N)</td><td>Cons Hashing</td><td>Async</td><td>Dynamo Vector Clock</td></tr><tr><td>Redis</td><td>Various Data Structure</td><td>Lock</td><td>Eventually Consistency</td><td>Cons Hashing</td><td>Async</td><td>No need (Leader Based)</td></tr><tr><td>Scalaris</td><td>Various Data Structure</td><td>ACID</td><td>Strong Consistency</td><td>Range</td><td>Sync</td><td></td></tr></tbody></table><h2 id="Document-Stores"><a href="#Document-Stores" class="headerlink" title="Document Stores"></a>Document Stores</h2><p>Unlike KV Store, the document store provides the ability to query by multiple columns.</p><table><thead><tr><th></th><th>Concurrence Control</th><th>Consistency</th><th>Partition</th><th>Sync Partition</th><th>Feature</th></tr></thead><tbody><tr><td>SimpleDB</td><td>N.A</td><td>Eventually Consistency</td><td>Manually</td><td>Async</td><td>Does not allowed nested docs</td></tr><tr><td>CouchDB</td><td>MVCC on single Doc</td><td>ACID on one document</td><td>Manually</td><td>Async</td><td></td></tr><tr><td>MongoDB</td><td></td><td>Atomic Writes on field</td><td>Automatic sharding key defined by user</td><td>Master-Slave Async</td><td></td></tr></tbody></table><h2 id="Extensible-Records-Stores"><a href="#Extensible-Records-Stores" class="headerlink" title="Extensible Records Stores"></a>Extensible Records Stores</h2><p>The extensible record stores have been motivated by Google Bigtable. Their basic data model is rows and columns and their basic scalability model is splitting both rows and columns over multiple nodes.</p><ul><li><p>Rows are split across nodes through sharding on the primary key. They typically split by range rather than hash so that the range query can be partition pruned.</p></li><li><p>Columns of the table are distributed over multiple nodes by using “Column Groups”. These are for the relevant columns to be grouped together. The column groups don’t have to be stored on the same node, that is kind of vertical partitioning.</p></li></ul><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>HBase is a distributed table store system patterned directly after Bigtable.</p><ul><li><p>HBase has a Log Structured merge file index allowing fast range queries and sorting</p></li><li><p>HBase persistent storage uses HDFS instead of GFS, it writes to the RAM immediately and periodically flushes the data to the Storage</p></li><li><p>Row operations are atomic with row level locks and there’s optional support for transaction within a wide range based on MVCC. If multiple versions have conflict, the following ones will be aborted.</p></li></ul><h3 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a>Cassandra</h3><ul><li><p>Cassandra is similar to HBase in the storage process that immediately writes to RAM and periodically flushes to disk.</p></li><li><p>Cassandra has a weaker consistency model where there’s no locking mechanism and replicas are updated asynchronously.</p></li><li><p>Cassandra uses an ordered hash index which should give most of the benefit of both hash and BTree but the sorting will be slower than BTree.</p></li><li><p>Cassandra uses quorum read to provide a way to get the latest data.</p></li></ul><h2 id="Scalable-Relational-Database"><a href="#Scalable-Relational-Database" class="headerlink" title="Scalable Relational Database"></a>Scalable Relational Database</h2><p>From MySQL Cluster, several new products have come out, e.g VoltDB and Clustrix. It appears that the RDMS are restricted by two provisions:</p><ul><li><p>Small Scope Operation: Join over many tables will not scale well with sharding.</p></li><li><p>Small transactions: Transactions that span many nodes are going to be very insufficient with the communication of 2PC overhead.</p></li></ul><h3 id="MySQL-Cluster"><a href="#MySQL-Cluster" class="headerlink" title="MySQL Cluster"></a>MySQL Cluster</h3><p>MySQL Cluster works by replacing the InnoDB layer with NDB layer and the data is sharded over multiple database servers (shared nothing architechture). Every shard s replicated to support recovery.</p><p>MySQL Cluster support in-memory storage as well as disk storage.</p><h3 id="VoltDB"><a href="#VoltDB" class="headerlink" title="VoltDB"></a>VoltDB</h3><p>VolteDB’s scalability and availability features are competitive with MySQL Cluster and NoSQL systems</p><ul><li><p>Tables are partitioned over multiple servers and clients can call any servers.</p></li><li><p>Selected tables can be replicated over servers, e.g for fast access to read-mostly data.</p></li><li><p>Fault Tolerance Data can be recovered from peer nodes in the event of one node crashs.</p></li></ul><p>VoltDB eliminates nearly all “waits” in SQL execution, allowing a very efficient implementation:</p><ul><li>The system is designed for a database that fits in the RAM on servers so that the system need never wait for the disk. Indexes and record structures are designed for RAM rather than disk and the overhead of Disk cache&#x2F;buffer is eliminated as well.</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
      <tag>RDMS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distributed Lock</title>
    <link href="/2022/06/10/DistributedLock/"/>
    <url>/2022/06/10/DistributedLock/</url>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><blockquote><p>Distributed locks are a very useful primitive in many environments where different processes must operate with shared resources in a mutually exclusive way.</p></blockquote><p>Developers want to ensure that a shared resource (which can be an operation Update, or some content Read) on multi-instance nodes in the cluster can only be shared by one client at a time ( <strong>mutual exclusion</strong> ).</p><p>After reading the debate between Martin Kleppmann (DDIA Author) and Antirez (Father of Redis ) on whether the Redlock algorithm is safe, it is found that the application of distributed locks can actually be divided into different business scenarios:</p><ul><li><p>Efficiency: Using distributed locks can prevent different nodes from repeating the same work, but if the lock occasionally fails, it will not bring too severe impact. For example: a <strong>verification code</strong> is repeatedly sent.</p></li><li><p>Correctness: The business expects that under no circumstances will there be an error situation, because once it happens, it will have a huge impact. For example: repeat transactions (deductions &#x2F;additions ).</p></li></ul><p>Based on the above analysis, the implementation of distributed locks usually has two schemes based on Redis and ZK to my knowledge. Their comparison is shown in the following table:</p><table><thead><tr><th></th><th>强一致</th><th>高可用</th></tr></thead><tbody><tr><td>典型组件</td><td>ETCD, ZooKeeper, Chubby</td><td>Redis, Abase</td></tr><tr><td>时延 (Latency)</td><td>跨洋RTT 200ms</td><td>国内多机房 2ms 一次读写</td></tr><tr><td>优点</td><td>不会出现数据不一致的问题</td><td>脑裂切换的时候可能会出现不一致</td></tr><tr><td>缺点</td><td>延迟和吞吐受限</td><td>延迟极低，吞吐极大</td></tr></tbody></table><p>The selection of distributed locks is based on the concept of a certain component at the beginning of the design. For example, the Redis distributed solution is designed to be a highly available model at the beginning. According to CAP, however, by consulting the official doc of Redis , I found that there are actually many open source practices that have introduced additional algorithms on top of the basic version of Redis to achieve business expectations.</p><h3 id="Distributed-Lock-Principle-x2F-Rules"><a href="#Distributed-Lock-Principle-x2F-Rules" class="headerlink" title="Distributed Lock Principle &#x2F; Rules"></a>Distributed Lock Principle &#x2F; Rules</h3><ul><li><p><strong>Mutex (Consistency)</strong>: At Anytime, there’s only one client can have the lock.</p></li><li><p>Safety: Avoid deadlock, when client failover blocking the lock, the lock should be release as well.</p></li><li><p><strong>Availability:</strong> Avoid standalone service, when the master node failover, there must be some follower node can take over and continue providing service.</p></li><li><p>Reentran: For the same lock, only the process that acquired the lock can release the lock.</p></li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Strong-Consistency-Distributed-Lock"><a href="#Strong-Consistency-Distributed-Lock" class="headerlink" title="Strong Consistency Distributed Lock"></a>Strong Consistency Distributed Lock</h3><p>Based on strong consistency, we will have ZooKeeper, etcd , and Google’s Chubby. In these implementations, ZK is based on the ZAB consensus algorithm and etcd is based on Raft. On the other hand, although Google proposed <a href="https://research.google.com/archive/chubby-osdi06.pdf">chubby </a>in the paper, the open source code has not been published, so Chubby is generally not used as a distributed lock solution, and Chubby has an open source version called ZooKeeper.</p><p>Chubby&#x2F; ZK is inspired by the file system and designed as a service for <strong>metadata management</strong> . Different from large files, these two systems are designed for small data, usually the data volume should be below 10G. Chubby internally achieves consistency through Paxos, and ZK ensures internal consistency through ZAB.</p><p>In addition to metadata management, ZK also supports the following:</p><ul><li><p>Distributed lock</p></li><li><p>Coordination services: For example, the Master in the MapReduce can be implemented in ZK to issue tasks to idle workers.</p></li><li><p>“Sequencer”: write sequentially</p></li></ul><p>For Raft-based etcd , it is clear that throughput and latency are limited by the performance of the Raft Group Leader. If we deploy our services across continents or IDCs , e.g. VA , SG , RU , EU  TTP , etc. The entire system must do transoceanic network transmission for communication. No matter where the leader is, the Follower must go far away to synchronize the RaftLog from the leader, so the performance will be very low.</p><blockquote><p>I have discussed with one of my friends who’s from Tsinghua University about this problem and he provides me some basic insights that Leaderless &#x2F; Multi-Leader Architecture might be able to ease this high latency problem.</p></blockquote><p>For ZAB-based ZooKeeper, its latency should be smaller than ETCD , and the throughput should also meet the needs of the business. But from the monitoring point of view, SG ‘s cluster does not have much traffic, so it is speculated that the usage in SG’s business should not be very large and the infra support is not quite well.</p><h4 id="etcd-Distributed-Lock-Implementation"><a href="#etcd-Distributed-Lock-Implementation" class="headerlink" title="etcd Distributed Lock Implementation"></a>etcd Distributed Lock Implementation</h4><blockquote><p>KV Pair form to do the lock, based on Raft guarantee Linearizability, high latency, in fact, the company’s internal support scheme for ETCD and ByteKV similar.</p></blockquote><p>I feel that the performance of these strongly consistent locks is very related to the deployment method, and it is impossible to generalize which method will be better than the other. And the fineness of different business scenarios is not the same. To my knowledge, business operations scenarios do not require very high mutual exclusion of critical sections, or there are some fall back strategies, which can reduce the dependence on strong consistency. Because the probability of the consistency problem of highly available distributed locks is relatively small after all, it may be painful to choose a strongly consistent KV with low availability for small probability events.</p><p>Etcd provides Lock API：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">LOCK [<span class="hljs-keyword">options</span>] <span class="hljs-symbol">&lt;lockname&gt;</span> [<span class="hljs-keyword">command</span> arg1 arg2 ...]<br></code></pre></td></tr></table></figure><p>Description：LOCK acquires a distributed mutex with a given name. Once the lock is acquired, it will be held until <strong>etcdctl</strong> is terminated.</p><p>Options:</p><ul><li>ttl - time out in seconds of lock session. (Default Value: 60s).</li></ul><p>If the client side does not release the lock due to some problem, other processes will be blocked by the time of the TTL . The default time of the TTL is 60s, which means that the lock service is unavailable during these 60s.</p><h4 id="ZK-Distributed-Lock-Implementation"><a href="#ZK-Distributed-Lock-Implementation" class="headerlink" title="ZK Distributed Lock Implementation"></a>ZK Distributed Lock Implementation</h4><p>Based on ZNode, the only way to use file namespace to do Mutex, based on the ZAB protocol to ensure synchronization between replicas, the delay is lower than that of Raft (further investigation is required).</p><p>The following code is written based on the understanding of Chubby. Maybe ZK will be slightly different. I only have limited time to look at ZK for the time being. I wrote it according to the previous understanding of Chubby’s paper.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs Go">x = Open(<span class="hljs-string">&quot;/$&#123;PSM&#125;/$&#123;biz_content&#125;/resource_key&quot;</span>)<br><span class="hljs-keyword">if</span> TryAcquire(x) == ok &#123;<br>    <span class="hljs-comment">// Lock Success</span><br>    SetContents(x, resource_random_value)<br>    reply(ok)<br>&#125;<span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">// Lock failed</span><br>    reply(err)<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="HA-Distributed-Lock"><a href="#HA-Distributed-Lock" class="headerlink" title="HA Distributed Lock"></a>HA Distributed Lock</h3><p>Based on the idea of High Availability, we will have Redis (-liked) storage system. Redis is the typical implementation in the industry. Though it might contradict with the principle idea of distributed lock, some programmers consider this mechanism due to easy implementation, low latency but sacrifice some consistency on the application layer. In which, the application has to ensure that even when there are some mistakes like two process enter the <strong>critical region</strong>, the application won’t reveal some mistakes.</p><h4 id="3-2-1-Redis-Distributed-Lock-Implementation-StandAlone"><a href="#3-2-1-Redis-Distributed-Lock-Implementation-StandAlone" class="headerlink" title="3.2.1 Redis Distributed Lock Implementation (StandAlone)"></a>3.2.1 Redis Distributed Lock Implementation (StandAlone)</h4><ul><li><strong>Acquire Lock</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell">SET resource_key resource_random_value NX PX 30000<br>SETNX<br>EXpire<br></code></pre></td></tr></table></figure><ol><li><p>PX: Add a TTL to the lock: To prevent deadlock when client acquires lock then permanently failover.</p></li><li><p>NX: To ensure mutex in distributed lock which means that only when <code>resource_key</code>  is not exist, the process can acquire the lock.</p></li><li><p><code>resource_random_value</code>: some unique identifier which denotes the owner of the distributed lock to prevent process A wrongly release the lock own by process B.</p></li></ol><p><strong>Counter Example:</strong></p><p>![AcLock](..&#x2F;img&#x2F;tech&#x2F;Distributed Lock&#x2F;AcLock.jpg)</p><ul><li><strong>Release Lock</strong></li></ul><p>The unlock operation is like a CAD (Compare and Delete) command, where the redis should maintain its atomic semantics.</p><ul><li>In Redis, we can use LUA script to implement this.</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell">if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then<br>    return redis.call(&quot;del&quot;,KEYS[1])<br>else<br>    return 0<br>end<br></code></pre></td></tr></table></figure><p>![AcLock](..&#x2F;img&#x2F;tech&#x2F;Distributed Lock&#x2F;RlsLock.jpg)</p><h3 id="Redis-Distributed-Lock-Cluster-Redlock"><a href="#Redis-Distributed-Lock-Cluster-Redlock" class="headerlink" title="Redis Distributed Lock (Cluster + Redlock)"></a>Redis Distributed Lock (Cluster + Redlock)</h3><p>As can see from last part, the problem for <strong>Naive Redis</strong> Distributed Lock is the inconsistency during <strong>Leader Transfering</strong>. There has been a Algorithm, Redlock, proposed by Antires (father of Redis), might be able to handle this problem. The typical flow is described as below:</p><p>Assume that there are five node in a Redis Cluster, in order to acquire lock, the client performs the following operations:</p><ol><li><p>START: Gets the current timestamp.</p></li><li><p>It tries to acquire locks in all the N instances sequentially, using the command described in 3.2.1. The client will set a small timeout compare to the valid time of lock. For example, if the TTL for lock is 10s, then the timeout for client waiting for nodes’ reply may be around 5-50 ms. This prevents client waiting nodes’ reply for so long and when there’s a instance timeout, the client should immediately try next node in Redis Cluster.</p></li><li><p>After step:2 success, the client compute the current timestamp, END and minus START for the time of acquiring lock. If the time is under lock valid time and there are majority nodes reply <code>ok</code> of step:2 THEN</p></li><li><p>If the lock was successfully acquired, its validity time is considered to be the initial validity time minus the time elapsed in step:3.</p></li><li><p>If the client failed to acquire the lock <code>(END-START &gt; Lock valid time || less then majority nodes reply ok)</code>, then the client will try to unlock all the instances.</p></li></ol><p><strong>[Clock Drift]</strong></p><p>The <a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">time</a> in Distributed System is not reliable due to network delay or atomic drifting. So any service that deploys across different server cannot rely on the TRUETIME assumption.</p><p>The <a href="https://redis.io/docs/reference/patterns/distributed-locks/">original redlock post</a> states that the algorithm do not rely on synchronized clock and take clock drift in distributed system into account.</p><p><strong>[Retry on Failure]</strong></p><p>After the client fails to acquire the lock, it should sleep for a while so that other client will not synchronizing acquire lock which might give rise to a split brain (where no client can acquire the lock). Ideally, the client should try to send the SET command using multiplexing.</p><p><strong>[More Reliable]</strong></p><p>The client can extend the lock’s TTL during the computation of lock’s valid time. By sending an extension command with LUA script to all the instances that the key exist on, the client can extend the liveness of the lock, but should be stay within the validity time.</p><p><strong>[Performance, Crash Recovery and Fsync]</strong></p><ul><li>Persistence</li></ul><p>Without persistence, the lock service cannot guarantee that mutex semantic. For example, when client A acquires the lock in 3 in 5 instances and one of the instances restarted, then we will have 3 instances are lock-free at this point.</p><p>AOF: Redis by default will fsync to disk by every seconds, it is possible that after a  restart, the lock key is missing. In theory, if we want to ensure any kind of instance start, we need to enable <code>fsync=always</code> but will affect performance due to DiskIO.</p><ul><li>Delay Restart</li></ul><p>To guarantee that there’s no two instance acquire lock at the same time, we can implement delay restart mechanism in which the shutdown server can only restart after <code>TTL</code>. After it recover, all servers is clean and can be locked.</p><p>But there’s downsides for this strategy when there are too many servers shutdown within <code>TTL</code> so that the lock service is unavailable for at least <code>TTL</code>.</p><p>More Information about Redlock: There’s a famous debate session within this topic between Martin Kleppmann (Distributed System Expert) and <a href="http://antirez.com/">Antirez</a> (Father of Redis).</p><h4 id="Analysis-of-Redlock"><a href="#Analysis-of-Redlock" class="headerlink" title="Analysis of Redlock"></a>Analysis of Redlock</h4><p>Martin Kleppmann <a href="http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">analyzed Redlock here</a>. A counterpoint to this analysis can be <a href="http://antirez.com/news/101">found here</a>.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>Regarding distributed locks, the solutions that are better supported within the company can be distinguished according to high availability and strong consistency scenarios<ul><li>High Availability Scenario: With Redis , businesses need to be compatible with inconsistent events with low probability.</li><li>Strongly consistent scheme: Using ByteKV :<ul><li>For a single cluster across the room, the business needs to tolerate the delay of Raft synchronization across the room.</li><li>For multi-cluster cross-room, the business needs to tolerate that the service cannot write data to the leader of multiple clusters at the same time, otherwise unpredictable errors will occur.</li></ul></li></ul></li></ul><p>Personally speaking, I think a better solution is to align with the overall service structure + business scenario, because different service structures may require different TradeOff, and different business scenarios have different preferences for different scenarios. But overall, through this survey, about a week before and after, I gained some understanding of the company and open source projects, including: ByteKV , ZooKeeper, Chubby, Redis , ETCD ; also let me The concept of distributed locks and business selection TradeOff has more experience.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SegmentQuery</title>
    <link href="/2022/06/06/SegmentQuery/"/>
    <url>/2022/06/06/SegmentQuery/</url>
    
    <content type="html"><![CDATA[<h1 id="区间问题"><a href="#区间问题" class="headerlink" title="区间问题"></a>区间问题</h1><h2 id="QuickView"><a href="#QuickView" class="headerlink" title="QuickView"></a>QuickView</h2><p>读者可以自行思考一个问题：对于一个数组 a，如果我想对 a 的进行以下三种操作，在不使用本文数据结构的情况下，暴力算法的时间复杂度为多少：</p><ul><li><p>单点修改，区间查询：将第x个元素加k，求出 [x, y] 的区间和。</p></li><li><p>区间修改，单点查询：将 [x, y] 区域内的元素加k，并且求出第$ i (x &lt;&#x3D; i &lt;&#x3D; y) $个元素的值。</p></li><li><p>区间修改，区间查询：将 [x1, y1] 区域内的元素加k，并求出 [x2, y2] 的区间和</p></li></ul><h2 id="本文介绍数据结构"><a href="#本文介绍数据结构" class="headerlink" title="本文介绍数据结构"></a>本文介绍数据结构</h2><ul><li><p>差分数组</p></li><li><p>树状数组</p></li><li><p>线段树</p></li></ul><h3 id="差分数组"><a href="#差分数组" class="headerlink" title="差分数组"></a>差分数组</h3><p>定义：假设原数组为: <code>a = int[]&#123;2,5,7,3,6,9&#125;</code>，则差分数组b 的每一项为：<br>$$<br>b(i)&#x3D;\left{ \begin{aligned}  &amp; a[i] - a[i-1], i &gt; 0 \ &amp; a[0], i&#x3D;0 \ \end{aligned} \right.<br>$$<br>如下图所示。</p><table><thead><tr><th>index</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><td>a[i]</td><td>2</td><td>5</td><td>7</td><td>3</td><td>6</td><td>9</td></tr><tr><td>b[i]</td><td>2</td><td>3</td><td>2</td><td>-4</td><td>3</td><td>3</td></tr></tbody></table><h4 id="差分数组的性质"><a href="#差分数组的性质" class="headerlink" title="差分数组的性质"></a>差分数组的性质</h4><ul><li>性质一：</li></ul><p>求原数组中元素 a[i]，相当于求差分数组的前缀和<br>$$<br>a[i] &#x3D; \sum_{j&#x3D;0}^{i}b[j]<br>$$</p><ul><li>性质二：<ul><li><p>求原数组中的前缀和：$\sum_{i&#x3D;0}^{x}a[i] $，代入上式可得：</p></li><li><p>$\sum_{i&#x3D;0}^{x}a[i] &#x3D; \sum_{i&#x3D;0}^{x}\sum_{j&#x3D;0}^{i}b[j] &#x3D; \sum_{i&#x3D;0}^{x} (x-i+1)*b[i] $</p></li><li><p>关于第二个等号，可以举个例子来说明：</p></li><li><p><img src="/img/tech/SegmentQuery/diff_array_eg.png" alt="img"></p></li></ul></li></ul><h4 id="差分数组的用途"><a href="#差分数组的用途" class="headerlink" title="差分数组的用途"></a>差分数组的用途</h4><blockquote><p>区间修改，区间查询</p></blockquote><ol><li>快速处理区间修改操作</li></ol><p>区间修改的操作可以分两步执行，例如对于操作给 [x, y] 区间加 K 这个操作：</p><ul><li><p>b[x] +&#x3D; K</p></li><li><p>b[y+1] -&#x3D; K</p></li></ul><ol><li>快速处理区间查询问题</li></ol><p>由性质二，可以在O(n) 的时间范围内查出原数组的前缀和；对于区间 [L, R] 的和为：</p><p>$SUM_{L, R} &#x3D; SUM[R] - SUM[L-1] $</p><p>对于区间查询的问题，可以通过进一步地维护一个额外的空间来Trade-Off来压缩时间复杂度，感兴趣的读者可以自行了解。</p><h3 id="树状数组"><a href="#树状数组" class="headerlink" title="树状数组"></a>树状数组</h3><p>树状数组 t[x] 的每一个元素保存了以x为根节点的子树中叶子节点值的和，如下图所示。</p><p><img src="/img/tech/SegmentQuery/TreeArray.png" alt="img"></p><p><strong>Lowbit 操作</strong></p><p>Lowbit(x): 表示x在二进制位表示下最低位的1及其之后的0构成的数值。如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Plain">lowbit(44) = lowbit(0b101100) = (0b100) = 4<br></code></pre></td></tr></table></figure><p>对于任意一个数 x 来说，它的lowbit值为：<code>lowbit(x) = x &amp; (-x)</code></p><p><strong>性质</strong></p><ul><li><p>在t[x] 数组中，Lowbit(x) 表示了t[x] 覆盖的元素个数。</p></li><li><p>t[x - lowbit(x)+1] 为 t[x] 覆盖原数组的第一个元素</p></li><li><p>t[x + lowbit(x)] 为 t[x] 在树状数组中的父节点元素</p></li></ul><p><strong>使用场景</strong></p><blockquote><p>单点修改，区间查询</p></blockquote><p>树状数组对于以下场景的计算具备良好的时间效率：</p><ul><li><strong>单点修改，区间查询</strong>，例如：修改 a[i]， 查询 $SUM_{L,R} $</li></ul><p>反应到树状数组中，主要提供两个API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Plain">Ask(x): Return the sum value from 0 to x<br>Add(x, k): Add k to x<br></code></pre></td></tr></table></figure><p>对于Ask(x) 操作，从树状数组 t[x] 开始，往树的根节点方向寻找元素并相加，直到最顶层。伪代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Go">sum := <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i:=x;i&gt;<span class="hljs-number">0</span>;i-=lowbit(i) &#123;<br>    sum += t[i]<br>&#125;<br></code></pre></td></tr></table></figure><p>对于Add (x, K) 操作，从树状数组 t[x] 开始，往树的根节点方向寻找元素，并且把寻找到的每个元素值加 K，直到最顶层。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Go"><span class="hljs-keyword">for</span> i:=x;i&lt;n;i+=lowbit(i)&#123;<br>    t[i] += K<br>&#125;    <br></code></pre></td></tr></table></figure><h3 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h3><p><img src="/img/tech/SegmentQuery/%E7%BA%BF%E6%AE%B5%E6%A0%91.jpg" alt="线段树"><br>线段树解决的问题是：区间修改，区间查询。</p><p>线段树的每个叶子节点对应原数组中的值，每个非叶子节点表示子节点的节点值之和。</p><p>每个节点的数据结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">struct</span> TreeNode&#123;<br>  left, right <span class="hljs-type">int</span> <span class="hljs-comment">// 表示当前节点覆盖的最左和最右节点的下标</span><br>  val <span class="hljs-type">int</span> <span class="hljs-comment">// 表示当前节点的节点值</span><br>  lazy <span class="hljs-type">int</span> <span class="hljs-comment">// lazy标记</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>区间查询</strong>的一般步骤为，自顶向下地递归查询：</p><ol><li>遇到query 区间和当前节点区间不相交的时候直接返回0；</li><li>如果query 区间完全覆盖当前节点的区间，则直接返回当前节点的值；</li><li>如果query 区间和当前节点的区间相交，则向下寻找被完全覆盖的节点并且返回该节点的值。</li></ol><p><strong>区间更新</strong>的操作为</p><blockquote><p>大体包括四个步骤：判断是否覆盖 -&gt; 看是否有lazy标记 -&gt; lazy标记下推 -&gt; 更新左右节点 -&gt; 更新父节点</p></blockquote><ol><li>如果Query 区间完全覆盖当前区间则给当前区间打上Lazy标记后返回，回溯的过程中更新父节点的值；</li><li>如果Query 区间完全覆盖的区间上有lazy标记，则将lazy标记下推，下推的操作是将当前的lazy操作应用到两个子节点上，然后返回，回溯的过程中更新父节点的值；</li></ol><h3 id="使用场景的总结"><a href="#使用场景的总结" class="headerlink" title="使用场景的总结"></a>使用场景的总结</h3><p><img src="/img/tech/SegmentQuery/%E5%8C%BA%E9%97%B4%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.png" alt="区间问题总结"></p><h3 id="可能帮助你理解的例子"><a href="#可能帮助你理解的例子" class="headerlink" title="可能帮助你理解的例子"></a>可能帮助你理解的例子</h3><h4 id="732-我的日程安排表-III"><a href="#732-我的日程安排表-III" class="headerlink" title="732. 我的日程安排表 III"></a><a href="https://leetcode.cn/problems/my-calendar-iii/">732. 我的日程安排表 III</a></h4><h4 id="1109-航班预订统计"><a href="#1109-航班预订统计" class="headerlink" title="1109. 航班预订统计"></a><a href="https://leetcode.cn/problems/corporate-flight-bookings/">1109. 航班预订统计</a></h4><p><strong>For More Information:</strong> <a href="https://github.com/SharingSource/LogicStack-LeetCode">SharingSource</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
      <tag>High Advanced DS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次面试算法题准备过程</title>
    <link href="/2022/06/03/OnceInterviewPreparation/"/>
    <url>/2022/06/03/OnceInterviewPreparation/</url>
    
    <content type="html"><![CDATA[<h2 id="我面试之前是如何准备算法题的"><a href="#我面试之前是如何准备算法题的" class="headerlink" title="我面试之前是如何准备算法题的"></a>我面试之前是如何准备算法题的</h2><p>先说说笔者自身的情况：</p><ul><li>北航本 &#x2F; 新加坡Top2 硕</li><li>本科（毕业后）的时候先后在腾讯和字节跳动实习 + 转正工作，经历过大大小小的面试总计大概10来场吧，选择性的粘贴一下之前的面经，可以感觉到不同的公司有不同的面试风格:<ul><li><p>阿里支付宝：比较偏重于Java世界，对于New Grad来说八股文的考察比较多，但很基础，电话面试。</p><ul><li><a href="https://www.nowcoder.com/discuss/862658?source_id=profile_create_nctrack&channel=-1">蚂蚁面经</a></li></ul></li><li><p>腾讯CSIG：比较偏重于 Java，特别是Spring框架，会考察应试者的知识广度，包括消息队列等等，但没有涉及分布式的topic。</p></li><li><p>字节跳动：字节的面试风格比较清新，直接了当，上来先写题，写完再聊，聊的东西也比较入门级别（对于NG而言），包括：Redis的基本数据结构，MySQL的并发设计；对于搞过分布式系统&#x2F;分布式存储的同学来说，只要算法没问题，基本秒过。</p><ul><li><a href="https://www.nowcoder.com/discuss/862652?source_id=profile_create_nctrack&channel=-1">Tiktok直播 SG实习</a></li><li><a href="https://www.nowcoder.com/discuss/596218?source_id=profile_create_nctrack&channel=-1">字节基础架构</a></li></ul></li><li><p>商汤：商汤面的<strong>算法岗</strong>，主要是就着Paper 问，只要Paper中的工作是自己做的，把AI框架搞懂了，Pass 应该没啥问题。但是感觉商汤的算法题有点天马行空，事先可能准备不到。</p></li><li><p>美团：美团面的基础架构，主要做AP系统的应该是，可能我给面试官的感觉比较菜？全程没问我分布式的内容，在聊项目，MySQL聊的比较多，Redis分布式锁。</p><ul><li><a href="https://www.nowcoder.com/discuss/865348?source_id=profile_create_nctrack&channel=-1">美团基础架构面经</a></li></ul></li></ul></li></ul><p>回到现实：这篇Blog记录了我第一次在新加坡准备面试的刷题路线，因为感觉论坛中热门的Hot200， Top100基本都刷了好几遍了，并且向UC Berkerly &#x2F; FaceBook 的大佬请教了经验之后，认为分类刷题是一种很好的习惯。准备面试算法，就和准备高中数学期末考试一样，既有广度，也要考察深度，我应对这种考试一般的方式就是分类刷题，每一个类别都需要细致的研究和总结。</p><p>下面这个链接是我的刷题记录。</p><p> <a href="https://github.com/GaryGky/leetcode-update">个人力扣刷题记录：leetcode-update</a></p><p> 最后还想说一点笔者的拙见：【心态要好】其实面试的过程是双向的，应试者没有必要抱着舔狗或者非要进XX公司不可的心态去面试，而应该将面试看做是一个双向了解和沟通的过程，面试官既是在考察应试者的技术能力，应试者也可以通过面试官的提问路线（是否循序渐进，是否能指出错误，沟通是否顺畅）来评估自己是否适合进入该组工作。</p><h3 id="隆重推荐，没收广告费"><a href="#隆重推荐，没收广告费" class="headerlink" title="隆重推荐，没收广告费"></a>隆重推荐，没收广告费</h3><blockquote><p>GitHub整理的LeetCode刷题指南：<a href="https://github.com/youngyangyang04/leetcode-master">https://github.com/youngyangyang04/leetcode-master</a></p></blockquote><p><img src="/img/tech/Algorithm/Category.png" alt="Leetcode Category Distribution"></p><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><blockquote><p>已掌握：双指针 滑动窗口 前缀和 动态规划</p></blockquote><blockquote><p>听说过未掌握：线段树</p></blockquote><h3 id="搜索旋转排序数组"><a href="#搜索旋转排序数组" class="headerlink" title="搜索旋转排序数组"></a>搜索旋转排序数组</h3><p>都需要在 **O(logN)**的时间复杂度内完成，因此思路都是希望每次迭代排除一半的元素。</p><ul><li>寻找旋转排序数组中最小值 (不允许重复)</li></ul><p><strong>解法</strong>：对于数组中最后一个元素，在最小值左边的元素，都严格大于最后一个元素，在最小值右边的元素，都严格小于最后一个元素。基于此发现，可以将中间位置与数组最后一个值比较。</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs Swift">int left<span class="hljs-operator">=</span><span class="hljs-number">0</span>, right<span class="hljs-operator">=</span>nums.length<span class="hljs-operator">-</span><span class="hljs-number">1</span>;<br><br><span class="hljs-keyword">while</span>(left<span class="hljs-operator">&lt;</span>right)&#123;<br><br>    int mid <span class="hljs-operator">=</span> (left<span class="hljs-operator">+</span>right)<span class="hljs-operator">/</span><span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] <span class="hljs-operator">&lt;=</span> nums[right])&#123; <br><br>        right<span class="hljs-operator">--</span>; <span class="hljs-comment">// 这里会导致变成O(n)</span><br><br>    &#125;<span class="hljs-keyword">else</span> &#123;<br><br>        left <span class="hljs-operator">=</span> mid<span class="hljs-operator">+</span><span class="hljs-number">1</span>;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> nums[left];<br></code></pre></td></tr></table></figure><ul><li>寻找旋转排序数组中最小值 (允许重复)</li></ul><p>是否允许重复并不会影响搜索的结果，所以解法和上面一道题相同。</p><ul><li>寻找旋转排序数组中是否存在某个值（不允许重复）</li></ul><p>高亮的部分严格控制了区域内的有序性，根据这个有序性，每次排除掉一半的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-type">int</span> left=<span class="hljs-number">0</span>, right=nums.length-<span class="hljs-number">1</span>;<br><br><span class="hljs-type">int</span> last=nums[right];<br><br><br><br><span class="hljs-keyword">while</span>(left&lt;=right)&#123;<br><br>    <span class="hljs-type">int</span> mid=(left+right)/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] == target) <span class="hljs-keyword">return</span> mid;<br><br>    <br><br>    <span class="hljs-keyword">if</span>(nums[mid]&gt;last)&#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&gt;target &amp;&amp; target&gt;last)&#123;<br><br>        <span class="hljs-comment">// 这里一定时严格有序的</span><br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>    <span class="hljs-keyword">else</span> &#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&lt;target &amp;&amp; target&lt;=last)&#123;<br><br>        <span class="hljs-comment">// 这里一定是严格有序的</span><br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><ul><li>寻找旋转排序数组中是否存在某个值（允许重复）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-type">int</span> left=<span class="hljs-number">0</span>, right=nums.length-<span class="hljs-number">1</span>;<br><br><span class="hljs-type">int</span> last=nums[right];<br><br><br><br><span class="hljs-keyword">while</span>(left&lt;=right)&#123;<br><br>    <span class="hljs-type">int</span> mid=(left+right)/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-keyword">if</span>(nums[mid] == target || nums[left]==target || nums[right]==target) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br><br>    <br><br>    <span class="hljs-comment">// 这种情况下无法判断 -&gt; O(N)</span><br><br>    <span class="hljs-keyword">if</span>(nums[mid]==nums[right] &amp;&amp; nums[mid]==nums[left])&#123;<br><br>        right--;<br><br>        left++;<br><br>        <span class="hljs-keyword">continue</span>;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">if</span>(nums[mid]&gt;last)&#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&gt;target &amp;&amp; target&gt;last)&#123;<br><br>        <span class="hljs-comment">// 这里一定时严格有序的</span><br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<span class="hljs-keyword">else</span> &#123;<br><br>        <span class="hljs-keyword">if</span>(nums[mid]&lt;target &amp;&amp; target&lt;=last)&#123;<br><br>        <span class="hljs-comment">// 这里一定是严格有序的</span><br><br>            left=mid+<span class="hljs-number">1</span>;<br><br>        &#125;<span class="hljs-keyword">else</span> &#123;<br><br>            right=mid-<span class="hljs-number">1</span>;<br><br>        &#125;<br><br>    &#125;<br><br>&#125;<br><br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br></code></pre></td></tr></table></figure><h3 id="删除有序数组重复项"><a href="#删除有序数组重复项" class="headerlink" title="删除有序数组重复项"></a>删除有序数组重复项</h3><blockquote><p>O(N)</p></blockquote><ul><li>不能重复 LC26</li><li>最多只有一个重复 LC80</li></ul><p>思路：以<strong>不能重复</strong>为例，使用快慢指针，slow指针之前的元素都是唯一的，fast之前的元素都被检查过。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Java">如果 nums[slow-<span class="hljs-number">1</span>] = nums[fast]，说明fast这个位置是重复的元素，应该跳过，直接更新fast++<br><br>如果 nums[slow-<span class="hljs-number">1</span>] != nums[fast]，有序性保证了fast位置的元素在slow之前都没有出现过，所以替换nums[slow]=nums[fast], 并且slow++，fast++<br></code></pre></td></tr></table></figure><p>循环做以上逻辑，直到fast超出原始数组长度，结束。此时slow保存了去重后的数组长度。</p><h3 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h3><p>能够求解的问题：</p><ul><li>判断图中的连通分量 LC547</li><li>判断是否能够产生二分图 LC785</li></ul><p>模板：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">UnionFind</span>&#123;<br><br>    <span class="hljs-type">int</span>[] parents;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">UnionFind</span><span class="hljs-params">(<span class="hljs-type">int</span> nodeNum)</span>&#123;<br><br>        parents = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[nodeNum];<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;nodeNum;i++) parents[i]=i;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">find</span><span class="hljs-params">(<span class="hljs-type">int</span> node)</span>&#123;<br><br>        <span class="hljs-keyword">while</span>(parents[node] != node)&#123;<br><br>            node = parents[node];<br><br>        &#125;<br><br>        <span class="hljs-keyword">return</span> node;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">union</span><span class="hljs-params">(<span class="hljs-type">int</span> node1, <span class="hljs-type">int</span> node2)</span>&#123;<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">root1</span> <span class="hljs-operator">=</span> find(node1);<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">root2</span> <span class="hljs-operator">=</span> find(node2);<br><br>        parents[root1]=root2;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isConnected</span><span class="hljs-params">(<span class="hljs-type">int</span> node1, <span class="hljs-type">int</span> node2)</span>&#123;<br><br>        <span class="hljs-keyword">return</span> find(node1) == find(node2);<br><br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>使用注意：</p><ul><li>二维矩阵问题需要转换为一维问题。</li></ul><h3 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a><a href="https://leetcode-cn.com/problems/last-stone-weight-ii/solution/yi-pian-wen-zhang-chi-tou-bei-bao-wen-ti-5lfv/">背包问题</a></h3><p>背包问题的定义是：给定一个背包的容量target，再给定一个数组nums表示物品，能否按照一定的方式选取nums中的元素得到target。</p><blockquote><p>在解决实际问题的时候， 通常是将背包类型和问题类型做笛卡尔乘积，然后选择合适的算法。</p></blockquote><p><strong>按照背包类型进行分组</strong>：</p><table><thead><tr><th>背包类型</th><th>内层遍历顺序</th><th>例题</th></tr></thead><tbody><tr><td>0-1 背包：每个元素只能使用一次</td><td>倒序遍历</td><td>给定背包容量，最多可以拿价值多少的物品 目标和问题 石头最小剩余的重量</td></tr><tr><td>完全背包问题：每个元素可以重复取</td><td>正序遍历</td><td>零钱兑换问题</td></tr><tr><td>组合背包问题：每个元素要求有序</td><td>正序遍历</td><td></td></tr><tr><td>分组背包：多个背包 （没见过）</td><td></td><td></td></tr></tbody></table><p><strong>按照问题的类型进行分组</strong>：</p><table><thead><tr><th>问题类型</th><th>递推公式</th><th>例题</th></tr></thead><tbody><tr><td>组合问题</td><td><code>dp[i]+=dp[i-num] </code></td><td>目标和</td></tr><tr><td>最值问题</td><td><code>dp[i]=min/max(dp[i],dp[i-num]) </code></td><td>剩余最少石头的数量 零钱兑换 完全平方数</td></tr><tr><td>存在性问题</td><td>&#96;dp[i]&#x3D;dp[i]</td><td></td></tr></tbody></table><h3 id="股票买卖"><a href="#股票买卖" class="headerlink" title="股票买卖"></a>股票买卖</h3><blockquote><p>股票买卖是一道用动态规划记录状态转移的问题</p></blockquote><ul><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/">LC121 股票 1</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii">LC122 股票 2</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iii">LC123 股票 3</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iv">LC 188 股票 4</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-cooldown">LC 309 股票 + 冷冻期</a></li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee">LC714 股票 + 手续费</a></li></ul><p>无法复制加载中的内容</p><p><strong>K 次买卖</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> Math.min(k, n/<span class="hljs-number">2</span>);<br><br>    <span class="hljs-type">int</span>[][][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][k+<span class="hljs-number">1</span>][<span class="hljs-number">2</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">// init: 第零天买入</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;=k;i++)&#123;<br><br>        <span class="hljs-comment">// dp[0][i][0] = 0;</span><br><br>        dp[<span class="hljs-number">0</span>][i][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>];<br><br>    &#125;<br><br>    <span class="hljs-comment">// DP</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 在第i天买入</span><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">1</span>;j&lt;=k;j++)&#123;<br><br>            <span class="hljs-comment">// buy：昨天的买入状态 or 昨天卖出今天买入</span><br><br>            dp[i][j][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]-prices[i]);<br><br>            <span class="hljs-comment">// sell: 昨天sell的状态 or 今天买入 然后卖出的状态</span><br><br>            dp[i][j][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">0</span>], dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>]+prices[i]);<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(dp[i][j][<span class="hljs-number">0</span>], ans);<br><br>        &#125;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>N次买卖包含冷冻期</strong></p><blockquote><p>添加一个冷冻期的状态frozen；</p></blockquote><ul><li><blockquote><p>卖出状态的前一天一定是冷冻期</p></blockquote></li><li><blockquote><p>冷冻期前一天可以是冷冻期或者买入状态</p></blockquote></li><li><blockquote><p>买入状态前一天可以是买入或者卖出状态</p></blockquote></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">k</span> <span class="hljs-operator">=</span> n/<span class="hljs-number">2</span>;<br><br>    <span class="hljs-type">int</span>[][][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][k+<span class="hljs-number">1</span>][<span class="hljs-number">3</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">// init: 第零天买入</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;=k;i++)&#123;<br><br>        <span class="hljs-comment">// dp[0][i][0] = 0;</span><br><br>        dp[<span class="hljs-number">0</span>][i][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>];<br><br>    &#125;<br><br>    <span class="hljs-comment">// DP</span><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 在第i天买入</span><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">1</span>;j&lt;=k;j++)&#123;<br><br>            <span class="hljs-comment">// buy：昨天的买入状态 or 昨天卖出今天买入</span><br><br>            dp[i][j][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]-prices[i]);<br><br>            <span class="hljs-comment">// frozen: 昨天是冷冻期 或者 昨天卖出了股票</span><br><br>            dp[i][j][<span class="hljs-number">2</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">2</span>], dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">1</span>] + prices[i]);<br><br>            <span class="hljs-comment">// sell: 昨天是冷冻期</span><br><br>            dp[i][j][<span class="hljs-number">0</span>] = dp[i-<span class="hljs-number">1</span>][j][<span class="hljs-number">2</span>];<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(Math.max(dp[i][j][<span class="hljs-number">0</span>], dp[i][j][<span class="hljs-number">2</span>]), ans);<br><br>        &#125;<br><br>    &#125;<br><br><br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>实际上，在进行无限次交易的时候，可以简化一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices)</span> &#123;<br><br>        <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>        <span class="hljs-type">int</span>[][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][<span class="hljs-number">3</span>];<br><br>        <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br>    <br><br>        <span class="hljs-comment">// init: 第1天状态</span><br><br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = dp[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 卖出和冷冻期都是0 因为不进行操作</span><br><br>        dp[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>]; <span class="hljs-comment">// 买入是 -price[0] 因为第一天买入</span><br><br>    <br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>            <span class="hljs-comment">// 第i天买入状态：i-1天买入状态 今天不操作 或者 i-1天卖出状态 今天买入</span><br><br>            dp[i][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] - prices[i]); <br><br>            <span class="hljs-comment">// 第i天冷冻期状态 i-1天必是买入状态</span><br><br>            dp[i][<span class="hljs-number">2</span>] = dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + prices[i];<br><br>            <span class="hljs-comment">// 第i天卖出状态 i-1天冷冻期 或者 i-1卖出今天不操作</span><br><br>            dp[i][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]);<br><br>            <span class="hljs-comment">// 记录最大值</span><br><br>            ans = Math.max(dp[i][<span class="hljs-number">0</span>], dp[i][<span class="hljs-number">2</span>]);<br><br>        &#125;<br><br>        <span class="hljs-keyword">return</span> ans;<br><br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>N次买卖包含手续费</strong></p><p>除了状态转移方程外，其他与N次买卖相同：假设手续费为：fee</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxProfit</span><span class="hljs-params">(<span class="hljs-type">int</span>[] prices, <span class="hljs-type">int</span> fee)</span> &#123;<br><br>    <span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> prices.length;<br><br>    <span class="hljs-type">int</span>[][] dp = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[n][<span class="hljs-number">2</span>];<br><br>    <span class="hljs-type">int</span> ans=<span class="hljs-number">0</span>;<br><br><br><br>    <span class="hljs-comment">// init: 第1天状态</span><br><br>    dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 卖出为0 因为不进行操作</span><br><br>    dp[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = -prices[<span class="hljs-number">0</span>]; <span class="hljs-comment">// 买入是 -price[0] 因为第一天买入</span><br><br><br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;n;i++)&#123;<br><br>        <span class="hljs-comment">// 第i天买入状态：i-1天买入状态 今天不操作 或者 i-1天卖出状态 今天买入</span><br><br>        dp[i][<span class="hljs-number">1</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] - prices[i]); <br><br>        <span class="hljs-comment">// 第i天卖出状态 i-1卖出今天不操作 或者 在今天卖出</span><br><br>        dp[i][<span class="hljs-number">0</span>] = Math.max(dp[i-<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], dp[i][<span class="hljs-number">1</span>] + prices[i] - fee);<br><br>        <span class="hljs-comment">// 记录最大值</span><br><br>        ans = Math.max(dp[i][<span class="hljs-number">0</span>],ans);<br><br>    &#125;<br><br>    <span class="hljs-keyword">return</span> ans;<br><br>&#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="图-amp-树"><a href="#图-amp-树" class="headerlink" title="图 &amp; 树"></a>图 &amp; 树</h2><blockquote><p>树：<strong>连通非循环无向图</strong></p></blockquote><ul><li>搜索算法：BFS，DFS，<strong>拓扑排序</strong></li><li>单元最短路径：Dijkstra，Floyd （要求图中没有环）</li><li>最小生成树：Kruskal 算法</li></ul><h2 id="递归回溯"><a href="#递归回溯" class="headerlink" title="递归回溯"></a>递归回溯</h2><blockquote><p><a href="https://leetcode-cn.com/problems/subsets/solution/c-zong-jie-liao-hui-su-wen-ti-lei-xing-dai-ni-gao-/">力扣</a></p></blockquote><p><strong>DFS和回溯的区别</strong></p><p>DFS是搜索算法，在搜索过程结束之后返回到上一层不会再多执行操作。而回溯算法在返回到上一层之后，会再次进行搜索。</p><p>DFS和回溯最大的区别是：<strong>有无状态重置</strong>。</p><p><strong>何时使用回溯算法</strong></p><p>当问题需要“回头”来找出所有解，即满足结束条件或者发现不是正确路径之后，要撤销选择，回退到上一个状态，继续尝试，直到找出所有解。</p><p><strong>回溯算法的模板</strong></p><ul><li>画出递归树，找到状态变量（写出回溯函数的参数）</li></ul><p>例如，在子集问题中，给定一组不含重复元素的数组，返回所有可能的子集：</p><p><img src="/img/tech/Algorithm/backtrack.png" alt="img"></p><ul><li>根据题意，确立结束条件</li><li>找准选择列表</li><li>判断是否需要剪枝</li><li>做出选择，递归调用，进入下一层</li><li>撤销选择</li></ul><p><strong>回溯问题的类型</strong></p><blockquote><p>回溯问题通常需要找到一个<strong>序列</strong>而不是<strong>总数（种类数），</strong>如果要求总数的话，可以使用背包来优化时间复杂度。因为回溯的时间复杂度是$$O(2^n)$$</p></blockquote><table><thead><tr><th>类型</th><th>题目链接</th></tr></thead><tbody><tr><td>子集、组合</td><td>LC78 子集1 LC90 子集2</td></tr><tr><td>全排列</td><td>LC46 全排列1 Lc47 全排列2</td></tr><tr><td>搜索</td><td>LC 79 单词搜索 N皇后</td></tr></tbody></table><h2 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h2><blockquote><p>深度优先搜索，一种用于<strong>遍历搜索</strong>树或者图的算法，过程是对每一个可能的分支路径深入到不能深入为止，并且每个节点只能访问一次。</p></blockquote><blockquote><p>DFS的发明者在1986年获得图灵奖。</p></blockquote><p>比较经典的例题：</p><ul><li>LC 111 二叉树深度</li><li>LC 98 验证二叉搜索树</li><li>LC 113 路径总和</li></ul><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><blockquote><p>排序算法的任务简单，但是解决问题的思想非常经典。应用在快排、归并排序中的分治思想、递归实现在计算机的世界里有着广泛的应用。</p></blockquote><p><strong>注意：</strong></p><p>很多算法题中不会直接考察排序算法，而是考察排序算法中的思想，比如：快排的Partition操作，计算逆序对和第K大元素等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Leetcode</tag>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>spanner</title>
    <link href="/2022/05/31/spanner/"/>
    <url>/2022/05/31/spanner/</url>
    
    <content type="html"><![CDATA[<p>Spanner: Google’s Globally-Distributed Database<br>前言：个人感觉这篇paper组织的逻辑比 Dynamo和GFS 难理解一些，因为时而讲存储引擎层的内容，时而讲存储Service层的内容。笔者按照自己的逻辑重新组织了一下文章，希望能够帮助读者更好的理解Spanner。</p><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnALxkfZlATXInaIF2BcyMUf">Spanner</a>.</p><p>特征</p><ul><li>可扩展的</li><li>基于多版本读 (MVCC) </li><li>基于两阶段锁的写 (2PL)</li><li>同步复制</li><li>提供 Externally Consistent<br>Spanner 简介<br>Spanner是一个基于Paxos的，支持数据分片的全球部署的强一致数据库。<br>即使在极端自然灾害下，Spanner能够对上层应用提供高可用的服务，因为Spanner的数据是跨大陆存储的。F1 Google 是Spanner最初的用户，它为实现HA在美国部署了5个分片。<br>大多数应用对于HA的要求基本是能够容忍1-2个DC断电，根据Paxos算法 (N &#x3D; 2F + 1)，Spanner只需要提供 3-5 个服务节点就可以满足Fault Tolerance的需求。</li></ul><p>Spanner 主要的关注点在于跨机房的数据同步。</p><ul><li>对比Bigtable，Spanner能够提供DB级别更好的支持：<ul><li>支持更加复杂的数据Schema；</li><li>在全球化部署的同时保证强一致性；</li></ul></li><li>对比MegaStore，Spanner提供更为高级的支持有：<ul><li>在支持同步复制的基础之上，提供更高的读写吞吐；<br>此外，Spanner还能为数据提供多版本的读写，每个版本通过一个TimeStamp来标识。旧版本的数据可以通过配置化的GC来回收，释放存储空间。Spanner提供的是类SQL接口，并且支持ACID的事务操作。<br>Spanner作为一个全球部署的数据库，其实考虑了边缘计算的概念，比如：用户可以访问最近的副本来读取数据（数据是强一致的，所以总是能读到最新的数据）；并且能够支持配置每一个DC存储的数据，数据距离用户的距离（提升读速率），多副本之间的距离（提升写速率），以及维护多少个数据副本。<br>Spanner作为一个分布式的数据库，提供了两个非常宝贵的功能：</li></ul></li><li>支持 Externally Consistency (我认为可以理解为 Linearizable)；</li><li>支持全球的用户同一个时刻读到一致的数据。<br>对于如此高级别的一致性提供保障最重要的一个功能是 TimeAPI，Google通过原子钟和卫星授时服务保证每个节点都能获得一个 &lt;10ms 的时间戳，这个时间区间成为 Clock Uncertainty ，服务节点在CU内是需要阻塞请求的。<br>Spanner 架构简介</li><li>Zone：Spanner 是通过Zones来进行管理的，每一个Zone是最小的管理单元，也是数据复制的范围（Replicas只能分布在同一个Zone中）</li><li>ZoneMaster：主要负责与Server交互，每一个Zone会有一个ZoneMaster主管数据的分配以及副本管理。</li><li>Location Proxy：主要负责与Client交互，告知Client SpannerServer的信息。</li><li>UniverseMaster 我理解主要是一个控制台和可视化界面。</li><li>Placement Driver 主要负责数据的迁移和心跳检测。<br>ZoneMaster + Location Proxy  + Placement Driver 几乎等于GFS中Master Server的功能。</li></ul><p>Storage Arch<br>首先每一个Replica中存储的数据是形如：<br>(key:string, timestamp:string) -&gt; value:string</p><p>（存储层）支持的数据结构比较单一，只有String类型；每一个Replica中都存放了成百上千个tablet，每一个tablet中又存放了大量的KV键值对。Tablet是在分布式文件系统Colossus持久化的，通过一种类似于B树的文件形式存储，使用WAL来保证故障恢复。<br>（业务层）每一个Replica都是运行在一个Paxos Server上的，保存相同数据的Replica构成一个Replicas Group，Replica Group会选择一个Leader对外提供一致性的读写服务。<br>对于Leader的任期，在Spanner的版本中会给Leader发一个默认10s 的Lease，在这10s内都会以持有Lease的服务节点为Leader，用于防止Split Brain。<br>Leader上会额外存储两个数据结构：</p><ul><li>Lock Table：用来表示Key Range的上锁状态。2PL：因为Spanner和Bigtable一样都需要满足对于长事务的支持，长事务如果使用MVCC这样的乐观并发控制性能会比较差，因为需要解决MultiVersion的冲突，导致事务不断地 Abort - Retry。所以基于悲观控制的方法来做长事务是一个Trade-Off的选择。</li><li>Transaction Manager：用于协调分布式事务。2PC：直接充当了2PC中的Coordinator，用来发布Prepare 和 Commit 的命令。</li></ul><p>Replication<br>Spanner 数据模型的概念比较 多 &#x2F; 复杂，需要仔细分析。<br>我理解在Spanner 中， Directory 相当于是分布式系统中的 Partition，也等于是数据最小的移动单位。一个Directory 是一系列拥有共同前缀的 Key组成的。<br>Spanner 一个Paxos Group管理一个Tablet，一个Tablet是许多Directory的组合（为了利用空间局部性原理，将经常被同时访问的Key 在物理上就近存储）。<br>从应用层的视角来看，Dir是数据存放的最小单位，APP在写入数据的时候可以通过 Tag的方式指定Dir，这样就能将数据写入到不同的Region中。<br>在Dir内部，还有一个更细粒度的数据存储单元：Fragment，当Dir数据过大的时候，会被拆分为不同的Fragment，不同的Fragment由不同的Paxos Group存储。</p><p>感觉这块内容描述得过于细致了，理解起来有点困难。。</p><p>Data Model<br>Spanner 暴露的接口为：半关系型的数据模型，类SQL接口，事务。Spanner对于事务和半关系型数据模型的支持很大程度是受MegaStore启发的，因为在谷歌内部，APP通常在MegaStore 和 BigTable选型的时候，会因为MegaStore对于跨机房同步的支持是同步，强一致的，而选择MegaStore；即使MegaStore的性能会因为一致性而受到削弱。<br>在对于事务的支持上：Spanner 的作者认为，即使2PC带来的性能损失比较大，但是不能总是让业务在无事务的存储层上编码，建议是业务层应该通过一些方式规避2PC带来的性能瓶颈。BigTable就是因为对cross-row的事务支持不够完善而饱受诟病，Google 内部为了解决BigTable对于事务的缺失，专门开发了一套叫做 Perlocate 的系统。（据笔者了解，PingCAP很多产品 (TiDB &#x2F; TiKV) 仍然使用Perlocate的方案来支持事务。）<br>Spanner暴露给用户的的存储结构是类似传统关系型数据库的，row, columns, SQL-Like Query 等等，但底层的存储却是一种KV的模型：通过每一个Row的PrimaryKey映射到其他Columns。<br>$$(PrimaryKey -&gt; (V1, V2, V3 …))$$<br>并且PrimaryKey是按照某种规则组成的自增唯一Key （索引）。<br>以下为一个用户存储的照片元信息对应的例子，从图中可以观察到 Spanner的接口语言，存储结构。</p><p>TrueTime<br>时间是一个很特殊的物理量，我们通常看到的时间只是时间的观察值，而不是时间的绝对值。</p><p>三个API<br>TT.now() -&gt; int[]: TTInterval: [earliest, latest]<br>TT.after(t) -&gt; bool: true if t has definitely passed<br>TT.before(t) _&gt; bool: true if t has definitely not arrived</p><p>这里需要强调一下的是，在TrueTime这里的授时不是一个确定的时间（与我们通常从电脑&#x2F;手机&#x2F;手表上看到的时间不同）。TT返回的是一个不确定的时间范围。<br>对于一个写事务 w 来说，它提交的时间 无法复制加载中的内容满足：无法复制加载中的内容。</p><p>时间的来源<br>TT 的时间来源于两个方面，GPS和原子钟</p><ul><li>GPS：主要受限于信号传递有可能会失败的影响；</li><li>原子钟：原子震荡可能产生误差，使其与真实的时间相差较大。<br>TrueTime 架构<br>无法复制加载中的内容<br>整体的TrueTime 架构如图所示，每个DataCenter中包含一个TimeMaster 集群用于给Spanner Server提供时间。每个Spanner Server会启动一个Daemon 进程，每隔30s向TimerMaster发起时间 Poll，用于更新本地最新的时间。TimerMaster内部会通过一些补偿机制来调整自己的时间，使得各个机器的时间误差尽可能小，在此不赘述了，感兴趣的话可以参考Google Spanner 白皮书。<br>并发控制 &#x2F;&#x2F; Concurrency Control<br>Spanner为了支持分布式事务，对于不同的事务类型：Read-Write &#x2F; Read-Only 采用了不同的策略；为了实现强一致，在事务隔离级别的基础之上加入了绝对时间，将事务执行的效率大大提升。<br>不得不引出一段经典名言：<br>We believe it is better to have application programers deal with performance problems due to overuseof transactions as bottleneck arise, rather than always coding around the lack of transactions.</li></ul><p>Read Write Transaction</p><ul><li>2PL </li><li>2PC</li><li>Timestamp: Commit Time<br>Read Only Transaction</li><li>Snapshot + TrueTime</li><li>Timestamp: Start Time<br>总结：分布式系统时钟授时方案<br>分布式系统中，主要有两种授时的方式：</li><li>通过网络授时，不同节点之间可能会存在网络延迟；</li><li>通过石英震荡产生时间，但石英震荡会产生较大的误差；<br>所以想要通过物理时钟来构建一个全序（以时间为比较单位）的分布式节点集合是不够的。基于此，谷歌为业界提供了几种解决思路:<br>Spanner: True Time方案，通过给每一个分布式节点配备一个GPS时间校准和原子钟硬件，来对接国际标准时间，并且通过卫星通信的方式进行授时，以及通过集群中的原子钟来提供双保险。<br>集中授时服务TSO：通过集群内一个发号器获取自增长的逻辑时间，为了避免单点故障，这个发号器通常使用Paxos来构成一个Group互相备份。(TiDB 使用的方案)<br>混合逻辑时钟HLC：可以保证同一个进程内部事件的时钟顺序，但是解决不了系统外事件发生的逻辑前后顺序与物理时间前后顺序的一致性，因此做不到: Linearizability and external consistency.</li></ul><p>Reference</p><ul><li>Google F1</li><li>MegaStore</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce</title>
    <link href="/2022/05/22/MapReduce/"/>
    <url>/2022/05/22/MapReduce/</url>
    
    <content type="html"><![CDATA[<p>MapReduce<br>Abstract: A high-level idea of Map-Reduce</p><ul><li>Map<br>用户使用Map函数计算出一系列的中间 K-V pairs</li><li>Reduce<br>Reduce 函数在对所有的中间K-V Pairs进行聚合（Merge）操作<br>假设给我十个数组，让我找出每个数组的第五个元素，那么Map函数的作用就是找到每一个数组的第五个元素，Reduce函数就是将每个数组的第五个元素进行聚合，得到一个新的数组。</li></ul><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnOSf3ldikYI6JOgdn5B6Gac">MapReduce</a></p><p>Introduction<br>Google对于处理大数据做了上百种special-purpose 的优化，这些计算方法用来处理大量的原始数据，比如：文档爬虫，Web日志等；也用来计算各种类型的衍生数据，包括倒排索引，每天最经常出现的请求等等。<br>如果在可接受的时间内完成运算，如何处理并行计算，如何分发数据，如何处理错误…都对系统设计提出了很大的挑战。<br>Google的工程师在函数式编程的Map &amp; Reduce函数中获得了启发，并且将其应用到了大数据计算中，使得大量的计算能够并行而且通过重试来处理fault。<br>Outline</p><ul><li><p>Sec.2: 描述一些基本的编程模型 </p></li><li><p>Sec.3: MapReduce的基本接口实现</p></li><li><p>Sec.4: 优化</p></li><li><p>Sec.5: 实验</p></li><li><p>Sec.6: MapReduce在Google中的应用</p></li><li><p>Sec.7: 未来的发展方向<br>Programing Model<br>Example：统计文件中单词出现的数量</p></li><li><p>Map<br>The map function emits each word plus an associated count of occurrences, 1 in this example.<br>map(string key, string value):<br>  &#x2F;&#x2F; key: document name<br>  &#x2F;&#x2F; value: document content<br>  for word in value:<br>  EmitIntermediate(word, “1”)</p></li><li><p>Reduce<br>The reduce function sums all counts emitted for a particular word.<br>reduce(string key, Iterator values):<br>  &#x2F;&#x2F; key: a word<br>  &#x2F;&#x2F; values: a list of counts<br>  for v in values:<br>  result +&#x3D; ParseInt(v)<br>  Emit(AsString(result))</p></li></ul><p>在概念上用户定义的Map和Reduce函数都有相关联的类型：<br>map(k1,v1) -&gt;list(k2,v2)<br>reduce(k2,list(v2)) -&gt;list(v2)</p><p>Other Example</p><ul><li>Distributed Grep: Map函数将emit一行如果改行能够匹配用户定义的Pattern，Reduce函数将数据拷贝到输出Buffer中。</li><li>Count URLAccess Frequency：类似单词统计，每次遇到URL，Map函数都会emit 1，Reduce函数将结果聚合得到 &lt;URL, total_count&gt;。</li><li>Inverted Index (倒排索引)：Map函数emit (word, document ID), Reduce 函数对word进行聚合，得到 (word, list<Document ID>)<br>Implementation<br>Map-Reduce 整体工作流程</li></ul><p>用户能够决定将Input File分割成多个数据片段分发到不同的机器上进行处理，使用分区函数将Map产生的中间函数分布到不同的分区上进行执行，Reduce的调用也被分布到多台机器上执行，用户可以自定义分区函数和分区数量。<br>任务执行过程</p><ol><li>用户程序将输入文件划分为M块，通常每块的大小为16MB-64MB，然后在Cluster上创建大量的程序副本。（注意是程序副本而不是数据副本）</li><li>其中一个程序是特殊的，作为Master对其他所有的副本程序 (Worker) 发起控制请求；Master总共有M个Map任务和R个Reduce任务将一一分配给空闲的Worker。</li><li>Worker被分配任务后会读取输入文件对应的 split，并执行Map函数，中间结果被保留在内存缓冲区中。</li><li>Worker会将中间结果周期性的写入磁盘，并且按照Partition函数划分为R个区域，并且将文件在磁盘上的位置返回给Master。</li><li>当Reduce Worker收到来自Master的处理请求后，会向Master发来的Location发起RPC调用，读取磁盘的数据。在收到磁盘数据之后，会先对key进行排序，如果内存不足需要使用外部排序。需要排序是因为不同的key会被match到相同的Reduce任务上。</li><li>Reduce Worker会遍历排序后的Key，并将这个Key和它对应的中间值传给Reduce函数，Reduce函数的输出将被追加到所属分区的输出文件。</li><li>当所有的Map&amp;Reduce任务完成后，Master唤醒用户程序，并将结果返回给用户程序。<br>通常在MapReduce任务结束之后，会产生R个文件（每个Reduce Worker产生一个输出文件），通常用户不需要合并这R个文件，而是将他们传给另一个MapReduce函数或者分布式应用对文件进行后处理。</li></ol><p>Fault Tolerance<br>Worker Failure</p><ul><li>Master会向Worker定期发送心跳，如果判定Worker Failure：<ul><li>Complete Map Task or In-Progress Map Task 会被设置为IDLE状态， 并且重新执行。Complete Map Task也要重新执行，因为Worker Failure之后，其磁盘的数据是不可访问的。</li><li>Complete Reduce Task不需要重新执行，因为Reduce的结果是直接写入Output File的，Output File存储在GFS上<br>Master Failure</li></ul></li><li>Master会定期将metadata 进行checkpoint 保存到磁盘上，recover的时候加载最新的checkpoint恢复数据。</li><li>Master如果Failure 会将任务Abort，由Client进行重试。<br>Semantics in the Presence of Failure</li><li>Map 和 Reduce都是deterministic的函数（一致性），他们的输出理论上都是确定的，不论发生任何错误。<br>使用原子提交 (Atomic Commit) 来保证一致性：</li><li>Map：每个Map任务执行完成后，生成R个临时文件，并将临时文件的地址封装为Message发送给Master，如果Master判断这个Message已经收到过，则直接忽略它。</li><li>Reduce：Reduce的原子提交是依赖操作系统的重命名命令实现的 (mv in Linux), 一个Reduce任务执行完成后，会将临时文件重命名为输出文件名。如果同一个Task在多台Reduce Worker上执行，最终只有一个Worker的文件能够被保留。<br>空间局部性<br>在MapReduce提出的年代，网络带宽会成为系统的bottleneck。所以Master会考虑从GFS中寻找距离最近的Replica以减少网络的带宽使用。如果任务失败了，Master会优先在保存有Input File (Split) 数据拷贝的机器上执行Map任务。这样当执行大规模MapReduce任务的时候，大多数的Input data都是从本地读取的。(Worker 会缓存Data)，Master作为GFS的Client，不会缓存Data[1]。<br>备用任务<br>为了防止“木桶效应”，一个Worker的慢执行，导致整个MapReduce任务执行变慢，Master会在一个任务马上完成的时候，调度备用Worker来执行剩下的处于in-progress中的任务。不论是backup Worker还是初始Worker完成了任务，Master都会认可这个任务被完成。<br>应用场景</li><li>分布式Grep</li><li>统计URL访问频率</li><li>构建倒排索引</li><li>分布式排序：分布式MergeSort<br>Related System</li><li>Distributed Storage：GFS</li><li>Batch Processing: Spark</li><li>Stream Processing: Flink</li><li>High Availability: Chubby<br>总结<br>MapReduce的成功归因于以下三点：<br>模型使用起来非常简单，因为开放的API接口隐藏了内部的故障处理和并行机制；<br>大规模的应用场景能够使用MapReduce来处理，比如：机器学习，数据挖掘，搜索推荐；<br>谷歌的工程师们提供了一套完整的实现，使其能够应用于大规模的机器集群上。</li></ul><p>经验<br>编程模型的约束使得任务的并行和计算，故障容错变得更加容易，在MapReduce中用户只需要定义Map函数，Reduce函数和一些超参就能完成对任务的执行。<br>网络带宽是宝贵资源，（现在都光纤了，应该不存在这个问题了吧）。<br>冗余的处理能够一定程度上避免木桶效应。</p><p>局限<br>历史局限，当时的背景下单机的性能远远不足，所以没有考虑使用内存进行更高效的计算。<br>没有将资源调度和计算调度分离。在后续的Hadoop生态中，MapReduce只关注与计算而资源调度由Yarn进行管理。</p><p>Reference<br>[1] Google File System<br>[2] <a href="https://research.google.com/archive/mapreduce-osdi04.pdf">https://research.google.com/archive/mapreduce-osdi04.pdf</a><br>[3]<br><a href="https://tanxinyu.work/mapreduce-thesis/">https://tanxinyu.work/mapreduce-thesis/</a><br>[4] 正在实现的：<br><a href="https://github.com/GaryGky/MIT-6.824/tree/lab1-mr/src/mr">https://github.com/GaryGky/MIT-6.824/tree/lab1-mr/src/mr</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Google</tag>
      
      <tag>Distribtued System</tag>
      
      <tag>Distributed Computing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Twitter 缓存应用分析</title>
    <link href="/2022/05/22/Twitter-%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90/"/>
    <url>/2022/05/22/Twitter-%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>Twitter 缓存应用分析</p><p>为了更好的阅读体验，请移步至 <a href="https://lo845xqmx7.feishu.cn/docs/doccn329gaovix2csddT3FXFzvb">飞书文档</a></p><p>Abstract</p><ul><li><p>TTL 缓存过期时间的设置十分重要。</p></li><li><p>FIFO 在workload非常大的时候工作得更好（至少和LRU性能差不多，但是理解起来会容易很多）。<br>1 Introduction<br>In Memory-Cache: </p></li><li><p>Redis</p></li><li><p>MemCache<br>将研究者的注意力吸引到：缓存命中率，吞吐量和减小延迟。目前的评估方法是存在缺陷的，大多数是基于：Storage Caching Trace，KV Database Benchmark。但是现在的评估方法都没有将Object Size考虑进去，然而Object Size是影响缓存命中率和吞吐量的重要因子。<br>本文基于100个Twitter缓存集群的模拟和分析，提出了以下几个重要观点：</p></li><li><p>In-memory 缓存并不总是服务于Read-Heavy的场景，有很多缓存也是服务于Write-Heavy的场景的。</p></li><li><p>TTL 会影响工作集的大小所以必须将TTL考虑到缓存的设置中，有效的清除缓存应该优先与缓存淘汰策略。</p></li><li><p>Slab-Based 的缓存比如：Memcache，会受到对象大小分布不均的影响；并且瞬时增大，或者周期性变化的workload，也会对缓存造成影响。</p></li><li><p>根据Zipfian（奇夫定律），访问量最大的key通常是访问量第二大的key的两倍，这在写吞吐较大的系统中会出现明显的倾斜。</p></li><li><p>在合理的缓存空间中，FIFO和LRU在数据量很大的情况下性能差异不明显，LRU在小数据集中的性能更好。<br>2 Twitter 体系中使用的In-Memory 缓存<br>2.1 服务架构和缓存<br>2011 年， Twitter开始了向微服务和容器化迁移的进程，其中缓存作为一项重要的基础架构，随着DRAM的容量扩大，也随之发展。<br>在Twitter中，主要有两种常用的业务缓存组件：Twemcache 和 Nightawk，前者是基于Memcache构建的缓存，具备低延迟、高吞吐的特点；后者是基于Redis构建的缓存，具有更丰富的数据结构，主备提供了高可用的服务。<br>2.2 Twemcache Overview<br>Twemcache fork了Memcache早期的版本，加入了Twitter自身的一些改进。</p></li><li><p>Slab-Based 内存管理<br>传统对于缓存对象的分配策略是使用 <code>heap memory allocators</code>，比如：ptmalloc，jemalloc；这些分配策略会产生“无限大”的外部碎片，而Twitter缓存对象的大小从 若干字节到 10s KB不等，传统分配策略不适用与在小容量容器中使用。<br>基于Slab的分配方式如上图所示，内存会被分为若干等级的Slab，每一个Slab内部又会将内存划分为大小相同的item；Slab的等级越高，表示每一个对象占用的内存越多，每一个缓存对象会被分配到大小最合适的Slab中。Slab-Based 内存分配策略防止了外部碎片，将内存碎片局限在一个item当中。</p></li><li><p>Slab-Based 淘汰策略<br>当Twemcache接收到一个新的缓存对象obj时，首先会查找对应的slab-class是否存在合适大小的块，如果存在的话会直接分配给obj，否则会创建一个更大的slab-class。当内存不足时，需要淘汰掉一些内存中的内容。<br>Memcache 的淘汰策略基本都是基于<code>item</code>这个粒度去做的，通常使用LRU算法对内存中的缓存对象进行淘汰。这么做在 Key Size稳定的情况下没有问题，但是如果 Key Size 随着使用不断增大，后来的大对象将没有位置存储，这种情形被称为：Slab-Calcification.<br>为了解决上述问题，Twemcache使用了一种基于Slab的内存淘汰策略，包括 Slab-Random, Slab-LRU, Slab-LRC。<br>2.3 Cache 使用场景<br>Twitter使用缓存的场景可以概括为三类：存储、计算和短暂的数据。</p></li></ul><p>2.3.1 Cache for Storage<br>最常见最常用的场景，因为后台的数据库延迟高、带宽小。<br>该领域一些重要的研究内容包括：提升缓存命中率，重新设计一个更密集的存储设备来适应更大的工作集，增强负载均衡，增加写吞吐。<br>如上图所示，该缓存集群在整个业务缓存中使用的比例虽然只有30%，但是涵盖了大多数的请求，占用了大多数的内存。<br>2.3.2 Cache for Computation<br>实时流计算使用较多<br>包括一些深度学习框架中也十分常见（PyTorch &#x2F; TensorFlow）<br>跟机器学习系统相关的场景接触的可能会比较多，该场景主要是缓存一些模型计算的中间变量，包括：特征(Feature), 和预测结果(Prediction)。<br>2.3.3 Cache for Transient Data<br>用来存储一些瞬态的数据，不落库的数据。<br>在该场景下可能会有一些数据丢失，但是Twitter的开发人员在这种高速访问和data loss做过trade-off，保留了该场景的使用。<br>一些典型的例子：</p><ul><li>Rate Limiter: 用来记录某个用户访问请求的次数，通常用于防止DoS攻击；<code>(UserID, Cap)</code></li><li>Deduplication Cache：用于去重。<code>(Key, Cap=1)</code><br>3 评估方法<br>收集分析日志<br>Twemcache 每一个集群都有一个<code>built-in</code> 的非阻塞日志系统：klog. klog的默认设置是每一百条请求生成一个日志，为了保证分析的覆盖性：研究者手动将采样率调整为100%，每一个集群中采样两个缓存实例进行分析。<br>作者从153个Cluster中的306个实例收集了 80TB的日志数据，每一个Cluster的QPS为1000。为了方便分析，作者筛选了54个流量最高的集群，他们处理了90%的QPS以及76%的内存。<br>3.1 缓存命中率 or Miss Ratio</li></ul><p>Miss Ratio<br>图a 展示了10个缓存缺失率最高的缓存服务，8 &#x2F; 10的缓存缺失率在5%以下，6 &#x2F; 10的缓存缺失率在1%以下。唯一的例外是最上面的点，它的缓存缺失率高达70%，它是一个write-heavy的缓存，相比其他的CDN缓存，明显缺失率要高很多。<br>Miss Ratio Stability<br>除了缓存缺失率，缓存缺失率的稳定性也十分重要，因为对于一个后端系统来说，往往是最高的缓存缺失率决定了系统需要提供的QPS能力，缓存缺失率稳定性定义为：$$\frac{mr_{max}}{mr_{min}}$$。很多时候一个稳定性更高的缓存服务比一个不稳定的缓存更受青睐。从图b可以看到，缓存不稳定性越高的服务，它的缓存命中率更低，说明后端系统对于Corner case的考虑不足，导致系统偶尔会出现一些spike。<br>3.2 请求率和Hot-Keys</p><ul><li><p>蓝线：请求量；</p></li><li><p>红线：访问的object数量；<br>大多数时候对于spike的理解是hotkey导致的，但是系统并不总是这种情况，因为从右图可以看到在request出现spike的时候，object 访问量也会上升，说明不是因为热key引起。某些可能的原因是：客户端重试，网络拥塞，scan-like请求，周期性任务。<br>3.3 Operation on Cache<br>Write Heavy Cache：写操作超过30% 的workloads。</p></li><li><p>从图b可以看出，超过35%的缓存集群都是write-heavy的，并且有超过20% 的缓存集群的写操作超过50%。<br>一些write-heavy的场景：</p></li><li><p>频繁更新的数据：机器学习的场景，用缓存保存一些不需要持久化的数据，所有的计算和更新都是在缓存上操作的。</p></li><li><p>预加载的数据：记录用户行为的场景，很多服务都是懒加载的，但在用户行为分析的场景中，根据局部性原理，可能会将用户的信息一次性加载到缓存中。但这些数据又不是可复用的（一个用户看完之后就应该要删除了），所以这些预加载的数据会被频繁更新。</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>Twitter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VM-FT</title>
    <link href="/2022/05/22/VM-FT/"/>
    <url>/2022/05/22/VM-FT/</url>
    
    <content type="html"><![CDATA[<p>VM-FT<br>本文是由VMWare于2010 年提出的一个虚拟机容错解决方案，使用的是一个Primary-BackUp的架构。</p><p>推荐阅读：为了更好的阅读体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccny051ESMjndx9mYC7wpg0cc">VM-FT</a></p><p>前言<br>在听取了MIT6.824的课程以及阅读了VM-FT的paper之后，笔者个人感觉要让VM级别的应用实现Fault Tolerance的难度会比DB (In-Memory &#x2F; Disk) 级别的应用大不少，主要是由于VM级别的机器需要关注更底层信息的一些复制，包括中断，异常等。<br>本文介绍方法的大致思想是使BackUp保留一份Primary的状态，当Primary出现故障的时候BackUP能够平滑接替Primary的功能，并且是对用户是透明的。<br>为了实现这个目标，首先在论文Intro部分介绍了全量复制和增量复制两种方法：</p><ul><li>State Transfer：全量地将所有信息，包括（CPU，Disk，I&#x2F;O）的变化都复制给BackUP，这种brutal force的方法简单粗暴，但是对网络资源的开销太大；试想一个数据库更新一行需要将整个数据库在网络上传输一次，这显然是不现实的。</li><li>Replicated State Machine: 将服务器抽象为确定状态机，让Primary和BackUP接受相同的Client Request，这种方法实现复杂，但是对网络带宽的占用很小，适合长距离的传输。<br>架构设计<br>暂时无法在文档外展示此内容<br>Component</li><li>VMM 是该文章复制的Node主体，他是建立在Hypervisor上的一个应用，运行在虚拟机中的操作系统是GuestOS，主机上的操作系统是HostOS。</li><li>Primary 作为一个逻辑概念，标识一台VMM节点，是Client唯一能够感知到的机器。所有的网络输入或者其他输入（磁盘、鼠标、键盘）都会进入Primary VM</li><li>Primary VM接收的所有输入都通过logging channel转发到BackUp进行同步，以保证两者的状态相同。Backup VM指令的结果与Primary相同，但是backup的返回结果会被hypervisor丢弃。<br>Deterministic Replay<br>对于一些不确定的指令或操作：</li><li>随机数生成指令</li><li><code>Time.Now</code>这样的获取时间戳指令</li><li>时钟中断指令</li><li>多核CPU之间的并发执行指令<br>对于不确定的操作，VM-FT中会添加一些额外的信息来保证BackUP对于这些操作的执行结果是与Primary一致的。VM-FT中会添加不确定的指令的执行结果，在backup中replay这些指令的结果。<br>一个logEntry大概需要包含以下内容：<br>Interupt Number &#x2F;&#x2F; 事件发生时的指令号<br>Type &#x2F;&#x2F; 事件类型：网络输入 &#x2F; 其他输入<br>Data &#x2F;&#x2F; 数据，对于不确定指令，此数据应该是Primary的执行结果</li></ul><p>Bressoud &amp; Schneider 提出使用了batch update的方式在Primary和Backup之间进行同步，他们提出将执行序列划分成若干个epoch，然后每次Primary-backup之间同步一个epoch的内容，来减少信息传递的次数。<br>FM Protocol</p><ul><li>输出要求<br>当Primary failover之后，Backup要能够立即接管Primary的任务，并且保证自身的信息和Primary是强一致的。</li><li>输出规则<br>Primary 在发出同步信息之后会等待Backup返回ACK之后，才把Output返回给Client。</li></ul><p>VM-FT不能保证<code>Exactly Once</code>语义，因为Primary在返回给Client Output之后发生了故障，backup在takeover之后也向Client发送故障，此时Client有可能会收到两个重复的数据包。<br>paper中说这个重复的数据包是通过TCP可靠传输来保证不重复的，但是笔者认为在此基础上，Client应该保证自身的幂等。</p><p>VM-FT和GFS的 Fault Tolerance</p><ul><li>VM-FT备份的是计算，它对于一致性要求的实现会更加复杂一些，主要体现在对于不确定性指令操作的处理上。</li><li>GFS备份的是存储，所以GFS的备份策略会比VM-FT更加高效和简单。<br>Reference</li><li>Paper</li><li>MIT 6.824</li><li>NUS CS5223 Lab-2</li><li>Chapter 5 - Replication</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed System</tag>
      
      <tag>Distributed Storage</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Partition</title>
    <link href="/2022/05/22/Partition/"/>
    <url>/2022/05/22/Partition/</url>
    
    <content type="html"><![CDATA[<p>Partition的目标是用来提升读写的请求吞吐的，每一个Partition保留的整个数据集的一部分，针对不同数据的请求会被路由到不同的Partition中</p><ul><li>MongoDB，ES，SolrCloud中的Shard</li><li>HBase中的 Region</li><li>BigTable中的 Tablet</li><li>Cassandra和Riak中的 VNode</li><li>Couchbase中的 vBucket<br>1 Partition &amp; Replication<br>分区和复制通常是结合使用的，每一条记录落在特定的分区中，但是分区中有若干个Node形成一个Cluster，提升可用性 (Fault-Tolerance)。<br>Leader-Follower 架构的复制 加上 Partition 构成的整体架构如下图所示：<br>可以将每一个Partition理解为一个Database，Partition中的Replica可以分布在不同的Server Node上。</li></ul><p>推荐阅读：为了更好的阅读体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccn7ps0hMiJoJIw4DmRWn4HUd">Partition</a></p><p>2 Partitioning of KV Data<br>理想状态下：如果将一个完整的数据集分为10个子集，那么吞吐量理论上来说应该是划分前的十倍。如果划分的数据倾斜（Skew），在极端情况下，一台机器可能需要承担所有的数据请求（这种机器叫做HotSpot），此时读写瓶颈仍然收敛到单台机器的性能上，分区失去了意义。<br>最简单的分区策略是随机分配（Random），能够将数据均匀的分布到每一个Partition上，但是如果需要Read，则必须遍历所有的Partition，找到对应的Key之后才能读Value。事实上，有比随机分配更好的策略。<br>2.1 Partition By Key Range<br>将一段连续的Key划分到一个分区中，如果能够知道待查询key属于的范围，就能够确定key的分区；确定了key所在的分区之后，就可以直接向分区所在的Node发起请求。<br>直接按照字典序对Key进行分区的话，可能会造成数据倾斜，比如在下图的分区1中，包含的是a-b开头的key，而在分区12中包含的是t-z的key；假设a-b开头的单词大于t-z开头的单词，就会导致分区1中的数据量大于分区12中的数据量。因此，为了让数据更均匀的分布，需要让分区的边界能够根据Data进行自适应。<br>分区的边界设置通常由两种方法：</p><ul><li>DBA手动设置；</li><li>由DataBase自动设置（Reparation）；<br>应用实例：<br>包括：HBase, RethinkDB, MongoDB, BigTable。</li></ul><p>在每一个分区中，可以按序组织Key（SSTables、LSM-Trees），这样有利于范围查询。但是在某些场景下，存在一个弊端，比如：存储网络传感器数据，按照TimeStamp对Key进行排序，那么在追加写的时候，可能只会写当天的数据，那么保存当天数据的Partition就会成为HotSpot，而其他的Partition都是IDLE状态。<br>为了防止这种情况，可以考虑使用其他的Key作为“主键”，比如：按照传感器的ID进行分片，查询的时候，按照不同的传感器进行范围查询，然后聚合。<br>2.2 Partition By Key Hash</p><p>可以使用Hash函数对上面的String Key Range进行优化，使用Hash函数将一个String映射为一个无符号整型，然后将整数划分为不同的Partition。MongoDB，Cassandra 使用MD5作为哈希函数，Voldemort使用Fowler-Noll-Vo函数作为哈希函数。Java的hashcode和Rust的 Object#hash有一个问题：在不同的进程下返回的结果不同。<br>使用hash 做了散列之后，丢失了原本按照字典序划分的Partition的天然排序性，也即使用了Hash散列之后，Partition不再高效地支持范围查询了。在MongoDB中，所有的范围查询会被发送给所有Partition，Riak，CouchBase和Voldemort都不支持按照主键进行查询。<br>Cassandra中使用了一种复合主键 primary_key &#x3D; (key1, key2, key3)，第一项用于分片，其他几项用于索引来对数据进行排序（SSTable）。针对主键key1的范围查询是不支持的（因为分布在不同的shard上），但是如果指定了key1，就能定位到一个分片，然后根据其他列就能够支持范围查询。<br>2.3 Skewed Workloads &amp; Relieve Hot Spots<br>极端情况下的数据不平衡<br>实际中的场景：某一个大V的动作或者评论，可能引起很多粉丝去访问同一个ID（Action或者User），这个时候即使对ID做了Hash处理，但因为同一个ID的哈希值是一样的，所以粉丝们大量的请求还是会打到同一个Partition上。<br>解决：使用一些先验。<br>如果我们事先知道一些key可能会非常hot，我们可以在Key的前后添加一些随机值，只需要两个decimal<br>Random number就能让数据分布到100多个分片上。（将同一个Key的数据分布到不同的Partition上。）<br>3 Partitioning And Secondary Indexes<br>讲的主要是ES模型的分片原理<br>次级索引通常不是唯一定义一条元素的标识，DDIA 中指出次级索引是关系型数据库的bread and butter，许多KV数据库 (HBase, Voldemort) 没有实现次级索引，因为这会极大地增加实现复杂度。但有一些NoSQL如（Riak）开始使用次级索引，因为次级索引更有利于数据建模。次级索引的存在是为了支持更复杂的查询请求，它也是<code>Solr and ElasticSearch</code>引擎存在的意义。<br>针对次级索引的分片可以分为两种类型：</p><ul><li>基于document分片</li><li>基于term分片<br>3.1 Partitioning Secondary Key By Document<br>根据主键分片，次级索引在不同的Shard中是独立存在的，即每条次级索引只会关联到该Shard中的数据，而不会跨Shard关联。基于主键分片的次级索引的组织方式也称为：<code>Lcoal Index</code>。</li></ul><p>这种方式存在一个问题，如果仅根据颜色或者制造商进行查询，而不知道数据的主键，则需要对所有分片进行查询然后聚合，这种方式叫做: Scatter&#x2F;Gather（可以使用Map-Reduce）。<br>应用实例：<br>MongoDB, Riak, Cassandra, ElasticSearch, SolrCloud, VoltDB.<br>3.2 Partitioning Secondary Key By Term<br>按照次级索引进行分片，比如：color按照字典序划分：</p><ul><li><code>a-r</code>在第一个分区</li><li><code>s-z</code>在第二个分区<br>这种组织次级索引分片的方式叫做<code>term-based</code>，term这个名词来源于全文索引，其中的term是text中的每一个单词。term-based的方式更适合用于进行某些有实际意义的范围查询，因为主键通常是没有语义信息的，比如：想寻找价格在 (x, y) 范围内的车辆，如果希望流量在不同节点中分布的更加均匀，则可以使用Hash对term进行处理。</li></ul><p>Term-based 的组织方式有利于多读写少的场景，因为在读的时候，Client能够根据term定位到一个分片；然而在处理写请求的时候，可能需要涉及多个分片，这时候需要开启分布式事务，但大多数分布式数据库都不支持分布式事务。<br>在实际应用中，term-based 索引的更新通常都是异步地，如果对于一条索引的读请求距离上一次写请求很短，是有可能读到旧数据的。例如：Amazon-DynamoDB官方声明了他们global-index的更新会存在秒级延迟，如果基架出问题了则可能延迟更长的时间。</p><p>4 Rebalancing Partitions<br>Rebalancing (重分区 &#x2F; 重平衡)：指的是数据和请求从一个服务节点（Replicas）移动到另一个服务节点上的过程。</p><p>触发重分区的时机</p><ul><li>查询量上涨，所以需要更多的CPU来处理请求。</li><li>数据大小上涨，需要更多的Disk 和 RAM存储空间。</li><li>一个节点挂了，需要拉起另一个保存了数据副本的节点继续服务。<br>重平衡的目标<br>这三点基本在Implementation部分的设计文档中都实现了，负载均衡使用的是Hash，为了保证第三点，使用了多次平均算法。</li><li>保证负载均衡。假设重平衡之后出现了HotSpot，极端情况下，所有请求都往HotSpot打，HotSpot会被请求撑爆，然后进行重平衡，又出现HotSpot……一台机器一台机器被打挂，导致整个Cluster不可用。</li><li>在重平衡的过程中，仍然需要支持读写请求。</li><li>尽可能少的移动数据，使用多次平均算法来保证尽可能少的移动数据。<br>High-Level 思想是每次寻找包含Shard最多的节点Max和包含Shard最少的节点Min，然后从Max节点找一个Shard移动给Min节点。<br>while (true) {<br>  int source &#x3D; getGidWithMaximumShards(gID2ShardIDs);<br>  int target &#x3D; getGidWithMinimumShards(gID2ShardIDs);<br>  if (source !&#x3D; 0 &amp;&amp; gID2ShardIDs.get(source).size() - gID2ShardIDs.get(target).size() &lt;&#x3D; 1) {<br>  break;<br>  }<br>  Integer swapShard &#x3D; gID2ShardIDs.get(source).stream().findAny().orElse(null);<br>  gID2ShardIDs.get(source).remove(swapShard);<br>  gID2ShardIDs.get(target).add(swapShard);<br>}</li></ul><p>4.1 重平衡算法<br>4.1.1 Mod N<br>最容易想到的重平衡算法是使用Mod N算法，但是在这个场景下非常不推荐使用Mod N算法，因为Mod N算法在N改变的时候，大多数的Key都需要移动，带来的开销巨大。<br>所以一般都是将key的哈希值分配到一个范围中，将某一个范围分配给一个节点存储。</p><p>上图这种情况下，n从三增加到了四，需要移动两个节点，读者可以将k设置得更大，可以观察到这种 Mod N的算法，会在重平衡的时候移动很多数据。<br>对比之下，将key按照Hash值进行范围映射的方法，也是我们熟知的Consistency Hashing。</p><p>所以，经过以上分析，几乎不会有infra产品选择Mod N这种重平衡算法。<br>4.1.2 Fixed Partition Number<br>思路：在数据库创建的时候，就将数据库划分为N个Partition 并且之后不会再改变，当有新节点加入的时候，每一个老节点都会分配一定数量的Partition给它。当有一个节点failover的时候，它的Partition会被均匀的分配给仍然存活的Node。<br>应用案例：Riak, ElasticSearch, CouchBase, Voldemort都使用了这种重平衡方法。以及在我自己实现的KV Store中也是使用的这种方式，Partition的大小一开始就被设置为10，随着Server的进出，重新分配Partition 即可。</p><p>有什么问题？<br>Partition的数量不能设置的过大，也不能设置得过小。如果Partition数量设置得过小，未来随着数据增长，Partition也会随之膨胀。如果Partition数量设置得过大，那么ShardMaster 就需要维护更多的元数据信息，效率不是很高。<br>事实上，选择合适的PartitionNumber是一件十分困难的事情，因为数据量会根据业务的变化而变化（即：数据量是不固定的）。如果一个Partition承载了过多的数据，那么故障恢复和重平衡的成本将会很大（全量复制一个大Partition）。如果Partition设置得过小，有点不划算，类似用牛刀杀鸡的感觉。<br>4.1.3 Dynamic Partitioning<br>Fixed Partition Number对于使用Key Range (refer 2.1)进行分片的数据库是非常不友好的，因为某一个范围的Key只能被分配到特定的Partition上，在刚开始的时候0-100,000都被分配到Partition-1上，此时Partition-1满负载运行而其他Partition几乎没有流量。</p><p>思路：动态扩展和合并分区，当分区大小超过设定阈值的时候进行分裂，小于设定阈值的时候进行合并。（有点类似B树）。HBase 和 RethinkDB使用的是这种分区策略。<br>对于Fixed Partition Number出现的问题，HBase 和 MongoDB使用了一种叫做pre-splitting的策略对其进行划分，即将一部分key set分配到idle的机器上，但这种操作需要DBA对数据的分布有一定的先验。<br>4.1.4 Partition proportionally to Nodes<br>4.1.2的Partition 大小 正比于数据集大小<br>4.1.3的Partition 数量 正比于数据集的大小</p><p>Cassandra 和 Ketama 提出了另一种分区方式，让Partition的数量正比于Server Node的数量，即：每个Server Node上保存X个Partition，Server Node增加的时候Partition也增加，反之减少。<br>这个算法的神奇之处在于，如果Server Node不增加，那么Partition的大小会随着数据量的增加而增加，在DBA手动扩容之后，每个Partition的大小又会下降到平稳的水平。整体来看是结合了4.1.2和4.1.3的优势，并且能够使CPU和存储资源保持稳定的一种算法。<br>Cassandra中设定每个服务节点上拥有256个Partition，当一个新的服务节点加入后，会随机选择256个Partition进行分裂，然后由新节点管理分裂出来的Partition数据。这种方式要求Key 是按照Hash的方式进行分片的，整体的算法有点类似Consistency Hashing。<br>4.2 Automatic &#x2F; Manual Rebalancing<br>关于手动重平衡或是自动重平衡的问题，在我们系统中使用的是Manual 扩容的方式，可能考虑到重平衡是一项比较危险的操作，因为有可能一台HotSpot的Delay时间突然增加，此时其他节点将其判定为Dead并开启重平衡，这时候网络上会因为有更多的数据流动而变得更加拥塞。（TCP拥塞避免就是这么来的）。<br>所以笔者认为Rebalancing的操作是一定需要人工卡点的，即使系统自动判断应该进行重平衡，也应该加入人工校验的流程，比如：不能在直播的高峰期对Partition进行重平衡，否则整个系统的稳定性和可用性都会受到冲击。<br>4.3 Requesting Routing<br>问题是重平衡之后，Client怎么知道我的请求应该往哪个Socket上发呢？<br>服务发现：不仅是DB系统，基本上任何Software Application后台都需要服务发现的功能，以此来保证系统的高可用。基本上大厂都会有自己的服务发现系统，比如TikTok的Consul，阿里的Dubbo (? Not quite sure).</p><p>三种服务发现的方式：</p><ul><li>允许Client访问任一Node（Round Robin），如果路由到的节点能处理请求则直接处理，否则将Client的请求进行转发。</li><li>添加一层Routing Tier，来决定Client的请求应该发送给哪个Node</li><li>Client感知Partition信息，并且直接向对应的Node发起请求。</li></ul><p>问题是Node之间的一致性 -&gt; Distributed Consensus。</p><ul><li><p>一种方法是使用一个分布式协调服务来管理元数据信息，向Kafka，HBase，SolrCloud使用ZooKeeper。</p></li><li><p>另一种方式是在Cassandra 和 Riak中使用的gossip protocal，每次检测到集群状态变化的时候就进行广播，然后Client按照方式1进行请求。</p></li><li><p>CouchDB使用了方式2来进行请求路由，Routing Tier的实现组件为Moxi。<br>Conclusion<br>Partition 的目标是让Data和查询请求在不同的机器上更加均匀的分布，避免HotSpot的出现。这对系统设计提出的要求是：</p></li><li><p>选择合适的 Partition Schema：By Range, By Hashing. </p><ul><li>By Range:<br>当Key是按照某种顺序排序的时候，可以按照Key Range进行Partition，1-100映射到Partition-1,101-200 映射到Partition-2。但是By Range这种方式可能会产生HotSpot，假设高热主播都是注册较早的用户，他们都被划分在Partition-1中，当他们同时开启直播的时候，Partition-1 的请求量暴增，而其他的Server Node基本都处于Cold的状态，显然是不均衡的。</li><li>By Hashing:<br>先将Key进行一次Hash，将Hash值按照Range进行分区，这样做虽然破坏了有序性，但是能够使数据更加均匀的分布。<br>还可以使用以上两种分区方式并用的方式，按照某个Key Hashing进行分区，分区内按照另一个Key进行排序。<br>比如在我们Global Live的系统中，通常是按照User-ID进行分片，然后每个分区内，Primary-Key是有序的。</li></ul></li><li><p>Rebalancing Partition.</p><ul><li>Mod N：一般不用。将Key Hashing分配给固定的节点。</li><li>Fixed Partition Number：一开始设置好分片的大小，在使用过程中不可改变。</li><li>Dynamic Partitioning：当分区大小扩张当一定程度的时候进行分裂，当分区大小小于一定值的时候进行合并。</li><li>Partition Proportionally to Nodes: 每个Node管理固定数量的Partition。<br>Partition and Secondary Key<br>分区和次级索引的关系</li></ul></li><li><p>按照主键分区，次级索引伴随主键进行分片；适合写多的次级索引。</p></li><li><p>按照次级索引分区。适合读多的次级索引。<br>Implemention<br>笔者也按照NUS 5223的Syllabus实现了一个Sharded KV Store，能够支持分片在不同的Replicas之间移动，负载均衡算法使用的是普通Hash。设计文档可以参考：<br>Sharded Transactional KV 设计文档</p></li></ul><p>Some Question<br>看了一下Reference Title，抛出几个问题：<br>Java’s HashCode is not Safe for Distributed System. Why?<br>3% of Twitter’s Servers Dedicated to Justin Biber. Any Strategy to fix it?</p><p>Reference<br><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html#bp-general-nosql-design-concepts">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html#bp-general-nosql-design-concepts</a><br><a href="https://cassandra.apache.org/_/index.html">https://cassandra.apache.org/_/index.html</a><br>DDIA</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>DDIA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transaction</title>
    <link href="/2022/05/22/Transaction/"/>
    <url>/2022/05/22/Transaction/</url>
    
    <content type="html"><![CDATA[<p>Transaction<br>几乎所有的关系型数据库和部分NoSQL能够提供事务支持，早在1975年IBM提出的第一个SQL系统System-R中，介绍了事务的概念。虽然在过去的40年中，事务的实现方式改变了许多，但是其基本的思想与40年前的System-R几乎是一致的 ，在MySQL，PostgreSQL, Oracle, SQL Server中 事务的实现原理几乎是与System-R相同的。</p><p>推荐阅读：为了更好的体验，请移步至飞书 <a href="https://lo845xqmx7.feishu.cn/docs/doccnNYVPZVwOG8KKcUirv37ELM">Transaction</a></p><p>新型的NoSQL数据中，大多都摒弃了事务的概念，因为在支持Partitioning 和 Replication的同时要支持事务的话，会对开发者和系统都提出很大的挑战。</p><ul><li>Spanner 架构的NoSQL中保留了事务的概念，并且系统是强一致（Serializable）的，按照CAP理论，它的Availablity有时候会因为网络分区而收到影响，表现为延迟较高，甚至系统不可用。<ul><li>BigTable，Chubby，ZK，GFS</li></ul></li><li>Dynamo 架构的NoSQL 已经抛弃了事务的概念，而且系统是弱一致（最终一致性）的，它的可用性较高，但是在网络分区的场景下，不同节点之间的一致性会收到影响，可能需要在用户侧执行Merge来处理冲突。<br>Riak (Bitcask Engine), Cassandra, and Voldemort (VoltDB) use leaderless replication models inspired by Dynamo.<br>目前不支持ACID的数据库系统，有时会被称为BASE (Base Availability, Soft State, Eventually Consistency) ，这是一个相对于ACID的概念，告诉用户“我的系统不支持ACID，是BASE的”，然后具体怎么理解交给用户，反正我的系统不支持ACID。<br>Overview<br>从下图中能够看到，在分布式事务中需要讨论的几个问题：</li><li>事务隔离级别</li><li>不同隔离级别的实现</li><li>某个事务隔离级别下的性能分析</li></ul><p>应用系统可能会面临的挑战</p><ul><li>数据库的软件或者硬件可能会在任何时候挂机，甚至是在读写进行到一半的时候宕机；（挑战了分布式系统的可用性）</li><li>网络分区：分布式集群中的一些节点可能无法与另一些节点进行通信，甚至集群与外部的联系也有可能被切断。</li><li>不同Client Propose不同的事务，导致Race Condition，或者Client可能读到一些部分更新的数据，这类数据是无意义的。这种问题在介绍索引的时候讨论过，对于MySQL这类直接覆盖记录的写法，如果不做保障，有可能更新到一半数据库突然挂机，这时候某一条记录中可能有一部分是新数据一部分是旧数据。但是对于Riak，VoltDB这样的数据库，都是采用Append的方式写入，后台使用一个守护进程定期对数据进行合并压缩，就不会出现部分更新的问题。<br>对于ACID一些有趣的解读</li><li>Atomic is not about Conccurency. Perhaps abortability would have been a better term than atomicity.</li><li>The word consistency is terribly overloaded: (Java 中overload表示重载)<ul><li>Replica Consistency and Eventually Consistency in async replicated system (Replication).</li><li>Consistent Hashing is a strategy used for rebalancing. (Partition)</li><li>In CAP theorum, C generally means linearizability.</li><li>In the context of ACID, C means “good state” in database system.</li></ul></li><li>ACID中的一致性其实是应用层的概念，而不是数据库层的概念。只有Client的应用能够定义什么是一致的什么是不一致的，而数据库只负责存储应用发过来的请求，并且只会进行相对较弱的校验（如：唯一索引校验，外键校验等）。所以ACID中，只有AID是在数据库层面保证的，而C是在应用层面保证的。<br>Transaction Isolation<br>Read Uncommit</li><li>脏读：读到一些基本不会存在的情况，比如：事务写了之后，但是回滚了。</li><li>脏写：两个事务同时写一个字段，导致partial update 的情况出现。<br>Read Commit<br>实现：通常通过行级锁实现，一个事物想要读&#x2F;写一个字段的时候需要先获得这条record的锁。<br>问题：一个很长的事务会阻塞大量的读请求，导致读请求堆积。<br>优化：数据库保留数据被加锁前的value，读请求在事务对record上锁的过程中可以读到old value。<br>SnapShot Isolation &amp; Repeatable Read<br>RC存在的问题是NonRepeatable Read 和 Read Skew<br>问题<br>场景一：可以容忍Read Skew，Alice看到自己两个account的钱总和不一致了，于是她刷新了一下界面，发现钱一致了，那么不会引起panic。</li></ul><p>场景二：BackUp如果读到了Master的old value，并且保存了Master的value，就会导致在应用副本的时候一直读取到旧数据，特别是如果Master挂了，BackUp take over之后，错误的数据会一直被使用下去。<br>场景三：OLAP系统中如果出现了不可重复读，会使数据分析产生偏差。<br>实现<br>SnapShot：Naive的思想是保证数据库在任何时候都能读到Consistent的数据，这样就不会产生Read Skew；在开启事务的时候打一个快照，之后所有的操作都在这个快照上进行。<br>MVCC的实现：每一行数据都有一个 <code>create_by</code> 和<code>delete_by </code>字段表示创建行的记录事务和删除行记录的事务。数据库通常都会有一个后台进程会不断地对产生的版本进行merge和GC的操作。</p><p>与SnapShot被一起定义的还有可见性原则，包括：</p><ul><li>在每一个事务开始的时候，数据库会标记此时所有正在执行的事务，所有正在执行事务的更新会被忽略。</li><li>任何被已回滚的事务提交的更新都会被忽略。</li><li>任何被大于当前事务ID的更新都会被忽略。</li><li>所有其他的更新都是立即可见的。<br>How is the index involved in MVCC?<br>理论上来说，可以让一条索引指向所有版本中的记录，当某条记录被删除之后，相应的索引项也会被删除。<br>在实际应用中，不同的数据库产品可能会采取不同的策略：</li><li>PostGreSQL中，禁止对某个多版本的object进行索引更新。</li><li>CouchDB中使用appendONLY + Copyonwrite的方式进行索引更新，由于这里数据库索引的底层使用的是B+树，所以从当前节点到根节点的page都会被copy一份。<br>More about Repeatable Read<br>Nobody knows the exact meaning of RR.<br>因为在System R诞生的时候，SnapShot Isolaiton还没被发明出来，而初始的数据库系统对于可重复读的定义特别模糊，导致后来的数据库产品对RR都有自己的定义。Oracle 把Snapshot 定义为可序列化级别，而MySQL将其定义为RR级别，但不管怎么定义，他们实现的功能都是能够保证一个事务内的读操作是一致的。<br>Dirty&amp; Write &amp; Lost Update<br>丢失更新个人理解是一种 <code>dirty write</code>的情况，一般来说，并发情况下，后一个执行的事务语句会覆盖掉前一个事务语句的更新。<br>可能会出现的场景：</li><li>对于用户充值和转账的并发情况，如果充值和转账出现了并发修改一个账户的情况，如果其中一个事务覆盖了另一个事务的执行结果，假设充值100，转账入账100元，用户的账户正常情况下应该是200元，但由于丢失更新少了100元，显然是不能接受的。<br>解决方案大体可以分为以下几种：</li><li>显式上锁（Manually）：<br>begin transaction;<br>select * from account where user_id &#x3D; xxx for update;<br>update amount where user_id &#x3D; xxx;<br>commit;</li></ul><p>显示上锁的问题是要求在业务层手动编码上锁：<code>for update</code>，如果程序员忘记对事务上锁的话，可能会引起丢失更新的问题。</p><ul><li><p>借助SnapShot (Automatically)<br>利用DB层提供的MVCC实现对丢失更新的自动检测，一旦事务检测到两个Write Version可能会引起冲突的时候，数据库会自动让后发的事务回滚。这样做的数据库实例有：PostgreSQL。<br>但是MySQL的InnoDB引擎并没有提供这种MVCC的功能，MySQL的RR级别是通过Gap-Lock来实现的。</p></li><li><p>CAS</p><ul><li>如果account_score 等于期望的old_value，那么事务可以更新account_score的值；</li><li>如果account_score 不等于期望的old_value，那么事务的更新操作是有可能失败的；<br>问题在于CAS的比较如果基于MVCC，那么数据库基于old-version返回给事务的结果是允许更新，但实际上new_version 的值是不等于old_value的。这样就会出现竞争情况，所以在使用CAS之前需要了解数据库在CAS条件下使用的隔离级别以及隔离级别的实现方式。<br>update account_score set account_score &#x3D; new_int<br>where user_id&#x3D;1 and account_score &#x3D; old_int</li></ul></li><li><p>Conflict Resolution &amp; Replication<br>Lock和CAS都是假设数据库在单机上工作的场景，但是对于分布式数据库，同一份data可能在不同的node上都有副本，并且副本的更新可能是并发或者异步的 (LeaderLess &#x2F; Multi-Leader)，这时候Lock和CAS可能没有办法很好的work。<br>对于Multi-Partition之间如何保证Atmoic Write，在Chapter 5 - Replication中提及过一些解决冲突的方式。即：DB允许在多副本上执行的顺序是不一致的，但是会通过一些手段来保证最终一致性，比如LWW (Last Write Win)，但LWW不能防止Lost Update。Riak2.0提供了多副本之间自动解决冲突并且能够防止Lost Update的方法。<br>总结：</p></li><li><p>Dirty Write &amp; Lost Update 都是为了解决对一个Key 的冲突，不涉及多个Key的竞争问题。<br>Serializable<br>Write Skew &amp; Phantom<br>Write Skew是MVCC无法解决的问题。<br>场景<br>必须保证至少有一个人在Oncall岗位上，Oncaller是可以短暂离开的，但是不论怎么样，必须保证一人oncall。<br>问题：A 和 B同时申请Leave，此时数据库根据当前值班人员有两人同时允许AB Leave，最终导致没人Oncall。—— Write Skew</p></li></ul><p>Write Skew 可以理解为：读同一条记录，更新不同的记录。在RR级别是无法防止Skew Write的，通常来说会采用以下两种方式解决上面的Write Skew问题：</p><ul><li>将隔离级别设置为Serializable，因为上面 Write Skew发生的场景是DB级别的事务同时被触发，但Serializable能够使事务串行执行，有了先后顺序之后，上面的Write Skew的问题可以得到解决。</li><li>如果无法使用Serializable这个隔离级别的话，需要手动上锁：<br>begin transaction;</li></ul><p>select * from doctors where on_call&#x3D;true and shift_id&#x3D;1234 FOR UPDATE;<br>update doctors set on_call&#x3D;false where name&#x3D;”Alice” and shift_id &#x3D; 1234</p><p>Commit;</p><p>其他场景分析</p><ul><li>预定场景：预定会议室：在预定之前先查询是否存在冲突的时间，如果不存在则插入一条记录，表示预约了该会议室。<br>begin transaction;</li></ul><p>select Count(1) from bookings where room_id&#x3D;1<br> and end_time&gt;&#x3D;’2022-05-01 12:00’ and start_time&lt;&#x3D;’2022-05-01 13:00’;</p><p>insert into bookings (room_id, start_time, end_time, user_id)<br>values (1, ‘2022-05-01 12:00’, ‘2022-05-01 13:00’, ‘A’ )</p><p>commit;</p><ul><li>消费场景：防止Double-Spending：在用户消费的时候，先check用户的account中是否有足够的coins，然后加入一条Transaction记录。但是在并发的情况下，可能会插入多条Transaction记录，这样会将用户的account扣成负数，并且多送出了一个礼物，公司赔钱。</li><li>注册场景：不能同一个用户名：先check数据库当前用户名是否使用过，如果没有，则将该用户名写入到数据库记录中。并发情况下，可能会出现重复的用户名；但是这个可以通过DB级别的Unique Key来保证。<br>Phantom<br>问题出现的过程：而且Insert 和 Delete 无法通过 For Update解决，因为库中根本不存在能够上锁的记录。MySQL中的解决方案是：<code>NextKey Lock</code>。<br>无法复制加载中的内容<br>Materializing Conflicting<br>Naive Idea是给不存在的row构造锁。<br>比如在会议室预定的场景中，可以新增一张表用于记录一个room的预约时间：每一行表示会议室被预约的时间(roomid, start_time, end_time)，初始化所有room所有可能的预约时间，当一个事务要预约的时候，需要先拿到对应room，对应时间段的锁（这一步操作是可以通过For Update实现的），因此可以解决Phantom问题。<br>缺点：</li><li>实现非常丑陋，需要新增一张Lock Table。</li><li>其次是需要将并发控制模型耦合到数据模型中，造成模型污染。<br>实在没有办法的话，才会选择这种实现方式，更常用的防止幻读的方式是通过：<code>Serializable</code>这个隔离级别来实现。<br>Serializable<br>所有并发事务的执行过程都和某一次顺序执行的结果一致，这种方法排除了所有事务并发的可能。<br>Serializable实现的方法大致分为以下三种</li><li>Actual Serial Execution</li><li>2PL</li><li>Serializable Snapshot Isolation<br>Actual Serial Execution<br>采用一种暴力回避并发冲突的方式，使用单线程一次只允许一个事务执行。虽然这是一个显而易见的方法，但是这个方法直到2007 年才被数据库专家们认可。因为在之前的30多年内，许多工程师都觉得使用单线程的方式会带来很多性能问题。</li><li>RAM变得更加便宜，使得机器的内存得到扩张，能够容量的数据集增长，使得事务能够存放在内存中，CPU能够更快的访问。</li><li>数据库工程师们发现：OLTP系统的事务总是特别短，通常只包含若干个 Read &#x2F; Write操作，而OLAP系统通常都是需要一系列较长的Read操作组成，所以其实只要保证不出现Partial Write的情况导致分析出错，业务方大多数情况下都还能够接受，因此OLAP是可以基于Snapshot来做事务的。<br>一些NoSQL如VoltDB, Redis, Datomic 都提供了对可序列化级别的事务支持，使用单核单线程的执行事务其实是一种tradeoff，一方面这样做能够减少上下文切换的开销，但是系统整体的吞吐量也因为单机而受到了限制。为了让单线程的事务得到更好的支持，需要将事务打包成一个特定的数据结构或者package传给DB层。<br>事务包装<br>购买机票场景：把用户的操作和交互封装在事务中，通过多次HTTP请求完成一个事务：（查询航班 (Read) - 查询航班票价和座位 (Read) - 选择航班和座位 (Write) - 填写旅客信息 (Write) - 付钱 (Write)），这一整个事务需要不断地通过HTTP请求与用户进行交互。</li><li>网络 和 DiskIO 能拖垮整个系统的响应时间。</li><li>用户的犹豫不决也让事务访问的对象迟迟得不到释放，使其他想要购票的旅客体验急剧下降。<br>所以，一种方法是将“交互”的模式转变为一次性的事务请求，即：一个HTTP只能关联一个事务。即便是这样，应用层和DB层也需要通过多次的交互来完成整个事务，如下图（上）所示。</li></ul><p>在可序列化级别下，为了保证性能甚至不允许应用层和DB层进行过多的交互，而是将整个事务包含的command一次性交给DB作为：”Store Procedure”。如上图（下）所示。<br>不同时代的系统对于Store Procedure有着不同的支持，早期 Store Procedure 还是存在很多问题的，比如 Metric &#x2F; Log等运维组件无法接入，或者一个 BadTx阻塞了整个CPU的执行也出现过。</p><ul><li><p>RDMS系统：</p><ul><li>Oracle, SQL Server, PostgreSQL 分别提供了 PL&#x2F;SQL, T-SQL, pgSQL&#x2F;PL 的Store Procedure的支持。这些实现都非常丑陋并且没有一个配套的生态，所以在开发中使用的也很少。</li></ul></li><li><p>NoSQL系统：</p><ul><li>VoltDB, Datomic, Redis 分别使用Java，Java，Lua来实现对Store Procedure的支持。由于Store Procedure能够在内存中执行，所以它也不会使系统的性能过度的退化；相反，它减少了网络和磁盘IO，使得系统的执行效率更高了。<br>总结对于Serial Execution的使用</li></ul></li><li><p>每一个事务必须是small and fast的，受到 <code>Single CPU Single Thread</code>的限制。</p></li><li><p>它要求数据集的大小能够存在于内存中，如果数据集太大或者事务涉及冷数据的时候，就需要与Disk进行交互，此时会拖慢整个系统。<br>这里可以使用一个叫做Anti-Caching的策略，将访问冷数据的事务挂起，执行另一个事务，并且在这个过程中异步地将第一个事务所需的数据加载到内存中，等加载完成后就能够加载第一个事务并执行。</p></li><li><p>如果对于写吞吐要求较高的场景，可能需要将数据进行Partition，每一个CPU负责若干个Partition上的事务。</p></li><li><p>如果事务涉及Cross-Partition，则需要额外的Coordinator和锁进行处理，又会进一步拖慢系统。所以总的来说，需要根据场景进行Trade-Off，如果能够支持Dynamo通过配置化定制不同的场景会是一个比较好的方案。<br>2PL<br>第一阶段：获取锁<br>第二阶段：释放锁<br>任何可能引起并发冲突的事务都会被串行化，通过并且读写会互斥，比Snapshot级别的读写并发效率低很多。<br>Predict Locking<br>在查询条件上进行上锁。<br>select * from booking where room_id&#x3D;123 and start_time&gt;’2022-05-12 12:00’ and end_time &lt; ‘2022-05-12 13:00’’</p></li><li><p>一个读请求必须保证查询条件上没有写锁，一个写请求必须保证查询条件上没有读锁。<br>Index Range Locks<br>Also Known as Next-Key Locking<br>Predict Lock对于性能有较大的损失，因为检查锁的过程非常耗时。<br>NextKey Lock会对事务请求的数据估计一个范围，然后对这个范围进行上锁，（通常需要配合索引使用，如果没有索引的话可能会锁住整张表）。比如：在查找会议室的时候：</p></li><li><p>如果通过room_id进行查找，则会在 room_id 进行上锁；</p></li><li><p>如果通过时间范围进行查找，则会对 时间范围 上锁。<br>Snapshot Isolation (SSI)<br>2PL 是一种极端悲观的并发处理方式，他会将检测冲突的行为前置于事务处理；SSI则是在乐观控制的基础之上加入了冲突检测机制，来判断两个事务的执行结果是否会存在并发问题，如果存在在的话，会挑选一个事务Abort。当然乐观控制也并不是完美的，在并发事务不多的情况下，能够work得很好，因为只需要回滚非常少量的事务；但是在事务并发很多的情况下，却会存在比较多的问题，因为一旦检测到事务并发冲突，就会回滚事务；这样在数据库已经接受很大流量的情况下，会使情况进一步恶化。<br>Decision based on outdated premise: 在预约会议室或者Doctor Oncall的例子中，并发问题的出现是由于两个事务同时基于读 数据Data，并且基于读到的Data进行分析，并且执行会影响数据Data的操作。如果数据库能够感知事务的执行结果会影响到事务过程中对Data 的分析，并对其做最坏的预估，就能够避免 Phantom &amp; Write Skew 的问题：</p></li><li><p>方法一：检测MVCC 对象的Version，如果有事务更新了当前Version的Data，就认为出现了事务并发问题；</p></li></ul><p>在MVVC这个隔离级别下，事务都是基于一个满足一致性的快照进行的。在SSI算法中，如果事务在即将提交的时候发现自己的premise不成立（SnapShot被修改），会立即Abort。</p><ul><li>为什么要等到事务提交才进行 Detect  + Abort操作？<br>事务有可能是只读的，一个只读的事务可以允许它基于一个Consistent的快照进行；在只读的事务中，只需要快照满足一致性即可，并不会出现 Write Skew的问题。而且一个事务在Read的时候，DB并不能预测事务未来是否会有写操作，因此不能 Read到快照被修改之后就立即Abort当前事务。</li><li>方法二：检测事务的Write操作是否会影响事务的Read操作结果。</li></ul><p>如果事务43在写执行Write操作的时候检测到事务42更新了它的读结果，此时会将事务回滚。Index-Range Lock 需要数据库在执行事务的过程中记录，这将带来额外的开销，但是由于其能够在事务提交或者回滚之后被删除，所以并不会对内存造成过大的占用。<br>SSI 相比 2PL最大的好处是 Read &#x2F; Write 事务能够并发，这样不会导致 Read事务因为 Write被阻塞，能够可观地增加系统的吞吐量以及降低延迟。Read 事务能够基于Snapshot进行。<br>SSI 相比 Serial Execution最大的好处是不用局限于单核单线程，能够最大限度地利用好多核的机器，增加系统的吞吐量。<br>Downside：SSL针对读写兼备的长事务不太友好，因为长事务需要一直执行到提交阶段才会进行并发检测，如果出现了Race Condition，会将执行了很长的事务回滚，这对于系统会造成不小的运行损失。</p><p>总结<br>Outline</p><ul><li>常见的并发问题<ul><li>脏读 &#x2F; 脏写</li><li>不可重复读</li><li>丢失更新</li><li>Write Skew</li><li>幻读</li></ul></li><li>事务隔离级别<ul><li>Read Commited</li><li>Read Repeatable</li><li>Serializable<ul><li>Serial Execution</li><li>2PL</li><li>SSI<br>总之，不论对于传统数据库还是新型的数据库来说，事务都是一个非常重要的Feature，因为它能够保证一系列操作的ACID特性，对于保持数据的一致性非常关键。</li></ul></li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>DDIA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamo</title>
    <link href="/2022/05/22/Dynamo/"/>
    <url>/2022/05/22/Dynamo/</url>
    
    <content type="html"><![CDATA[<p>Dynamo: Amazon’s High Available KV Store</p><p>DynamoDB是Amazon平台上构建的 “always on” availability 级别的NoSQL数据库，并且能做到无缝扩展 (high-scalability)。为了实现这个级别的可用性，它在某些故障场景中将不可避免的牺牲一致性。<br>最重要的两个核心点：High-Availability &amp; Scalability.<br>尽管牺牲了强一致性，Dynamo也是一个划时代的产品，他证明了去中心化也能够为系统提供高可用需求，同时Dynamo的成功也证明了最终一致性的DB基础架构也不失为高可用系统的一块基石。</p><p>推荐阅读：为了更好的体验，推荐移步至 <a href="https://lo845xqmx7.feishu.cn/docs/doccnAYOAFo5OQPUCapBWMqNAzb">飞书Dynamo</a></p><p>Introduction<br>亚马逊作为电商巨头，它对于用户能够添加购物车的场景尤为重视，Dynamo的提出很大程度上是受“加购”这个场景的驱动。它对于系统的要求是服务必须总是能够为用户提供 加购 和 查看购物车的功能，不论DC是否受到了灾难或者网络是否可达。<br>此外，Dynamo提供了可用性和扩展性的配置选项，因为在Amazon很多业务都需要根据自身的场景选择不同的存储系统，至少会有不同的存储需求。比如很多场景都只需要访问数据的主键，如果使用传统的关系型数据库则会出现性能瓶颈。<br>Dynamo 基于业界熟知的算法来满足对可用性和扩展性的需求：</p><ul><li>一致性Hashing：For Partition &amp; Replication</li><li>Object Version：For Consistency</li><li>Quorum Read &#x2F; Write</li><li>Gossip: Failure Detecting &amp; Membership Protocol<br>系统的假设 &#x2F; 要求</li></ul><ol><li><p>Query Model对数据项简单的读写是通过Primary Key做的状态存储为一个由唯一键确定的二进制对象。没有横跨多个数据项的操作，也不需要关系方案。<br>Dynamo主要用于小数据的存储，它的Store Server价格一定会高于GFS的Chunk Server。</p></li><li><p>ACID属性：在保证ACID的数据存储往往有很差的可用性，DynamoDB的目标应用程序是保证高可用性，弱一致性。<br>Dynamo 不提供任何隔离级别，因为它只允许单个Key上进行操作。</p></li><li><p>Efficiency：系统需运作在一般的Commodity Hardware上，服务必须能够通过配置Dynamo使他们不断达到延迟和吞吐量的要求。衡量Dynamo 性能的指标是 P99.9。<br>Design Consideration<br>很多传统的商务系统都会使用同步复制在多副本之间进行数据拷贝，为了实现这种级别的强一致性，系统会在网络不可达或者机器failover的时候被阻塞。<br>对网络延迟和服务宕机敏感的系统来说，可以通过乐观复制的技术来提升服务的可用性，即：数据的变化可以在后台更新，并发断网的操作也是可以执行的（并发断网：我理解是服务某个用户的Server和集群断连，多个用户的server可能同时面临这种情况；但是在该情况下仍然需要保证服务可用）。这样一来就会出现数据冲突Conflict， 设计时就需要考虑由谁在什么时候解决数据冲突的问题。</p></li></ol><ul><li>对于When解决冲突：很多传统的系统为了实现一致性，都使用了同步复制的方式，即：在Write的时候限制了Write不能达到所有的数据节点，则认为这个Write不能成功；而Read的时候可以向任一Server发起读请求，读到的都是最新的数据，简化了Read的逻辑。Dynamo采取了另一种思路：保证Write总是能成功，但是Read的时候需要做一些逻辑来保证一致性。（为了提升用户体验，如果用户发现自己无法将商品加入购物车，很可能就不会购买该商品，公司则会亏钱）。</li><li>对于Who解决冲突：Dynamo作者的思路是，如果将Reconciliation的逻辑放在DB层，那么用户可能没有自定义解决冲突的能力，因为DB层处理冲突通常都是采取LWW (Last-Write-Win) 的方式（可以想想MySQL是如何处理事务冲突的：Transaction）。从另一个思路来看，业务层其实是对某个场景的上下文理解比较全面的，所以Dynamo将冲突的处理留给了业务层。它向业务返回的时候，可能会返回多个版本，由用户来进行Merge。</li></ul><p>核心设计点：<br>动态增长的Server Node<br>对称：所有的节点一视同仁，没有特殊节点服务于特殊流量<br>去中心化：Peer-to-Peer 没有Leader决定Log Index<br>Heterogeneity：Server能够承担的流量要和Server主机的性能成线性关系</p><p>Related Works</p><ol><li><p>Peer-to-Peer (P2P) Systems<br>第一代P2P存储系统：Freenet and Gnutella 文件共享系统，特点是一次查询请求会通过洪泛法[2]将请求发送给系统中的其他节点，尽可能多的找到保存了查询数据的节点以返回。<br>第二代P2P存储系统：Pastry &amp; Chord，每一个节点保存一份路由信息，在O(1)的时间内找到系统中负责对应请求的节点。在此基础之上，OceanStore &amp; PAST也被工程师提出。OceanStore支持事务和持久化，它采用了为每一个事务赋予全局顺序的方式来解决冲突 (Conflict Resolution)。PAST则室友了一个抽象层来实现持久化存储。</p></li><li><p>分布式文件系统<br>P2P系统的路由表（也是命名空间的一种）只能保存简单的节点信息，而分布式系统的 NameSpace能够保存级联的路由信息。<br>产品：Farsite System (类似NFS的文件系统，去中心化)，GFS 是谷歌内部使用的中心化 (Single Master) 的文件系统 Google File System， Bayou 分布式RDB，提供最终一致性以及离线操作。<br>Bigtable 提供强一致的结构化数据存储，它的底层是使用多级的 <code>sorted map</code>实现。<br>这里想特别提一嘴的产品是 Antiquity，因为这个产品是笔者第一次见到能够抵御拜占庭故障的产品，它主要是为信托机构提供数据的强一致性。</p></li><li><p>讨论<br>Dynamo关注的点和传统的去中心化，面向强一致的存储系统不同，针对Amazon的电商场景，Dynamo更加注重用户的体验，为了让系统在网络分区的环境下仍然能够正常的运行。<br>Dynamo 使用场景的特殊性：</p></li></ol><ul><li>在 Failure &amp; Concurrent 的条件下提供 “Always Writable” 的数据存储服务。</li><li>应用在Amazon内部应用的基础架构层，所以可以保证所有节点都不是拜占庭节点。</li><li>使用Dynamo的应用不会像文件系统那样出现级联的命名空间，以及复杂的关联模型，而是简单的KV模型。</li><li>SLA级别：P99.9 要在几百毫秒内，提供给对延迟较为敏感的业务系统使用。<br> System Design</li></ul><ol><li>Dynamo API<br>get: key -&gt; ([value], context)<br>Exposes inconsistency: can return multiple values<br>Context is opaque to user (set of vector clocks)<br>Put: (key, value, context) -&gt; void<br>Caller passes context from previous get</li></ol><p>Context：包含了系统对于key的metadata，它是和key-value一起存储的，用户不需要对Context理解。Dynamo会将 key和value都视为一个字节序列，它会将key先用MD5 Hash进行加密得到128-bit的标识符，用于一致性Hashing决定哪个节点作为其Coordinator。<br>2. Partitioning<br>主要使用的是一致性Hashing，为了保证增加和删除节点的时候，系统每个节点的流量变化更加平均。更多Partition的信息可以参考：Partition is All You Need<br>3. Replication<br>Dynamo中三个可配置的参数: (N, R, W)<br>N 同一份数据需要复制到多少个节点上；<br>R 每次最少从R个节点中读取数据；<br>W 每次最少得到W个节点的Write数据；</p><p>通过一致性Hashing，可以计算出一个Key的Coordinator节点，这个节点除了将key写入到本地之外，还会在哈希环中顺时针寻找N-1个节点，同步到N-1个其他的节点中。一个Key对应多个Nodes，这些为同一个Key提供服务或者备份的节点称为节点的 “Preference List”，并且为了容错，preference list通常会包含多于N个节点，它只会包含物理节点信息，而不会包含虚拟节点信息（物理节点和虚拟节点的概念，需要读者自行了解一致性Hashing的原理）。<br>4. Data Versioning<br>Dynamo为上层应用提供最终一致性，因此所有write的操作都是异步地向Replica传递，一个Put操作可以在收到所有Replica的Ack之前向Caller返回结果（但必须大于超参W），所以Read操作是有可能读到Stale Data的。<br>在Amazon场景中可以允许Read到旧数据的，但是必须保证用户的Write操作不能被覆盖，比如：“加入购物车”和“删除购物车”，即使网络分区或者部分服务不可用的情况下，也要保证用户的写一定会生效，比如加入购物车的商品一旦加入，在未来的某个时刻用户必须能读到这个商品。过程中产生的冲突可以通过Reconciliation修复。<br>为了实现以上系统要求，Dynamo会将每一个更新操作后的数据视为不可变对象，并且允许系统中存在一个对象的多个版本，比如：Node-A上存在商品A，Node-B上存在商品B，Node-C上存在商品C。在大多数情况下，Dynamo本身就能在DB-Level对数据冲突进行修复（使用LWW），但如果同一个对象的状态因为并发写 &amp; 网络问题出现了Branch，就需要在Application-Level进行Merge。</p><p>应用层在使用Merge策略的时候，能够保证Add的商品一定不会丢失，但是Delete的商品不一定能够删除，提升用户的购买率。</p><p>为了使系统中能够存在多个版本的数据，Dynamo为每个对象添加了Vector Clock，VC用来表示系统中两同一个Object两个Branch的逻辑先后顺序，如果两个Branch是可比较的那么DB层保留最新的版本；如果两个版本是冲突的，则需要应用层进行Merge，每个Vector Clock表示为：<br>struct ClockNode{<br>    Node ServerNode,<br>    Counter int64,<br>}</p><p>struct VectorClock{<br>    VectorNodeItem list<ClockNode><br>}</p><p>在PUT的时候，Dynamo要求Client指定Version，Application包含在Context中传递给DB。如果Dynamo遇到了DB层无法合并的Branch，则会将所有状态都返回给Client，Client必须Issue一个新的Put请求，来解决冲突。理论上，如果存在Conflict的Branch越多，需要存储的Version Data越多，那么Context会膨胀，很吃内存。但Dynamo Paper中说实际上这种情况十分罕见，因为Writes总是会被分配到Preference List中Top N个节点，当出现网络分区的时候会造成Branching的情况，这种情况下需要对Vector Clock的大小进行限制（限制List的length），假设只允许存储10个ClockNode，当达到阈值时会采用FIFO策略进行淘汰。<br>Author们说在实际生产环境中很少遇到这种问题，因此也没有深入研究过这块算法。</p><ol start="5"><li>Get 和 Put操作的执行<br>负载均衡的方式：</li></ol><ul><li>使用LB中间件对Client的请求进行转发，这样Client不用感知Cluster中的节点。</li><li>让Client感知所有Partition，并直接对合适的Partition发起请求，这种方式会更快，因为不用LB转发。<br>Coordinator：处理Client请求的节点，通常是Preference List中Top N中的第一个节点。如果通过LB的方式选择Node，则需要考虑LB指定的Node是否在Preference List的Top N中，如果在的话，则成功成为Coor；如果不在的话，则需要将请求转发给Preference List中第一个节点。Top N个节点是Healthy的节点，即不存在网络分区的节点；当网络分区出现的时候，lower ranked的Node也会被Client访问到。<br>Dynamo Quorum：(N, R, W) 通常的配置是 (3, 2, 2)。<br>Dynamo Sloppy Quorum: R + W &gt; N （和Paxos类似，Paxos中使要求任何请求都获得大多数节点的认可，Dynamo只需要 R+W&gt;N 就能保证每次读到最新的结果，即使这些结果有冲突）。 </li><li>“Never Block Waiting for Unreachable Nodes”；</li><li>“Each Key Has Replicas &gt; N”</li><li>Hinted  Handoff: 当Failover出现的时候，Dynamo并不总是会选择TopN of Preference List。比如Consistency Hashing Ring如下图所示，如果A不可达，那么A上的Replica会被转移到D上，并且被转移的Replica上会携带一个HintNode的标志，表示它原先是属于A节点的Replica，当系统检测到A节点恢复之后D会把Replica返回给A，然后从本地存储中删除。<br>通过Hinted Handoff，Dynamo保证了系统总是能够保证Read &#x2F; Write能够正常执行，即使出现了暂时的Node Failover或者网络不可达。<br>Dynamo Cross DC Strategy: 由于Dynamo天然的Leaderless架构，支撑了它能够很好的扩展到不同的DC中，在Dynamo中一个Key的Preference List总是由不同DC的节点组成，并且每一份副本都会保存在不同的DC上，为了应对极端情况下DC断电或者网络短连，以及其他自然灾害。</li></ul><ol start="6"><li>Membership and Failure Detection<br>Ring Membership<br>Gossip-Based：每一个Node定期与其他节点的通信，感知到系统中存在的节点，有点类似（Rip路由算法），每个Node随机地和两个节点通信获取他们c的信息，这样系统中不断的进行信息交换最终能够得到完整的“路由信息”，缺点和Rip一样，就是每次删除或者更新节点的时候，其他节点需要反应一段时间才能意识到新加入了节点。<br>Join And Leave<br>在早期的Dynamo架构中使用了Gossip-Based的方法对新加入的节点或者移除的节点进行检测，而后期加入了显示的Node Join and Leave。其实这是对Node Failover 和 Node Addition做了不同的策略，对于节点的Failover，系统认为这是一种短暂的状态，所以会通过gossip的方式进行传播；而对于永久加入或者移除一台Server，系统会认为这是一种永久的操作，如：在DC中加入了Server不会在短期内弃用，所以是通过显示通知的方式告知所有节点。<br>Implementation<br>无法复制加载中的内容<br>Local Persistence<br>Dynamo的持久化层是可插拔的，可以针对不同的业务适配不同的持久化层，比如：BDB (Berkerly Data Store)适用于10-100KB的数据，MySQL适合更大的数据等。<br>Read Coordination<br>笔者理解Read Coordination是使用行为者模式设计的一种事件驱动模型，这一部分是整个Dynamo架构中的组织协调者，它将一个工作流拆分为不同的子阶段：<br>无法复制加载中的内容<br>在Coordinator得到所有Node的返回结果之后，会通过Version判断出最新的结果，并且会尝试更新那些保存着过期数据的节点，该过程被称为 Read-Repair。</li></ol><p>如果每次都选择Preference List中第一个节点作为Coordinator，可能会造成流量分布不均匀。为了解决这个问题，Dynamo中允许Preference List的Top N任意一个节点成为Coordinator。并且系统总是认为“每一个Write操作之后总会紧跟一个Read操作”，比如：（Write, A）, (Read, A)，那么前一个Write的Coordinator会自动成为后一个Read 的Coordinator，以此来增加“Read Your Write”成功的机会。</p><p>Anti-Entropy<br>Dynamo 使用了Merker Tree来检测不同副本之间的一致性以及最小化需要移动的增量数据。Merker Tree一种哈希树，它的叶子节点是每一个 Key的Hash值，非叶子节点是它子树节点的Hash值的和。Merker Tree最大的好处是在判断两个副本之间数据是否相同的时候，不一定需要加载整棵树结构。例如：如果两个副本根节点的Hash值相同，说明他们子树的值都是一致的，可以提前结束判断。如果副本根节点的Hash值不同，说明肯定存在至少一个数据副本是不一致的，此时需要遍历到树的根节点，找出所有不一致的数据副本，然后选择最新的内容进行同步。</p><p>在Dynamo中，每一个节点会为它所负责的 每一个Key-Range (不同Virtual Nodes会产生不同的Partition) 维护一棵Merker Tree，然后使用树的遍历算法计算两棵树是否相同。</p><p>Business Logic<br>Dynamo最大的优势是作为Infra Storage 为上层提供了自定义解决冲突的方式。</p><ul><li>购物车场景；使用Merge的方式解决Version Conflict。</li><li>User Session场景使用：LWW的方式解决冲突。</li><li>Adjust Params：<ul><li>对于Heavy Read场景可以将R设置为1，W设置为N。</li><li>对于Heavy Write场景，并且允许读到stale data的场景，可以将W设置为较小值，R响应增大。<br>Performance Analysis</li></ul></li><li>SLA: P99.9 &#x3D; 300ms。<br>2006 年Dynamo读写请求值的分布情况：</li><li>Write请求的延迟总是大于Read请求：Write请求通常需要Disk Access</li><li>Avg 总是低于 P99.9，因为P99.9 对流量洪峰、访问对象大小和网速更为敏感。</li></ul><p>Client Driven &#x2F; Server Driven Dynamo</p><ul><li>Client Driven<br>Client保留一份集群中节点的分区信息副本，每次Write &#x2F; Read 直接向对应的Node发起请求。</li><li>Server Driven<br>Client 每次将Write &#x2F; Read请求发送到LB，由LB选择Coordinator处理Client请求。</li></ul><p>Client Driven 的方式减少了IP包转发的次数，所以能够减少延迟。但是Client-Driven方式的效率很大程度取决于Client的local cache能够拿到多新的Cluster Image，在该方法中Client会周期性地 (10s) 随机地向 Dynamo Node 发起Pull请求，获取集群当前的信息。所以最坏的情况下，Client会保持10s的过期配置信息，但是Client会有一个热更新的操作当它检测到Node不可达会立即Pull一次Cluster Config。<br>补充内容：Dynamo Data Model</p><ul><li>Table, Items, Attributes</li><li>Primary Key</li><li>Secondary Key</li><li>DynamoDB Data Types</li><li>Item Distribution<br>Table Item Attribute<br>在RDB中，一个table有一个预先定义好的结构，比如：表名，主键，列名。而DynamoDB只需要一个主键，不需要提前定义各种属性或者数据类型，DynamoDB中的独立item可以拥有任何数量的属性，但每个item的size不能超过400KB。<br>Items中的每个属性都是一个 (K, V) pair 这里的value可以是集合、列表、String等数据结构。<br>Primary Key<br>用于唯一标识一个item，DynamoDB中支持两种类型的主键。<br>Partition Key（分区键）一种简单的Primary Key，仅有一个属性组成的Key，DynamoDB利用Key的Hash值计算Item应该被存储的分区 (Partition)。<br>Partition Key &amp; Sorted Key：由分区键和排序键组合的主键，DynamoDB利用Value确定存储的分区，对于在同一个分区中的Key按照Sorted Key进行排序。<br>注意：</li><li>Partition Key可以理解为 Hash Attribute，通过Hash计算，可以实现让Items 根据Key值平均分散到不同的位置上。<br>在DynamoDB中，Partition Key的属性只能为String, Number, Binary</li><li>Sort Key 可以理解为 Range Attribute，利用Sorted Key可以确保拥有相同Partition Key的item在物理结构上按照顺序紧密的存储在一起。<br>Secondary Index</li><li>Global Secondary Index：一种由Partition Key 和 Sorted 以外的Key组成的索引。</li><li>Local Secondary Index：一种由Partition Key和不同的Sorted Key组成的索引。<br>Item Distribution<br>DynamoDB 利用Partition 存储数据，Partition是基于SSD的并且会自动创建三个副本。</li></ul><ol><li>在只有Primary Key的情况下：<br>如果table中有一个Primary Key，DynamoDB会基于这个Key确定这个Key的分区；读写过程都是先基于Primary Key找到对应的Partition，然后Load和Store Data。</li></ol><p>注意：Partition Key最好选择那些差异程度最大的属性，来最大程度上避免collision。<br>2. Partition Key &amp; Sorted Key<br>写入的时候先根据Partition Key找到对应的物理分区，然后根据Sorted Key顺序写入；<br>读取的时候可以根据Partition Key进行读取，并且能够在Sorted Key上添加condition条件。</p><p>总结 &amp; 启发<br>Dynamo给我最大的启发就是它利用了一些业界的现有技术，打造了一个新概念的DB产品，很多Follow-UP的数据库都是基于Dynamo产生的，包括：RocksDB，VoltDB，以及字节内部使用的Abase，美团使用的TiDB。(A + B + C + D) 这种模式其实更多的被使用在学术界，笔者之前短暂从事过AI的研究工作，发现不少出名的工作都是基于“炒烂饭”被提出的。其实，借鉴前人的经验和工作并不是什么坏事，能够把握时机，在正确的时代背景下运用正确的技术手段也是一件伟大的工作。</p><p>Reference</p><ul><li>Dynamo Paper</li><li>OSPF</li><li>Vector Clock</li><li>Antiquity</li><li>Bayou</li><li>Google File System</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>NoSQL</tag>
      
      <tag>Amazon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GFS</title>
    <link href="/2022/05/22/Google%20File%20System/"/>
    <url>/2022/05/22/Google%20File%20System/</url>
    
    <content type="html"><![CDATA[<p>Google File System</p><ul><li>分布式文件系统，弱一致性模型 (Relaxed Consistency)；</li><li>谷歌三驾马车之一，与MapReduce和BigTable开启了新的大数据时代，引领工业界向NoSQL发展；</li></ul><p>推荐阅读：为了更好的体验，推荐移步至<a href="https://lo845xqmx7.feishu.cn/docs/doccncmo8iqkFFn424B8lZd44gh">飞书文档</a>。 <a href="https://lo845xqmx7.feishu.cn/docs/doccnadQrN9vNCbIGJh5YtTOGxh">GFS FAQ</a>。</p><p>Intro<br>单机系统的存储容量始终是有限的，随着数据量不断增大，如果只对单台机器进行scale up也会逐渐达到上限，所以scale out to multi-Server是发展的必然。<br>GFS主要支持谷歌内部的软件系统，比如：YouTube视频，Google Drive等等，谷歌并不对外开放GFS的接口，只对外提供上层服务。Bigtable 和 MapReduce 都是建立在GFS之上的应用。并且Hadoop生态中的HDFS也是在GFS系统之上发展出来的一个文件系统分支。</p><p>系统的主要目标和大多数分布式存储系统一致，主要包含三个方面：</p><ul><li>Reliable, High Performaces</li><li>Scalable</li><li>Available<br>系统设计时考虑的几个方面：</li><li>Normal Failure<br>文件系统由上百台廉价的存储机器组成，对外部大量的Client 提供服务。</li><li>Huge File<br>系统需要存储TB级别的数据集，由十亿级别的KB的数据组成，所以需要重新对磁盘IO和文件块大小进行评估。</li><li>Append rather than Overwriting<br>在GFS中几乎不存在随机写，并且一旦写入了文件，所有的读操作都是顺序读，系统的优化主要集中在这个点上。<br>Assumption<br>谷歌的工程师针对自家的业务场景对系统的情况规定了一些前提与假设：</li><li>系统中每一台Sever都是廉价的主机，所以经常会fail，需要对系统进行监控和容错。</li><li>GFS主要用于存储大文件，相比Chubby 主要是用来存储小文件的场景，GFS存储的文件量通常为几百万的100MB的文件甚至Multi-GB 的文件大小。</li><li>读场景主要有两种方式：<ul><li>大规模的顺序读写：顺序读写上百个KB大小的内容。</li><li>小规模的随机读写：几KB的在随机位置的读写。</li></ul></li><li>写场景主要是 大 和 顺序写，以Append的方式追加到文件末尾，并且GFS规定一旦文件写入就极少会被修改。</li><li>系统需要执行原子的高并发写入。</li><li>高持续带宽比低延迟更重要。<br>作为一个分布式文件系统，GFS在乎的是整个系统的吞吐量，由此可能会因为Replication 造成一些延迟的开销。</li></ul><p>Interface<br>不完全支持标准POSIX API接口，但提供了<code>create, delete, open, close, read, write</code>这些方法。<br>POSIX，Portable Operating System Inferface，可移植操作消停接口，是UNIX系统设计的一个标准。Linux和Windows部分支持该协议。</p><ul><li>Snapshot：可以对file 或者 目录打快照</li><li>Record Append：允许多个Client 同时向同一个文件追加记录，并且不用加锁。<br>Architecture<br>重点来了：GFS集群是一个经典的Master-Worker架构，Master充当任务分配和元数据管理的角色，而Worker是真正存储数据的角色。<br>整个系统的架构如下图所示。</li></ul><p>角色</p><ul><li>Chunk<br>数据块，每一个chunk的大小为64MB并且每一个chunk由一个uint64类型的 chunk handler 唯一标识，在创建的时候由master指定。为了保证容错，通常一个chunk会包含三个副本。<br>目前这种设计会造成hot spot，极端情况下，所有的Client都去访问某一个chunk，然而该chunk只有三个副本，会导致chunk所在的机器 IO次数急剧增大。<br>GFS的解决方案是给这些chunk更多的Replica，Author们也提到了可以让Client 向另一个 Client 请求数据。</li></ul><p>可以看到在GFS文件系统中使用了比Linux操作系统中更大的块，能够带来如下优点：</p><ul><li><p>减少Client 和 Master的交互次数<br>  一次请求就能获取所有chunk的位置信息，并且这个信息是可以在TTL规定的时间内缓存的 。</p></li><li><p>减少Client 和 ChunkServer的交互次数<br>  利用空间局部性和TCP的长链接，减少网络IO。</p></li><li><p>减少了元数据的大小<br>  如果chunk size设置的比较小，那么在master上就需要管理更多的元数据信息。一旦RAM装不下，就会使用Disk存储，这样磁盘IO又成为了性能瓶颈。<br>每一个文件会被split成不同的chunk，这些chunk可以存放在一台机器的磁盘上，也可以存放在不同机器的磁盘上。在GFS的视角看来，我需要存储和管理的文件就是chunk而不会关心file到底有多大；而从用户的视角来看，每一个file被切分出来的chunk1-n都是逻辑连续的。</p></li><li><p>Client<br>GFS Client 可以认为是依赖于 GFS lib 进行类库函数调用的线程，也是一个逻辑上的概念 [2]。<br>Client Side 不会缓存文件数据，因为数据太大了。</p></li><li><p>Master<br>提供In-Memory的元数据管理支持，主要包含：<br>无法复制加载中的内容<br>表示Master会通过operation log持久化到Disk上。对于chunks的位置信息，通过Heartbeats 周期性获取。<br>Master 在内存中存储了两个表：<br>Table 1:<br>Key<br>Value<br>Filename<br>An array of chunk handler (nv)</p></li></ul><p>Table 2:<br>Key<br>Value<br>Chunk handler<br>List of chunk servers<br>Chunk version number<br>Primary node<br>Lease expiration</p><ul><li>Chunk Server</li></ul><p>Consistency Model<br>Guarantees by GFS<br>文件名namespace的变化（文件创建）是由master在内存中进行的，整个操作通过namespace lock确保操作的原子性，master的operate log定义了全局创建操作的顺序。</p><p>这张表描述了在Append Record之后系统的状态：</p><ul><li>Consistent: 不论Client从哪一个Replica中读取数据，Client都能看到一致的信息。</li><li>Defined：当一个文件数据修改后，它能够保持Consistenct的状态并且所有Client能够看到全部修改的内容。是一种级别更高的一致性。</li><li>Consistent bu undefined: Client虽然能够看到相同的数据，但是不能及时看到其他进程的修改。</li><li>Inconsistent：不同Client能够在不同的时间看到不同的数据。<br>此外，表格中也显示了GFS中两种更新操作：</li><li>Write<br>Write是修改原来文件中的数据，相当于是一种overwrite的操作。</li><li>Record Append<br>Record Append是一种追加操作，它包含着“原子 + AtLeastOnce” 语义，因为没有AtMostOnce的语义，所以GFS是有可能存在Duplicate Record的。GFS为了保证写入的顺序，通常会由Primary Replica决定一个Chunk的写入顺序，并且由Master向Client返回一个Offset代表Record写入的起始位置。<br>Master 会通过心跳的方式与Chunk Server进行定期的通信，并且使用Version Number来判断Replica是否过期；一旦Master判定某个Replica是过期的，会对过期的Replica 进行GC。</li></ul><p>Client会对GFS的数据进行缓存，并且Client有可能缓存到旧数据，由于GFS中的数据都是以Append ONLY的方式写入的，所以Client 缓存的数据可以理解为只是部分不完整的数据，而不是错误的数据。<br>System InterActions<br>Read Process<br>Client 向Master请求文件所在的Replica，Master返回Chunk Handler和Location；<br>Client向最近的Replica发起请求，读取指定范围的数据。</p><p>Write Process<br>Lease &amp; Mutation Order：Master会给每一个Mutation的操作（每一个Mutation操作只对应一个Chunk，即不能超过chunksize）指定一个Primary Replica，给它赋予一个Lease，并且由这个Primary决定每chunk的写入顺序。<br>在Lease的时效 (60s) 内Primary可以不用与Master通信，而自行决定文件Append的执行顺序。<br>Master通过心跳的方式判断Primary的liveness，并且决定是否需要延长Lease的时效。</p><p>整个写操作可以分为如下7个步骤：</p><ol><li>Client向Master询问哪一个Chunk Server持有要进行写操作的Chunk的Lease</li><li>Master向Client返回Primary的Handler以及其他Replica节点的地址；Client收到Master的回复并进行缓存，当Primary不可达或者TTL后，会向Master按照Step1重新请求。</li><li>Client向所有的Replica推送数据，并且Replica会将数据写入到Buffer中。</li><li>当Replica都接收到了Client的数据，Client会向Primary发起Commit请求，Primary对多个写生成执行计划（主要是决定写入顺序），然后将该执行计划先应用于本地IO操作。</li><li>Primary将写操作转发给其他两个Replica，使其按照Primary的执行计划进行IO操作。</li><li>Follower给Primary 返回写入成功或者失败的Response。</li><li>Primary响应Client，并且返回该过程中发送的错误，Client如果收到了error，则会Reissue这次Write操作。（📢 这里会引起问题…）。<br>SnapShot<br>这个操作由于比较常见，所以在内容中增加一下。Redis持久化和一些新型的数据库引擎 (In Memory: BitCask; Disk: InnoDB-MVCC) 都使用了SnapShot来提升并发度保证一致性。<br>目标：尽量减少对正在进行写操作的影响。<br>GFS使用标准的Copy-On-Write技术来实现快照：</li></ol><ul><li>当Master收到SnapShot的请求后，首先会revoke所有Primary Replica的Lease，保证后续的Write操作都会经过Master。</li><li>当Lease撤回或者过期后，Master首先会将操作日志记录到磁盘，然后通过复制源文件以及目录树的metadata来将日志记录应用到内存中的状态。</li><li>当Client请求进行Write操作的时候，Master发现chunk上的引用技术大于1，便会重新命令chunk Server 创建一个新的chunk，在新的chunk上进行写操作。<br>Master’s Operation<br>Master主节点负责的工作有：</li><li>所有namespace的管理工作</li><li>管理整个系统中所有的chunk Replicas<ul><li>做出将chunk 实际存储在哪里的决定，创建新的Chunk和 Replica；</li><li>协调系统的各种操作（比如：读、写、快照），用于保证chunk正确且有效的进行备份。</li><li>管理chunkserver之间的负载均衡，回收没有被利用的存储空间。<br>Namespace Management and locking<br>不同于其他传统的文件系统，GFS并没有为每一个目录创建一个用于记录当前目录拥有哪些文件的数据结构，也不支持文件和目录的别名。GFS逻辑上把namespace当做一个查询表，用于将full pathname（目录或者是具体的文件）映射为metadata，并且使用前缀压缩使得这个表可以在内存中被高效的表示。<br>Master节点的每一个操作都必须先获得一系列的锁才能够真正的运行，当Client并发的对 <code>/home/user/bar</code> 和<code>/home/user/foo</code>进行修改的时候，会对bar和foo两个文件上写锁并对父节点上读锁。</li></ul></li></ul><p>一般来说，最底层的文件修改都需要上写锁，为了防止对文件进行并发的修改，而对于其父目录通常只需要上读锁，为了防止将目录重命名、删除和SnapShot。<br>并且所有的锁获取都需要按照顺序获取，以此来防止死锁。</p><p>Replicas Create &amp; Rebalance<br>Replicas 会在 Create, Re-replication, Rebalance的时候被创建。<br>创建<br>当Master需要创建Replicas的时候, Master会按照以下策略选择将 Replicas 放置到哪个位置上：</p><ul><li>Master希望选择一个磁盘使用率低于平均值的Server。</li><li>Master不希望所有最新创建的Replicas都集中在一台机器上，因为刚创建的Replicas可能隐含着即将进行大量的读写操作。</li><li>Master希望尽可能地把Replicas放到同一个DC中，不同的 racks，即不同的层上。<br>Re-Replication</li><li>某些Replicas不可用导致Replicas的数量低于预先设置的数量</li><li>程序员手动增加了Replicas的数量<br>Master会按照一定的优先级进行Re-Replicatoin：</li><li>首先，如果ChunkA的Replicas数量少于chunksB 那么优先对chunkA进行Re-Replication；</li><li>如果某个 chunk阻塞了Client请求的执行，那么优先对其进行Re-Replication；</li><li>最近活动 (Read &#x2F; Write) 的文件比最近删除的文件具有更高的优先级。<br>为了防止过多的clone操作占用系统的带宽，Master节点既会限制chunkserver的clone数量也会限制整个GFS集群同时进行clone的数量。<br>Rebalancing<br>Master会检查Replicas的分布，然后将Replicas移动到更合适的位置。通过这个机制，Master可以逐渐对新加入的Node进行Replicas填充，而不是瞬间加入大量的Replicas将其打爆。总的来说，重平衡是为了使磁盘利用率和Server QPS更加均匀的分布到各台机器上。<br>Garbage Collection<br>GFS使用了一种惰性删除的操作，当一个文件被应用删除时，Master节点会将此删除操作立即写入日志系统，但是Master不会立即将文件空间回收，而是将其文件名设置为一个不可见的name，并且包含删除指令的时间戳。<br>在Master定期对namespace扫描的过程中，如果发现一个chunk的删除时间已经超过三天，会将这个chunk的空间进行回收。在一些特殊的情况下，可能会出现orphaned chunks，chunkserver如果检测到orphaned chunk会立即将他删除。</li></ul><p>优点：这种删除方式简单可靠</p><ul><li><p>chunk可能在一些replicas上成功，在一些Replica上失败，因此会留下一些节点没有记录的chunk数据。当master发现其没有记录某个chunk信息的时候，会告知chunkserver，chunkserver会对那个空位置进行垃圾回收。</p></li><li><p>删除指令不一定能够送达chunkserver，在GFS的方案中，因为会有定时的心跳，因此master 最终一定会告知chunkserver删除相关的chunk。<br>缺点<br>这种方式在资源紧张情况下，无法及时回收存储空间。不过GFS提供了其他机制来确保快速的删除操作，比如动态地加快存储回收策略，在GFS不同区域采用不同的删除策略。<br>High Availability<br>Fast Recovery<br>Master 以及 Chunk Server不管出于何种原因的故障，都能在秒级时间恢复故障前的状态。<br>Chunk Replication<br>一个chunk复制到三台Replica上。<br>Master Replication<br>Shadow Master：在Master节点宕机后，Shadow Master能够对外提供只读的操作。Shadow Master与Master的通信类似MySQL PB 的Binlog操作。<br>Shadow Master允许略微滞后于Master，但是不会读到错误的旧数据，而是读到不完整的旧数据，Append Only保证的。</p></li></ul><p>Some Questions For MySelf &amp; Reader:</p><ul><li>GFS prohibits the size of a record append to exceed the chunk size. Use a sequence of events to demonstrate why a larger record append may lead to correctness issues.</li><li>When GFS can show some duplication?</li><li>Why can Client-B read Client-A’s update even though Client-A is returned an error?</li></ul><p>GFS FAQ<br>GFS Supplementary </p><p>Reference<br>[1] <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf</a><br>[2]<br><a href="https://spongecaptain.cool/post/paper/googlefilesystem/">https://spongecaptain.cool/post/paper/googlefilesystem/</a><br>[3] <a href="https://en.wikipedia.org/wiki/Google_File_System">https://en.wikipedia.org/wiki/Google_File_System</a><br>[4] MIT 6.824<br>无法复制加载中的内容<br>[5] Google Slides</p>]]></content>
    
    
    
    <tags>
      
      <tag>Distributed Storage</tag>
      
      <tag>Google</tag>
      
      <tag>File System</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
